{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAN\n",
    "\n",
    "- [x] Acquisition\n",
    "    - [x] Select what list of repos to scrape.\n",
    "    - [x] Get requests from the site.\n",
    "    - [x] Save responses to csv.\n",
    "- [x] Preparation\n",
    "    - [x] Prepare the data for analysis.\n",
    "- [ ] Exploration\n",
    "    - [ ] Answer the following prompts:\n",
    "        - [ ] What are the most common words in READMEs?\n",
    "        - [ ] What does the distribution of IDFs look like for the most common words?\n",
    "        - [ ] Does the length of the README vary by language?\n",
    "        - [ ] Do different languages use a different number of unique words?\n",
    "- [ ] Modeling\n",
    "    - [ ] Transform the data for machine learning; use language to predict.\n",
    "    - [ ] Fit several models using different text repressentations.\n",
    "    - [ ] Build a function that will take in the text of a README file, and makes a prediction of language.\n",
    "- [ ] Delivery\n",
    "    - [ ] Github repo\n",
    "        - [x] This notebook.\n",
    "        - [ ] Documentation within the notebook.\n",
    "        - [ ] README file in the repo.\n",
    "        - [ ] Python scripts if applicable.\n",
    "    - [ ] Google Slides\n",
    "        - [ ] 1-2 slides only summarizing analysis.\n",
    "        - [ ] Visualizations are labeled.\n",
    "        - [ ] Geared for the general audience.\n",
    "        - [ ] Share link @ readme file and/or classroom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "ADDITIONAL_STOPWORDS = ['readme', '\\n\\n\\n', '-PRON-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACQUIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have decided to search Github for \"san antonio data\" and scrape the results.\n",
    "# https://github.com/open-austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_repo(url):\n",
    "    \"\"\"\n",
    "    This function takes a url and returns a dictionary that\n",
    "    contains the content and language of the readme file.\n",
    "    \"\"\"\n",
    "    response = get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    readme = soup.find('div', id='readme')\n",
    "    language = soup.find('span', class_='lang')\n",
    "    \n",
    "    d = dict()\n",
    "    if readme is None:\n",
    "        d['readme'] = 'No readme file.'\n",
    "    else:\n",
    "        d['readme'] = readme.text\n",
    "    if language is None:\n",
    "        d['language'] = 'No language specified.'\n",
    "    else:\n",
    "        d['language'] = language.text\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This line to test out the function.\n",
    "# get_github_repo('https://github.com/open-austin/atx-citysdk-js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_links(url):\n",
    "    \"\"\"\n",
    "    This function takes in a url and returns a list of links\n",
    "    that comes from each individual repo listing page.\n",
    "    \"\"\"\n",
    "    response = get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for link in soup.findAll('a', itemprop='name codeRepository', attrs={'href': re.compile(\"^/\")}):\n",
    "        links.append(link.get('href'))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This line to test out the function.\n",
    "# get_github_links('https://github.com/open-austin?page=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_github_links(path, num_pages):\n",
    "    \"\"\"\n",
    "    This function takes in a url path and number of pages\n",
    "    and returns a list of lists of all links.\n",
    "    \"\"\"\n",
    "    all_links = []\n",
    "    for i in range(num_pages):      # Number of pages plus one\n",
    "        page = i + 1\n",
    "        response = get(path + str(page))\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        all_links.append(get_github_links(path + '?page=' + str(page)))\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # This line to test out the function.\n",
    "# get_all_github_links('https://github.com/open-austin', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(o, tree_types=(list, tuple)):\n",
    "    if isinstance(o, tree_types):\n",
    "        for value in o:\n",
    "            for subvalue in traverse(value, tree_types):\n",
    "                yield subvalue\n",
    "    else:\n",
    "        yield o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_readme(url, num_pages, cache=True):\n",
    "    if cache and os.path.exists('github_readme.json'):\n",
    "        readme_text = json.load(open('github_readme.json'))\n",
    "    else:\n",
    "        data = get_all_github_links(url, num_pages)\n",
    "        readme_text = []\n",
    "        for value in traverse(data):\n",
    "            print('https://github.com'+value)\n",
    "            readme_text.append(get_github_repo('https://github.com' + value))\n",
    "        json.dump(readme_text, open('github_readme.json', 'w'))\n",
    "    return readme_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nBase images\\nHow to make updates:\\n\\nCreate a new branch\\nIf you're adding a python dependency:\\n\\nRun make run-base\\nRun poetry add --dev <package> (drop the --dev if it's a production\\ndependency)\\nFor other operations see the\\npoetry docs\\nMaybe edit pyproject.toml by hand if necessary\\nRun poetry lock\\n\\n\\nIf it's a node dependency:\\n\\nRun make run-dev\\nDo whatever node/yarn things you people do ;-)\\n\\n\\nBump the version in VERSION file\\nBump version in dev/Dockerfile\\nCommit your changes\\nOpen a pull request; if necessary\\nCommit and tag it make tag\\nMerge it to master\\nDelete the branch\\nPush\\nUpdate child projects to use this new version\\n\\n\\n\\n\",\n",
       "  'language': 'Dockerfile'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nSoftware to collect donations for nonprofits. It integrates with Saleforce, Stripe, Amazon Pay, Slack and Sentry.\\nDonations\\n\\nPython running Flask\\nsupports single and recurring donations\\neasily deployed on Heroku\\n\\nGetting Started\\nThe recommended method for running this repo locally is to use Docker. If you don't already have Docker set up, you'll want to install Docker for Mac to get a Docker environment set up on your computer.\\nYou'll also need to have an env file set up with the environment variables for Stripe\\nand Salesforce so that Docker can find them. By default the Makefile will look for\\nenv-docker but this can be overridden with the DOCKER_ENV_FILE environment variable.\\nYou should also install pre-commit, which we use for managing Git hooks (including JS formatting via Prettier). Once downloaded, run pre-commit install at the root of this repo. You'll also need Node version 8.\\nRequirements\\nPython 3.6+\\nSee requirements.txt and dev-requirements.txt for specific Python packages and versions.\\nEnvironment\\n\\n\\n\\nVariable\\nExample\\n\\n\\n\\n\\nPUBLISABLE_KEY\\npk_test_12345\\n\\n\\nSECRET_KEY\\nsk_test_12335\\n\\n\\nSALESFORCE_HOST\\ntest.salesforce.com\\n\\n\\nSALESFORCE_CLIENT_ID\\n\\n\\n\\nSALESFORCE_CLIENT_SECRET\\n\\n\\n\\nSALESFORCE_USERNAME\\n\\n\\n\\nSALESFORCE_PASSWORD\\n\\n\\n\\nSALESFORCE_TOKEN\\n\\n\\n\\nCELERY_BROKER_URL\\namqp://guest:guest@rabbitmq:5672/\\n\\n\\nCELERY_RESULT_BACKEND\\nredis://redis:6379/0\\n\\n\\nFLASK_SECRET_KEY\\nb'f;\\\\xeb\\\\x9bT2\\\\xcd\\\\xdb\\\\xe1#z\\\\xfb\\\\xab\\\\xf8(\\\\x03'\\n\\n\\nENABLE_SENTRY\\nFalse\\n\\n\\nSENTRY_ENVIRONMENT\\nproduction\\n\\n\\nSENTRY_DSN\\nhttps://user:pass@sentry/7?timeout=10\\n\\n\\nENABLE_SLACK\\nFalse\\n\\n\\nSLACK_API_KEY\\n\\n\\n\\nSLACK_CHANNEL\\n#donations\\n\\n\\nMAIL_SERVER\\nmail.server.com\\n\\n\\nMAIL_USERNAME\\n\\n\\n\\nMAIL_PASSWORD\\n\\n\\n\\nMAIL_PORT\\n25\\n\\n\\nMAIL_USE_TLS\\nTrue\\n\\n\\nDEFAULT_MAIL_SENDER\\nfoo@bar.org\\n\\n\\nACCOUNTING_MAIL_RECIPIENT\\nfoo@bar.org\\n\\n\\nBUSINESS_MEMBER_RECIPIENT\\nfoo@bar.org\\n\\n\\nREDIS_URL\\nredis://redis:6379\\n\\n\\nSALESFORCE_API_VERSION\\nv43.0\\n\\n\\nREPORT_URI\\nhttps://foo.bar\\n\\n\\n\\nRunning the Project\\nRun make backing. This will start RabbitMQ and Redis.\\nRun make. This will drop you into the Flask app.\\nRun yarn run dev. You should then be able to interact with the app at localhost:80\\nC_FORCE_ROOT=True celery -A app.celery worker --loglevel=INFO &\\ncelery beat --app app.celery &\\n# gunicorn app:app --log-file=- --bind=0.0.0.0:5000 --access-logfile=-\\n\\nFront-end commands:\\n\\nyarn run dev: Start Flask development server and watch for JS and CSS changes\\nyarn run js:dev: Just watch for JS and CSS changes\\n\\nFront-end notes:\\n\\nOn yarn run dev, all files are built to /static/js/build, which is ignored from version control. That way you can make as many changes as you want when developing, Webpack will recompile the files, and they'll never show up in VC.\\nOn deploy, all files are built to /static/js/prod/. This is not ignored from VC because Heroku cannot create directories and thus needs it to exist in the repo. That's why it contains a .gitkeep file.\\nOn deploy, the production JS has to be built via the postinstall script. This means that, if you run yarn or yarn add <package> locally inside Docker, you'll get some compiled files in /static/js/prod/ that show up in version control. Delete them!\\n\\nImportant note: To build our JS on deployment, Heroku needs to run a postinstall script in package.json. This also means every time you run yarn or yarn add <package>, it's going to trigger that build and generate a bunch of files in static/js/prod/. Don't commit these!\\nRunning tests\\nTo run the project tests, run\\nmake test\\nSecurity\\nIf you find vulnerabilities in this repo please report them to security@texastribune.org.\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\n\\n  data-visuals-create\\n\\n\\n\\n\\n\\n\\nA tool for generating the scaffolding needed to create a graphic or feature the Data Visuals way.\\nKey features\\n\\n📐 HTML templating with a familiar, easy Jinja2-esque format via a modified instance of a Nunjucks environment that comes with all the functionality of journalize by default.\\n🎨 Supports SCSS syntax for styles compiled with the super fast reference implementation of Sass via dart-sass. All CSS is passed through autoprefixer and minified with clean-css in production.\\n📦 A configured instance of Webpack ready to go and optimized for a two-path modern/legacy bundle approach. Ship lean ES2015+ code to modern browsers, and a functional polyfilled/transpiled bundle to the rest!\\n📑 Full-support for ArchieML formatted Google Docs and key/value or table formatted Google Sheets. Use data you\\'ve collaborated on with reporters and editors directly in your templates.\\n🎊 And so, so, so much more!\\n\\nGetting started\\nnpx @data-visuals/create feature my-great-project\\ncd feature-my-great-project-YYYY-MM # the four digit year and two digit month\\nnpm start\\n\\nWhile you can install @data-visuals/create globally and use the data-visuals-create command, we recommend using the npx method instead to ensure you are always using the latest version.\\n\\nTable of contents\\n\\nInstallation\\nUsage\\nFolder structure\\n\\nconfig/\\ndata/\\nworkspace/\\nproject.config.js\\napp/\\napp/index.html\\napp/templates/\\napp/scripts/\\napp/styles/\\napp/assets/\\nOther directories you may see\\n\\n.tmp/\\ndist/\\n\\n\\n\\n\\nHow to work with Google Doc and Google Sheet files\\n\\nGoogle Docs\\nGoogle Sheets\\n\\n\\nSupported browsers\\nHow do JavaScript packs work?\\n\\nCreating a new entrypoint\\nConnecting an entrypoint to an HTML file\\n\\n\\nAvailable commands\\n\\nnpm start or npm run serve\\nnpm run deploy\\nnpm run data:fetch\\nnpm run assets:push\\nnpm run assets:pull\\nnpm run workspace:push\\nnpm run workspace:pull\\n\\n\\nEnvironment variables and authentication\\n\\nAWS\\nGoogle\\n\\nCLIENT_SECRETS_FILE\\nGOOGLE_TOKEN_FILE\\n\\n\\n\\n\\nLicense\\n\\nInstallation\\nWhile we recommend using the npx method, you can also install the tool globally. If you do, replace all instances of npx @data-visuals/create you see with data-visuals-create.\\nnpm install -g @data-visuals/create\\nUsage\\nnpx @data-visuals/create <project-type> <slug>\\nCurrently there are two project types available — graphic and feature.\\nnpx @data-visuals/create graphic school-funding\\nThis will create a directory for you, copy in the files, install the dependencies, and do your first git commit.\\nThe directory name will be formatted like this:\\n<project-type>-<slug>-<year>-<month>\\n\\nUsing the example command above, it would be the following:\\ngraphic-school-funding-2018-01\\n\\nThis is to ensure consistent naming of our directories!\\nFolder structure\\nAfter creation, your project directory should look something like this:\\nyour-project/\\n  README.md\\n  node_modules/\\n  config/\\n  data/\\n  workspace/\\n  package.json\\n  project.config.js\\n  app/\\n    index.html\\n    templates/\\n    styles/\\n    scripts/\\n    assets/\\n\\nHere are the highlights of what each file/directory represents:\\nconfig/\\nThis is the directory of all the configuration and tasks that power the kit. You probably do not need to ever go in here! (And eventually this will be abstracted away.)\\ndata/\\nWhere data downloaded and processed with npm run data:fetch ends up. You are also free to manually (or via your own scripts!) put data files here - they will get pulled in too! Be aware that the only compatible data files that belong here are ones that quaff knows how to consume, otherwise it will ignore them.\\nworkspace/\\nThe workspace directory is for storing all of your analysis, production and raw data files. It\\'s important to use this directory for these files (instead of app/assets/ or data/) so we can keep them out of GitHub and away from other parts of the kit. You interact with it using the npm run workspace:push and npm run workspace:pull commands.\\nproject.config.js\\nWhere all the configuration for a project belongs. This is where you can change the S3 deploy parameters, manage the Google Drive documents that sync with this project, set up a bespoke API or add custom filters to Nunjucks.\\napp/\\nWhere you\\'ll spend most of your time! Here are where all the assets that go into building your project live.\\napp/index.html\\nThe starter HTML page that\\'s provided by the kit. If your project is only a single page (or graphic), this will likely be where you do all your HTML work. No special configuration is required to create new HTML files - just creating a new .html file in in the app directory (but not within app/scripts/ or /app/templates/ - HTML files have special meanings in those directories) is enough to tell the kit about new pages it should compile.\\napp/templates/\\nWhere all the Nunjucks templates (including the base.html template that app/index.html inherits from), includes and macros live.\\napp/scripts/\\nWhere all of our JavaScript files live. Within this folder there are a number of helpful utilities and scripts we\\'ve created across tons of projects. JavaScript imports work as you\\'d expect, but the app/scripts/packs/ directory is special - learn more about it in the How do JavaScript packs work? section.\\napp/styles/\\nAll the SCSS files that are used to compile the CSS files live here. This includes all of our house styles and variables (app/styles/_variables.scss). app/styles/main.scss is the primary entrypoint - any changes you make will either need to be in this file or be imported into it.\\napp/assets/\\nWhere all other assets should live. This includes images, font files, any JSON or CSV files you want to directly interact with in your JavaScript - these files are post-processed and deployed along with the other production files. Be aware, anything in this directory will technically be public on deploy. Use workspace/ or data/ instead for things that shouldn\\'t be public.\\nOther directories you may see\\n.tmp/\\nThis is a temporary folder where files compiled during development will be placed. You can safely ignore it.\\ndist/\\nThis is the compiled project and the result of running npm run build.\\nHow to work with Google Doc and Google Sheet files\\n@data-visuals/create projects support downloading ArchieML-formatted Google Docs and correctly-formatted Google Sheets directly from Google Drive for use within your templates. All files you want to use in your projects should be listed in project.config.js under the files key. You are not limited to one of each, either! (Our current record is seven Google Docs and two Google Sheets in a single project.)\\n{ // ...\\n  /**\\n    * Any Google Doc and Google Sheet files to be synced with this project.\\n    */\\n  files: [\\n    {\\n      fileId: \\'<the-document-id-from-the-url>\\',\\n      type: \\'doc\\',\\n      name: \\'text\\',\\n    },\\n    {\\n      fileId: \\'<the-sheet-id-from-the-url>\\',\\n      type: \\'sheet\\',\\n      name: \\'data\\',\\n    },\\n  // ...\\n}\\nEach object representing a file needs three things:\\nfileId\\nThe fileId key represents the ID of a Google Doc or Google Sheet. This is most easily found in the URL of a document when you have it open in your browser.\\ntype\\nThe type key is used to denote whether this is a Google Doc (doc) or a Google Sheet (sheet). This controls how it gets processed.\\nname\\nThe name key controls what filename it will receive once it\\'s put in the data/ directory. So if the name is hello, it\\'ll be saved to data/hello.json.\\nGoogle Docs\\nArchieML Google Docs work as documented on the ArchieML site. This includes the automatic conversion of links to <a> tags!\\nGoogle Sheets\\nGoogle Sheets processed by @data-visuals/create may potentially require some additional configuration. Each sheet (or tab) in a Google Sheet is converted separately by the kit, and keyed-off in the output object by the name of the sheet.\\nBy default it treats every sheet in a Google Sheet as being formatted as a table. In other words, every row is considered an item, and the header row determines the key of each value in a column.\\nThe Google Sheets processor also supports a key-value format as popularized by copytext (and its Node.js counterpart). This treats everything in the first column as the key, and everything in the second column as the value matched to its key. Every other column is ignored.\\nTo activate the key-value format, add :kv to the end of a sheet\\'s filename. (For consistency you can also use :table to tell the processor to treat a sheet as a table, but it is not required due to it being the default.)\\nIf there are any sheets you want to exclude from being processed, you can do it via two ways: hide them using the native hide mechanism in Google Sheets, or add :skip to the end of the sheet name.\\nSupported browsers\\n@data-visuals/create projects use a two-prong JavaScript bundling method to ship a lean, modern bundle for evergreen browsers and and a polyfilled, larger bundle for legacy browsers. It uses the methods promoted in Philip Walton\\'s Deploying ES2015+ Code in Production Today blog post and determines browser support based on whether a browser understands ES Module syntax. If a browser does, it gets the modern bundle. If it doesn\\'t, it gets the legacy bundle.\\nIn practice this means you mostly do not have to worry about it - as long as you\\'re using the JavaScript packs correctly everything should just work. In terms of actual browsers, while we do still currently do a courtesy check of how things look in Internet Explorer 11, it\\'s not considered a dealbreaker if a complicated feature or graphic does not work there and would require extensive work to ensure compatibility.\\nFor CSS we currently pass the following to autoprefixer.\\n\"browserslist\": [\"> 0.5%\", \"last 2 versions\", \"Firefox ESR\", \"not dead\"]\\nHow do JavaScript packs work?\\nProjects created with @data-visuals/create borrow a Webpack approach from rails/webpacker to manage JavaScript entrypoints without configuration. To get the right scripts into the right pages, you have to do two things.\\nCreating a new entrypoint\\nBy default every project will come with an entrypoint file located at app/scripts/packs/main.js, but you\\'re not required to only use that if it makes sense to have different sets of scripts for different pages. Any JavaScript file that exists within app/scripts/packs/ is considered a Webpack entrypoint.\\ntouch app/scripts/packs/maps.js\\n# Now the build task will create a new entrypoint called `maps`! Don\\'t forget to add your code.\\nConnecting an entrypoint to an HTML file\\nBecause there\\'s a lot more going on behind the scenes than just adding a <script> tag, you have to set a special variable in a template in order to get the right entrypoint into the right HTML file.\\nSet jsPackName anywhere in the HTML file to the name of your entrypoint (without the extension) to route the right JavaScript files to it.\\n{% set jsPackName = \\'map\\' %}\\n{# This is now using the new entrypoint we created above #}\\nPack entrypoints can be used multiple times across multiple pages, so if your code allows for it feel free to add an entrypoint to multiple pages. (You can also add jsPackName to the base app/templates/base.html file and have it inserted in every page that inherits from it).\\nAvailable commands\\nAll project templates share the same build commands.\\nnpm start or npm run serve\\nThe main command for development. This will build your HTML pages, prepare your SCSS files and compile your JavaScript. A local server is set up so you can view the project in your browser.\\nnpm run deploy\\nThe main command for deployment. It will always run npm run build first to ensure the compiled version is up-to-date. Use this when you want to put your project online. This will use the bucket and folder values in the project.config.js file to determine where it should be deployed on S3. Make sure those are set the appropriate values!\\nnpm run data:fetch\\nThis command uses the array of files listed under the files key in project.config.js to download data to the project. This data will be processed and made available in the data folder in the root of the project.\\nYou can also set dataDir in project.config.js to change the location of that directory if necessary.\\nnpm run assets:push\\nThis pushes all the raw files found in the app/assets directory to S3 to a raw_assets directory. This makes it possible for collaborators on the project to sync up with your assets when they run npm run assets:pull. This prevents potentially large assets like photos and audio clips from ending up in GitHub. This also runs automatically when npm run deploy is used.\\nnpm run assets:pull\\nPulls any raw assets that have been pushed to S3 back down to the project\\'s app/assets directory. Good for ensuring you have the same files as anyone else who is working on the project.\\nnpm run workspace:push\\nThe workspace directory is for storing all of your analysis, production and raw data files. It\\'s important to use this directory for these files (instead of assets or data) so we can keep them out of GitHub. This command will push the contents of the workspace directory to S3.\\nnpm run workspace:pull\\nPulls any workspace files that have been pushed to S3 back down to the project\\'s local workspace directory. This is helpful for ensuring you\\'re in sync with another developer.\\nEnvironment variables and authentication\\nAny projects created with data-visuals-create assume you\\'re working within a Texas Tribune environment, but it is possible to point AWS (used for deploying the project and assets to S3) and Google\\'s API (used for interfacing with Google Drive) at your own credentials.\\nAWS\\nProjects created with data-visuals-create support two of the built-in ways that aws-sdk can authenticate. If you are already set up with the AWS shared credentials file (and those credentials are allowed to interact with your S3 buckets), you\\'re good to go. aws-sdk will also recognize the AWS credential environmental variables.\\nGoogle\\nThe interface with Google Drive within data-visuals-create projects currently only supports using Oauth2 credentials to speak to the Google APIs. This requires a set of OAuth2 credentials that will be used to generate and save an access token to your computer. data-visuals-create projects have hardcoded locations for the credential file and token file, but you may override those with environmental variables.\\nCLIENT_SECRETS_FILE\\ndefault: ~/.tt_kit_google_client_secrets.json\\nGOOGLE_TOKEN_FILE\\ndefault: ~/.google_drive_fetch_token\\nLicense\\nMIT\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nThis is the master repository for The Texas Tribune's Docker images.\\nThe full history for each image is stores in this repo. If the container has\\nits own repository, the full history will be there as well.\\nDeprecation Notice\\nThe texastribune/pg-tools Docker image is no longer built using this repo. All development has moved to texastribune/docker-pg-tools.\\nContributing\\nAll commits and pull requests should be done in the master repository: docks.\\nThis is so you only need one project in your editor up for n docker images.\\nAfter contributions are merged, they can be pushed back out to their individual\\nrepositories.\\nSubtrees\\nGit subtrees let us store all our Docker images in one place while giving each\\nimage its own Git remote and history at the same time. Using git subtrees also\\ngives us:\\n\\nAbility to do webhooks from the Index for these Dockerfiles\\nAbility to do tagged builds in the Index for these Dockerfiles\\nSimpler developer overhead of maintaining n projects:\\n\\nIt's easier to get started because you only have to check out one git\\nrepository and set up one project in your editor\\nYou can refer to other Dockerfiles easier\\nWe can implement best practices across all of our Dockerfiles at once\\n\\n\\n\\nUsage\\nTo help with the verbose arguments to git subtree, there's a helper included:\\nsubtree.\\nAdding all the Git remotes\\nEach Docker image has its own special remote. To make it easier for developers\\nto work on all of them, you can run this command to set up all the remotes at\\nonce:\\n./subtree remotes\\n\\nPushing updates\\nAfter you're satisfied with your updates to a Docker image, you can push\\nchanges back to their dedicated repository with:\\n./subtree push <name>\\n\\nCreating a new image\\nLet's say you're making a Docker image for the hot new app, snazzle:\\n\\nCreate the snazzle Docker image here:\\n\\nmkdir snazzle\\ncd snazzle\\nvi Dockerfile\\ngit commit\\netc.\\n\\n\\nCreate a new empty repository, let's say it's github.com:texastribune/docker-snazzle.git\\nUpdate subtree to add it to the REMOTES definition. Following the pattern,\\nit would look like: [snazzle]=github.com:texastribune/docker-snazzle.git\\nAdd the snazzle remote: ./subtree remotes\\nTell git-subtree that the snazzle directory is associated with the\\nsnazzle remote: ./subtree init snazzle\\nPush your code to the snazzle repo: ./subtree push snazzle\\nContinue editing snazzle and committing changes\\nAs you're ready, keep pushing changes to snazzle with ./subtree push snazzle\\nWhen you're ready to publish to the Docker Registry, add it using the\\ntexastribune/docker-snazzle.git remote, not the remote for this project.\\n(Advanced) If you need to maintain two versions of snazzle, the easist\\nway is to just treat it as a separate project.\\n\\nCheckout the texastribune/docker-snazzle.git remote like you normally\\nwould.\\nBranch and edit the project from there, instead of this main integration\\nproject.\\n\\n\\n\\nForking an existing repo\\nFor a more complex scenario, let's say we're forking an existing project:\\n\\nFork the project in github. Let's say we're forking utensils/fork to\\ntexastribune/fork\\nUpdate ./subtree's remote command to add a remote named fork to\\ntexastribune/fork\\nRun ./subtree remotes to add the remote\\nRun ./subtree init fork to link the two\\nRun ./subtree pull fork --squash to bring the project into this one\\nMake your updates and commit them as you normally would:\\n\\nBranch\\nCommit\\nPull Request\\nMerge\\n\\n\\nRun ./subtree push fork to push changes up, triggering a new build in the\\nIndex\\n\\nMaintaining multiple branches in a remote\\nWhile this is possible, there is not a simple way of doing this using git\\nsubtrees. While there are several possible approaches, we're waiting until we\\nactually need to do this.\\n\\n\\n\",\n",
       "  'language': 'Dockerfile'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nDS Toolbox\\n\\nThis is just a test for a proof of concept of centralizing our design base for our sites\\n\\n[This is for demo purposes only] We'll use this repo to house the base of our shared styling system. CSS comments are parsed to create a JSON object of documentation. That data is rendered with nunjucks for now to give us a visual representation of the various components and rule-sets we're building.\\nyarn\\nyarn start\\nTo Dos\\n\\n Watch task\\n Actions/Versioning and Deploy visual output to URL[https://medium.com/devopslinks/automate-your-npm-publish-with-github-actions-dfe8059645dd] rules\\n Pre commit linting\\n File size tracker\\n Component status tracker\\n Accessibility compliance checking\\n Make VS code comment snippet\\n Fix code preview\\n GitHub search feature\\n Way to easily build universal stylesheet\\n Way to easily build universal sprite\\n Allow for hiding main demo\\n\\nSCSS docs boilerplate\\nWe use a comment parser along with some extra logic to generate our docs. To add a new section of documentation, add a boilerplate above your CSS rules like the one below:\\n// Title of Section (root-class-name)\\n//\\n// Description {{isWide}} {{isHelper}}\\n//\\n// root-class-name-modifier - desc\\n//\\n// Markup: 6-components/test/test.html\\n//\\n// Styleguide 6.0.1\\n//\\n.root-class-name {\\n  \\n}\\n\\n{{isWide}} is used to display the demo of each modifier at full width\\n{{isHelper}} is used to hide main demo and only display modifiers\\n\\n\\n\\n\",\n",
       "  'language': 'CSS'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTexas Tribune File App\\nThis app powers the file system that the Texas Tribune uses withing its CMS.  This app provides a flexible file system api for use in the browser. It strives to have minimal dependencies and use modern javascript to\\nenable better performance and unique features.\\nDocumentation\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\ntexastribune/pg-tools\\nA dockerized toolset for importing and exporting data from a postgreSQL database.\\nExample usage\\nImporting from S3\\ndocker run -it --rm \\\\\\n  --link=db:db \\\\\\n  --env=AWS_ACCESS_KEY_ID=<aws_access_key_id> \\\\\\n  --env=AWS_SECRET_ACCESS_KEY=<aws_secret_access_key> \\\\\\n  --env=DATABASE_URL=<database-url> \\\\\\n  --env=S3_SOURCE=<s3-source> \\\\\\n  texastribune/pg-tools /app/import-from-s3.sh\\n\\nImporting from a URL\\ndocker run -it --rm \\\\\\n  --link=db:db \\\\\\n  --env=DATABASE_URL=<database-url> \\\\\\n  --env=DATABASE_BACKUP_URL=<url-to-postgres-dump> \\\\\\n  texastribune/pg-tools /app/import-from-url.sh\\n\\n\\n\\n',\n",
       "  'language': 'Shell'},\n",
       " {'readme': 'No readme file.', 'language': 'Dockerfile'},\n",
       " {'readme': 'No readme file.', 'language': 'Dockerfile'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nthermometer\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\n\\nwalls\\nThis queries Salesforce for opportunity information, massages it into a nice JSON format and then stores it in S3 where it will ultimately be accessed by browsers/web app(s).\\ntesting\\nmake test\\nrun it\\nConfigure env file with Salesforce anw AWS variables.\\nmake\\nLicense\\nThis project is licensed under the terms of the MIT license.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nscuole\\n(It\\'s Italian for \"schools.\")\\nPublic Schools 3!\\nSetup\\nThis project assumes you are using the provided docker postgreSQL database build. Make sure docker is up and running, then run:\\nmake docker/db\\nThis will create the data volume and instance of PostgreSQL for Django. To ensure Django knows where to look, you need to set the DATABASE_URL. If you are not using the docker provided database, use DATABASE_URL to tell the app what you\\'ve done instead.\\nexport DATABASE_URL=postgres://docker:docker@docker.local:5432/docker\\nFrom there, the app should be runnable as normal.\\nCreate your virtual environment:\\nmkvirtualenv scuole-dev\\nThen install the requirements:\\npip install -r requirements/local.txt\\nCheck for any migrations:\\npython manage.py migrate\\nThen see if it\\'ll run:\\npython manage.py runserver\\nAll good? Let\\'s go!\\nAdmin\\nThis likely won\\'t have an admin interface, but you are welcome to use it to check out how things are getting loaded. First, you\\'ll need to create a super user. (If you ever blow away your database, you\\'ll have to do it again!)\\npython manage.py createsuperuser\\nThen, after a python manage.py runserver, you can visit http://localhost:8000/admin and use the credentials you setup to get access. Every thing will be set to read-only, so there\\'s no risk of borking anything.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nscuole-data\\nA repository of data sets used in the scuole project.\\nCommon Core of Data (ccd)\\nSummary data for districts and campus via the National Center of Education Statistics Common Core of Data program.\\nAskTED Data\\nSuperintendents, Principals and directory information for all schools and districts.\\nTo download directory data, go to the AskTED homepage and download the School and District File with Site Address.\\nTo download superintents and principals data, go to the AskTED Download Personnel File page. Check the box for either Include Principals or Include Superintendents and indicate None on both Include District Staff and Include ESC Staff. Sort by Organization Number and Download File.\\nTAPR Data\\nAll stats collected by the Texas Education Agency. This data is released annually in late November/early December.\\nTo download TAPR data, go to the Texas Academic Performance Report homepage and find the most recent release. Click the Data Download link and go to the Advanced TAPR Data (Numerators, Denominators & Rates) option.\\nThis app requires sheets for Postsecondary Readiness & Non-STAAR Performance Indicators, Longitudinal Rate (4-Year, 5-Year, & 6-Year), and Staff & Student Information for Campus, District, Region and State. The Reference Information, Accountability Rating and Special Education Determination Status sheet is only required for Campus, District and Region.\\nThe data is referenced and mapped in the schema using the Master reference of TAPR elements. The reference tables can be found here:\\n\\nCampus student information\\nCampus staff information\\nCampus college admissions, college-ready graduates\\nCampus APIB, RHSP, annual dropout, attendance, advanced courses, higher education\\n\\nNote: These are tables for 2012-2013. We use the campus tables to collect the codes used in the TAPR tables and later remove the prefixes for state (S), region (R), district (D), campus (C) and the suffixes -- if there are any -- indicating year Usually year suffixes are included for fields like college ready graduates where there are two graduation times within the school year, but not for demographic data which is representative of the entire year. Prefixes and suffixes are both handled in the data loaders. Codes do not change from year to year.\\nTAPR Updates\\nFor campuses and districts that have changed their names have been removed or are new in the current year, email Lauren Callahan at lauren dot callahan at tea dot texas dot gov and ask for a CSV or Excel spreadsheet of:\\n\\nCampuses and districts that don't exist anymore\\nNew campuses and districts\\nCampuses and districts that have changed their name\\n\\nDistrict boundaries\\nTEA also provides shape files for each district that can be found on the TEA Data Download page. We don't display the actual shapes on the page because they're not accurate enough and may be misleading. They are useful for determining nearby districts and geolocating.\\nWe convert the TEA provided shapefile into a geojson file using ogr2ogr with the command:\\nFor school districts:\\n$ ogr2ogr -f GeoJSON -t_srs crs:84 districts.geojson [tea-provided-file-name].shp\\nFor campuses:\\n$ ogr2ogr -f GeoJSON -t_srs crs:84 campuses.geojson [tea-provided-file-name].shp\\nCohorts\\nThe Texas Higher Ed Coordinating Board (THECB) and TEA provide data for the Higher Ed Outcomes section of the app every year in late April/early May. To obtain and clean the data:\\nFirst, download the data from THECB\\nThen, Create a folder in this repository. Name it the year (YYYY) to which the data cooresponds. Open the spreadsheet. Unhide the Master Raw Data worksheet in the .xlsx file from THECB. Copy and paste that data into a new spreadsheet. Change the headers to match the list of fields in the loader, or copy and paste the headers from a previous year's data. Be sure the data matches the header, and save it as regionState.csv.\\nCopy and paste the data found in Region Cty Gender, Region Cty Econ and Region Cty Ethnicity into individual csv files. Name them countyGender.csv, countyEcon.csv and countyEthnicity.csv, respectively. Change the headers to match the list of fields in the loader, or copy and paste the headers from a previous year's data.\\nBe sure to remove any notes found at the top and bottom of all .xlsx files and make sure all counts are integers and all percents are floats (this might require changing their format in excel 😬).\\n\\n\\n\",\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nGet emailed when a website changes\\nKlaxon is a free, quick to set up and easy to use robot that checks websites regularly so you don\\'t have to.\\nYou list websites you want monitored and Klaxon will visit them and, if they change, email you what\\'s different. It saves you having to reload dozens of links yourself every day.\\nIt\\'s perfect for monitoring website changes you might miss, like freedom of information disclosure logs, court records, and anything related to Donald Trump. And it can even send notifications to your Slack channel.\\nRead more below, or say hello to the humans behind the project at the Google Group email list.\\n\\nAlerting journalists to changes on the web\\nBuilt and refined in the newsroom of The Marshall Project, Klaxon has provided our journalists with many news tips, giving us early warnings and valuable time to pursue stories. Klaxon has been used and tested by journalists at The Marshall Project, The New York Times, the Texas Tribune, the Associated Press and elsewhere.\\nThe public release of this free and open source software was supported by Knight-Mozilla OpenNews.\\nHow Does Klaxon Work?\\nKlaxon enables users to \"bookmark\" portions of a webpage and be notified (via email or Slack) of any changes that may occur to those sections. Learn more about bookmarklets on the help.md page.\\n\\n\\nSetting up your Klaxon\\nKlaxon is open source software built in the newsroom of The Marshall Project, a nonprofit investigative news organization covering the American criminal justice system. It was created by a team of three—Ivar Vong, Andy Rossback and Tom Meagher—and it is subject to the kind of shortcomings any young, small side project might encounter. It may break unexpectedly. It may miss a change in a website, or an email might not fire off correctly. Still, we’ve found it immensely useful in our daily reporting. We want other journalists to benefit from Klaxon and to help us improve it, but keep these caveats in mind and use it at your own risk.\\nOur team will keep hacking on Klaxon in spare moments, and we plan to keep it humming for our own use. But we think this project has the potential to help just about any newsroom. For it to succeed and to evolve, it will depend on the contributions from other journalist-developers. We are excited about the prospect of building a community around this project to help maintain it. So when you spot the inevitable bug, please let us know. And if you’d like to help us make this better, or add new functionality to it, we’d love to have your help.\\nGetting started\\nOne of our goals for Klaxon is to make it as easy as possible for reporters and editors without tech backgrounds to use and to set up. Getting your own Klaxon running in your newsroom will require you to run a handful of instructions one time through the help of online services Heroku and Github. Following these directions, it should take maybe 10 minutes to set up your Klaxon, including the time to create accounts on Heroku and Github if you need to.\\nWe use Heroku to deploy software in The Marshall Project newsroom. We think it makes some of the tedious work of running servers a lot easier to deal with, so we designed Klaxon to be easily deployable on Heroku. (If you’d like to run this in your newsroom’s preferred server setup — say using Docker or a Linux machine — we encourage you to do so, and to send it back to us, with documentation, in a pull request.) If you want to use our setup, you’ll need to create an account with Heroku, if you don’t already have one.\\nHow much will this cost?\\nIt should be free to get started with Klaxon on Heroku, but if you start using it a lot, you may need to pay a small amount to keep it running. Out of the box with Heroku, for free you’ll get:\\n\\n12,000 emails per month with SendGrid\\n10,000 records of changes in your Postgres database\\nYour web interface available 18 hours a day\\nChecks of each of your watched sites every 10 minutes with Heroku’s Scheduler.\\n\\nIf you find your newsroom hitting the limits of these free tiers, you can pay to expand them. To send up to 40,000 emails a month, you can upgrade your Sendgrid add-on in Heroku for $9.95 a month. If you need to store more records in your database, you can pay $9 a month for up to 10 million rows. And if you need your web interface running around the clock, you can upgrade your Heroku dyno from the Free to the Hobby level for $7 a month. Some of these won’t be necessary, particularly in smaller newsrooms, but it’s good to know.\\nIf you want to track files with Klaxon, you’ll have to set up an account with Amazon Web Services’ Simple Storage Service (S3). If you are tracking a few dozen files on the web that update irregularly, it should cost you pennies each month. (More on this option to come…)\\nLet’s do this\\nIf you have a Heroku account and you’re ready to go, it’s time to click on this button:\\n\\nYou must be logged into your Heroku account, and it will take you to a page to configure your new app in Heroku’s dashboard. First, give your app a name in the first box. While this is technically optional, this will also double as the URL for your Klaxon instance, so think carefully about it for a moment. Try maybe an abbreviation for your newsroom with a hyphen and the word klaxon, like “wp-klaxon” or “sl-klaxon”. This will become a URL as https://sl-klaxon.herokuapp.com/\\nScroll down to the “* Admin_emails” field, add a comma-separated list of email addresses for your newsroom’s Klaxon administrators. These administrators will be able to create accounts for any user in your organization, as well as configure various Klaxons and integrations with services like Slack.\\nClick the big purple “Deploy for Free” button. If you haven’t given Heroku your credit card yet, it will ask you for your information now. As long as you’re on the free settings, it won’t charge you, but Heroku wants to be prepared in case you change tiers. After that, give Heroku a few minutes for the app to build.\\nWhen you see this message:\\n\\n...you’re almost done.\\nClick on the button that says “Manage App”. This takes you behind the scenes of the various components powering your Klaxon. On this resources screen, click on the link for “Heroku Scheduler,” which will take you a new screen where you must add the very important piece. The scheduler is what runs every 10 minutes to actually check all the sites and pages you’re watching. Click the long, purple ‘Add new job” button. In the text box next to the dollar sign, type the words “rake check:all” with the colon and without the quotes. Under “Frequency,” change it from “Daily” to “Every 10 minutes”. Click the purple “Save” button and your scheduler item should look like this:\\n\\nNow, at the top of the scheduler page, click the link that is the name of your app (“sl-klaxon”). This will take you to your Klaxon’s login screen on the web.\\n\\nType the admin email address that you gave the Heroku dashboard earlier into the text box and hit the “Login” button. You’ll then be redirected to a page that says an email has been sent to you. Check your inbox. This may take a minute or two to arrive.\\nIn the email, click the “Go to Dashboard” button. You’re now authenticated in the system and can access your Klaxon.\\nConfiguring your Klaxon\\nOnce you’re logged in, you should see the main page that will fill up in the coming days with the feed of all of your Klaxon updates. Now, you want to go add other users in your newsroom to the system. Click the “Settings” button in the upper right corner, and select “Users” from the menu.\\nOn the right side of the page, click the “Create New User” button. Add the reporter’s first and last name and email address, and she will get an email allowing her into the Klaxon. Now, finally, you and your users can start adding web pages you want Klaxon to watch.\\nLimit new users to only those on specific email domain(s)\\nBy default, people with any email address can be added as new users. If you\\'d like to allow only users with specific email domains, set the APPROVED_USER_DOMAINS environment variable (or \"Config Variable\" in Heroku\\'s lingo). That variable should be a comma-separated list of domains, e.g., themarshallproject.org,nsa.gov.\\nNotify a Slack channel\\nYou’re all set for email notifications. If you’d like to also receive alerts through Slack, you can set that up now too. (If you want alerts from other services, we welcome pull requests) Click on the “Settings” button in the upper right corner of the page and choose “Integrations” from the menu. On the Integrations page, click the “Create Slack Integration” button. You can add an integration for any number of channels in your newsroom’s Slack. For each one, you just have to set up an Incoming Webhook. In Slack, click on the dropdown arrow in the upper left corner and choose “Apps & Integrations” from the menu. This will open a new window in your browser for you to search the Slack app directory. In the search box, type “Incoming Webhooks” and choose that option when it pops up. If you already have webhooks, you’ll see a button next to your Slack organization’s name that says “Configure.” Otherwise, click the green button that says “Install”.\\nNow, choose the channel that you want the Klaxon alerts to go to from the dropdown menu. We’d recommend that you not send them to #General, but maybe create a new channel called #Klaxon. After you create or choose your channel, click the green button that says “Add Incoming Webhooks Integration”. Near the top of the next screen, you should see a red URL next to the label “Webhook URL”. Copy that URL and switch over to your browser window with Klaxon in it. Paste the URL into the box labeled “Webhook URL,” and type the name of the channel you want your Slack alerts to go to into the “Channel” box (this should be the same channel name you used in Slack when you created the integration). Now click the “Create Slack Integration Button”. Now you should be all set. If you want to have the ability to send Klaxon alerts to other channels, for specific reporting teams or for certain projects, you can repeat this process.\\nApplying upgrades as the project develops\\nWhen we release major changes to Klaxon, we’ll make an announcement to our Google Group email list. At that point, you’ll likely want to adopt those in your system as well. If you\\'re comfortable using git on the command line, this would require just a few simple commands: pull the changes from the master branch of this repo, merge them into your forked repo and push it all to Heroku.\\nBut if you\\'re not a programmer, there is still a fairly painless way to upgrade by using Github and Heroku. First, you’ll need to fork our repo to your own Github account to receive the updates, and then you can use Heroku’s dashboard to push the changes to your application.\\nIf you don’t already have an account at Github.com, now is a good time to set one up (don’t worry, it’s free). This has the added benefit of giving you access to comment on the issues our community is working on developing. Once you’re logged into Github with your new account, go to the repo for the Klaxon project and click the “Fork” button. This copies our code into a separate version under your Github account that you can tie to your Klaxon instance running on Heroku’s servers.\\nNow, go to https://dashboard.heroku.com and choose your application (remember, the one you named when you first set up Klaxon, probably sl-klaxon or something similar if you followed our advice above). From the menu of options at the top of the page, click on the “Deploy” button. Look for section called “Deployment method,” which should be the second from the top of the Deploy page.\\nYou should see three buttons. Click the one in the middle that says “Github Connect With Github”. The options at the bottom of the page will change. Now, click the gray button that says “Connect To Github”. It will pop up a new window to log you into Github, if you aren’t already. In that window, click the “Authorize Application” button. The popup window should now close itself.\\nOn the Heroku page, in the “Connect to Github” section at the bottom, type ‘klaxon’ into the search box next to your Github username. Click the “Search” button. Next, click the “Connect” button next to the name of your forked repo that pops up below. Finally, select the \\'master\\' branch from the dropdown and click “Enable Automatic Deploys” button in the “Automatic deploys” section. This ties your Heroku server to your Github account, so that every time you merge updates into your forked version of the Klaxon repository, they will automatically go live on your server with the latest updates. You\\'ll only have to do all of this one time to set up the pipeline.\\nNote: if you are upgrading from version 0.2.0 or lower, please follow the additional instructions in migration_setup.md\\nFinally, each time an update is announced on the Google Group, you can go to your forked version of the repo on Github and click the green “New Pull Request” button to pull the changes from our master repo.\\nOn the \"basefork\" dropdown on the left, click and select your repo. Then click the “compare across forks” link and change the “head fork” on the dropdown menu to “marshallproject/klaxon”. Make sure both the branches are set to “master” (they should already be). Below that, a green checkbox and the words “Able to merge” should appear. If they do, click the green “Create Pull Request” button. Give this pull request a title. You might want to say “Merging Klaxon release 0.9.1” or whatever the new version number is and click the “Create Pull Request” button again.\\nYou should then get a response that looks like this:\\n\\nIf it does, and everything is green, you’re good to go. Just click the “Merge pull request” button then click the “Confirm merge” button and that’s that.\\nAcknowledgements\\nThe core contributors to Klaxon have been Ivar Vong, Andy Rossback, Tom Meagher and Gabe Isman.\\nWe\\'ve been grateful for additional contributions to the project from:\\n\\nJackson Gothe-Snape, SBS News\\nCameo Hill\\nEmily Hopkins\\nYolanda Martinez\\nJeremy Merrill\\nRyan Murphy\\nJustin Myers\\nKevin Schaul\\nAri Shapell\\nJeremy Singer-Vine\\nMike Stucka\\nBob Weston\\n\\nWe also owe thanks to Knight-Mozilla OpenNews, which supported the initial public release of this free and open source software.\\n\\n\\n',\n",
       "  'language': 'Ruby'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nGeoIP2\\nA super simple, Node.js-based deployment for getting the location of a user with an IP address.\\nLicense\\nMIT\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTalk ·  · \\nOnline comments are broken. Our open-source commenting platform, Talk, rethinks how moderation, comment display, and conversation function, creating the opportunity for safer, smarter discussions around your work. Read more about Talk here.\\nBuilt with <3 by The Coral Project, a part of Vox Media.\\nGetting Started\\nCheck out our Quickstart and Install guides to get started with Talk in our Technical Docs.\\nProduct Guide\\nLearn more about Talk, including a deep dive into features for commenters and moderators, and FAQs in our Talk Product Guide.\\nPre-Launch Guide\\nYou’ve installed Talk on your server, and you’re preparing to launch it on your site. The real community work starts now, before you go live. You have a unique opportunity pre-launch to set your community up for success. Read our Talk Community Guide.\\nAdvanced Usage\\nFor advanced configuration and usage of Talk, check out our Configuration and Integration how-tos. This covers topics in which you will need dev support to fully customize and integrate Talk, such as SSO/authentication, creating and managing assets and articles, styling Talk with custom CSS, and setting up Notifications and SMTP support.\\nVersions & Upgrading\\nCheck our Releases page for the latest recommended release version. Releases All future even-numbered versions are considered stable LTS versions. We recommend the latest verified release for use in production environments.\\nMore Resources\\n\\nOur Blog\\nCommunity Guides for Journalism\\nMore About Us\\n\\nEnd-to-End Testing\\nTalk uses Nightwatch as our e2e testing framework. The testing infrastructure that allows us to run our tests in real browsers is provided with love by our friends at Browserstack.\\n\\nLicense\\nTalk is released under the Apache License, v2.0.\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ntx_salaries\\nThis Django application was generated using the Texas Tribune Generic\\nDjango app template.\\n\\nInstallation\\nYou can install this using pip like this:\\npip install tx_salaries\\n\\n\\nUsage\\ntx_salaries is meant to be used in conjunction with data received from\\nvarious departments around the state.  You must request this data yourself if\\nyou want to use tx_salaries.\\n\\nImporting Data\\nData is imported using the import_salary_data management command. You can run it in the salaries.texastribune.org repo once tx_salaries is properly installed like so:\\npython salaries/manage.py import_salary_data /path/to/some-salary-spreadsheet.xlsx\\n\\nData is imported using csvkit, so it can import from any spreadsheet format\\nthat csvkit\\'s in2csv understands.\\n\\nSetup\\n\\nPull master for both salaries.texastribune.org and tx_salaries\\n\\nGet rid of your old virtual environment for salaries:\\nrmvirtualenv <name of virtual env>\\n\\n\\nMake a new virtual environment:\\nmkvirtualenv <name of virtual env>\\n\\n\\nInstall the requirements:\\npip install -r requirements/local.txt\\n\\n\\nInstall your local tx_salaries into this:\\npip install -e ../tx_salaries\\n\\n\\nIf you\\'pipre using the local postgres database (which I recommend!), then you need to set that up. First set the DATABASE_URL:\\nexport DATABASE_URL=postgres://localhost/salaries\\n\\n\\nThen pull down the backup:\\nmake local/db-fetch\\n\\n\\nAnd load it:\\nmake local/db-restore\\n\\n\\n\\nNow, you should be good to work on tx_salaries like normal! If you have any transformers already in progress, you\\'ll need to merge master of tx_salaries into it. Don\\'t forget!\\n\\nStart the salaries.texastribune.org server\\nIn the terminal, go to the salaries.texastribune.org repo. While the transformers live in tx_salaries, all of the data management happens in the salaries.texastribune repo, and that\\'s where you\\'ll run these commands:\\nworkon <name of virtual env>\\nexport DATABASE_URL=postgres://localhost/salaries\\npython salaries/manage.py runserver\\n\\nCheck localhost:8000, should be up and running.\\n\\nWriting a New Transformer\\nThis section walks you through creating a new importer.  We\\'re going to use\\nthe fictional \"Rio Grande County\" (fictional in Texas at least).\\nTransformers require two things:\\n\\nAn entry in the TRANSFORMERS map in tx_salaries/utils/transformers/__init__.py\\nAn actual transformer capable of processing that file\\n\\nEntries in the TRANSFORMERS dictionary are made up of a unique hash that\\nserves as the key to a given spreadsheet and a callable function that can\\ntransform it.\\nTo generate a key, run the following command in the salaries.texastribune.org virtualenv:\\npython salaries/manage.py generate_transformer_hash path/to/rio_grande_county.xls --sheet=data_sheet --row=number_of_header_row\\n\\nThe output should be a 40-character string.  Copy that value and open the\\ntx_salaries/utils/transformers/__init__.py file which contains all of the\\nknown transformers.  Find the spot where rio_grande_county would fit in the\\nalphabetical dictionary in TRANSFORMERS and add this line:\\n\\'{ generated hash }\\': [rio_grande_county.transform, ],\\nIf the generated hash already exists with another transformer, provide a tuple with a text\\nlabel for the transformer and the transformer module like this:\\n\\'{ generated hash }\\': [(\\'Rio Grande County\\', rio_grande_county.transform),\\n                        (\\'Other Existing County\\', other_county.transform), ],\\nNote that the second value isn\\'t a string -- instead it\\'s a module.  Now you need to\\nimport that module.  Go up to the top of the __init__.py file and add an\\nimport:\\nfrom . import rio_grande_county\\nSave that file.  Next up, you need to create the new module that you just\\nreferenced.  Inside the tx_salaries/utils/transformers/ directory, create a\\nnew file call rio_grande_county.py  At the first pass, it should look like\\nthis:\\nfrom . import base\\nfrom . import mixins\\n\\nimport string\\n\\nfrom datetime import date\\n\\n# add if necessary: --sheet=\"Request data\" --row=3\\n\\nclass TransformedRecord(\\n    mixins.GenericCompensationMixin,\\n    mixins.GenericIdentifierMixin,\\n    mixins.GenericPersonMixin,\\n    mixins.MembershipMixin, mixins.OrganizationMixin, mixins.PostMixin,\\n    mixins.RaceMixin, mixins.LinkMixin, base.BaseTransformedRecord):\\n\\n    MAP = {\\n        \\'last_name\\': \\'LABEL FOR LAST NAME\\',\\n        \\'first_name\\': \\'LABEL FOR FIRST NAME\\',\\n        \\'department\\': \\'LABEL FOR DEPARTMENT\\',\\n        \\'job_title\\': \\'LABEL FOR JOB TITLE\\',\\n        \\'hire_date\\': \\'LABEL FOR HIRE DATE\\',\\n        \\'compensation\\': \\'LABEL FOR COMPENSATION\\',\\n        \\'gender\\': \\'LABEL FOR GENDER\\',\\n        \\'race\\': \\'LABEL FOR RACE\\',\\n        \\'compensation_type\\': \\'LABEL FOR FT/PT STATUS\\'\\n    }\\n\\n    # The order of the name fields to build a full name.\\n    # If `full_name` is in MAP, you don\\'t need this at all.\\n    NAME_FIELDS = (\\'first_name\\', \\'last_name\\', )\\n\\n    # The name of the organization this WILL SHOW UP ON THE SITE, so double check it!\\n    ORGANIZATION_NAME = \\'Rio Grande County\\'\\n\\n    # What type of organization is this? This MUST match what we use on the site, double check against salaries.texastribune.org\\n    ORGANIZATION_CLASSIFICATION = \\'County\\'\\n\\n    # Y/M/D agency provided the data\\n    DATE_PROVIDED = date(2013, 10, 31)\\n\\n    # How do they track gender? We need to map what they use to `F` and `M`.\\n    gender_map = {\\'Female\\': \\'F\\', \\'Male\\': \\'M\\'}\\n\\n    # The URL to find the raw data in our S3 bucket.\\n    URL = ( \\'http://raw.texastribune.org.s3.amazonaws.com/\\'\\n        \\'path/to/\\'\\n        \\'rio_grande_county.xls\\' )\\n\\n    @property\\n    def is_valid(self):\\n        # Adjust to return False on invalid fields.  For example:\\n        return self.last_name.strip() != \\'\\'\\n\\n    @property\\n    def person(self):\\n        name = self.get_name()\\n\\n        print self.gender_map[self.gender.strip()]\\n\\n        r = {\\n            \\'family_name\\': name.last,\\n            \\'given_name\\': name.first,\\n            \\'additional_name\\': name.middle,\\n            \\'name\\': unicode(name),\\n            \\'gender\\': self.gender_map[self.gender.strip()]\\n        }\\n\\n        return r\\n\\n    @property\\n    def compensation_type(self):\\n        comptype = self.get_mapped_value(\\'compensation_type\\')\\n\\n        if comptype.upper() == \\'FULL TIME\\':\\n            return \\'FT\\'\\n        else:\\n            return \\'PT\\'\\n\\n    @property\\n    def description(self):\\n        comptype = self.get_mapped_value(\\'compensation_type\\')\\n\\n        if comptype == \\'FT\\':\\n            return \\'Annual gross salary\\'\\n        elif comptype == \\'PT\\':\\n            return \\'Part-time, annual gross salary\\'\\n\\ntransform = base.transform_factory(TransformedRecord)\\nEach of the LABEL FOR XXX fields should be adjusted to match the\\nappropriate column in the given spreadsheet. If the file requires special\\nsheet or row handling, note the --sheet and --row flags as a comment\\nat the top of the file.\\nTransformedRecord now represents a generic record.  You may need to\\ncustomize the various properties added by the mixins or replace them with\\ncustom properties in other cases.  See the mixins for further documentation on\\nwhat they add.\\nThe last line generates a transform function that uses the TransformedRecord\\nthat you just created.  Now you\\'re ready to run the importer.\\nBack on the command line, run this in the salaries.texastribune.org repo:\\npython salaries/manage.py import_salary_data /path/to/rio_grande_county.xls\\n\\nPay attention to any error messages you receive. Most transformer errors are due\\nto missing data -- either the user didn\\'t map to all the necessary fields,\\ndidn\\'t include a mixin to process a field or made an error in an overridden\\nproperty that is supposed to return an attribute.\\nNote the generate_transformer_hash and import_salary data\\nmanagement commands can take --sheet and --row flags if the agency gave\\nyou a spreadsheet with multiple sheets or a header row that isn\\'t the first row.\\nCongratulations!  You just completed your first salary transformer.\\n\\nUnderstanding Transformers\\nTransformers are callable functions that take two arguments and return an array\\nof data to be processed.  At its simplest, it would look like this:\\ndef transform(labels, source):\\n    data = []\\n    for raw_record in source:\\n        record = dict(zip(labels, raw_record))\\n        # ... create the structure required ...\\n        data.append(structured_record)\\n    return data\\nThe data contained in the fictitious structured_record variable is a\\ndictionary that must look something like this:\\nstructured_record = {\\n    \\'original\\': ...,  # dictionary of key/value pairs for the data\\n    \\'tx_people.Identifier\\': ...,  # dictionary of attributes for the Identifier\\n    \\'tx_people.Organization\\': ...,  # dictionary of attributes for the Organization\\n    \\'tx_people.Post\\': ...,  # dictionary of attributes for the Post\\n    \\'tx_people.Membership\\': ...,  # dictionary of attributes for the Membership\\n    \\'compensations\\': [\\n        # first dictionary of compensation and type\\n        # should contain at least one, can contain as many as necessary\\n    ]\\n\\n}}\\nThat record is structured such that its keys and values match the models and kwargs\\nfor storing tx_people and tx_salaries models. How do spreadsheets get structured?\\nThe import_salary_data management command runs through several modules to store\\nspreadsheet data. First it uses transformer.`transform`_, which uses the header\\nrow to identify the transformer necessary to import the spreadsheet.\\nThat transformer turns each row of the spreadsheet into a structured record with\\nthe help of mixins.py and base.py. base.py defines the template of the\\nrecord, and mixins.py provides functions to format the required data. Mixins\\nare included in the definition of TransformedRecord. However, mixins cannot\\nhandle all situations, and sometimes fields like CompensationType require\\nspecial logic. You can override mixins by writing a custom @property in the\\ntransformer. Errors often happen at this stage when a transformer and its mixins\\nfail to provide all the fields required by base.\\nAfter each of the rows of the spreadsheet are converted to structured records,\\na list of records is sent to to_db.save(), which unpacks and stores the data.\\nimport_salary_data also keeps track of the unique organizations and positions\\nthat are imported so it can denormalize the stats when the import finishes.\\nThat\\'s a high-level view of transformers. Read the comments in mixins.py and\\ncheck out the data template in base.py for more details on the specific attributes\\ntransformers require.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': 'No readme file.', 'language': 'Dockerfile'},\n",
       " {'readme': 'No readme file.', 'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nWelcome to open source at the Texas Tribune\\nWe'll fill in this space  soon to highlight more of our projects but here's a few to get started:\\n\\nOur pixel tracking tool to help monitor republished articles\\nOur site/software to help nonprofits collect donations\\nA flexible filesystem API for use in the browser\\n\\nSupporters\\nOur work is genererously supported by BrowserStack and Auth0. BrowserStack helps us snuff out cross-browser issues before they reach the user. Additionally, the ability to test projects from localhost saves us lots of development time.\\n\\n\\n\\n\\n\",\n",
       "  'language': 'CSS'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\narmstrong.core.arm_wells\\nProvides the basic content well code necessary for scheduling and arranging\\nmodels inside Armstrong.\\nYou can use wells to change the order of content.  They are meant to be\\ncombined with querysets to allow your users to control what appears first in a\\nlist of content.\\n\\nUsage\\nWells are one of the most powerful components in Armstrong; they are\\nfundamentally about the ordering and structuring of content.\\nFrequently there are parts of the site that simply display the most recently\\npublished stories that belong to some grouping (a section or tag for\\ninstance), but when there are big stories, editorial staff wants to feature\\nthem by placing them at the top. Wells allow editorial staff to pin stories\\nthrough the admin. The demo project uses a QuerysetBackedWellView to accomplish\\nthis use case with the \\'front_page\\' WellType.\\narmstrong.core.arm_wells provides 3 primary objects to work with. The\\nhighest level is a WellType, which is generally a specific location on your\\nsite where you\\'d like to order content. Each WellType has a number of Well\\nobjects. A Well object represents a specific ordering of content for that\\nWellType for a certain period of time. Every Well has a number of Node objects\\neach of which relates the Well to an object in the database through a\\nGenericForeignKey.\\n\\nOrdering Arbitrary Content\\nThe simplest way to use wells is with just one Well object associated\\nwith a WellType. Changing the content of that Well or how that content is\\nordered will immediately change what is displayed on the site.\\nThe use of GenericForeignKeys to link Node objects to content gives you a lot\\nof flexibility. Wells make it easy to feature data apps, videos, audio clips,\\nphoto galleries and liveblogs all in one well even if there is no common base\\nclass for all their object.\\nThe challenge of laying out various content objects with different styling and\\nfields spurred the development of armstrong.core.arm_layout which provides\\na framework for specifying named layouts for your various content objects.\\nBy thinking of wells as just simple tools for ordering content, you\\'ll start\\nseeing other places on your site where they\\'re a good fit.  Anything that has\\nobjects in the database that the writing or editorial staff would like to\\nreorder on the site is a good candidate for wells.\\n\\nScheduling\\nThe other major aspect to Wells is scheduling. If the editorial staff wants to\\nplan the front page for Thursday at 5pm, they can create a new Well with the\\n\\'front_page\\' WellType and the content they want displayed. By setting the new\\nWell\\'s pub_date for 5pm Thursday, no action will need to be taken at that time,\\nthe site will just start using the new Well.\\nSimilarly Well\\'s have an expires field if content should only be scheduled for\\na certain period of time and then revert to an earlier well. We recommend that\\nevery WellType have an empty Well object that never expires to provide a sane\\nfallback.\\nThe WellManager has a convenience method, get_current, that takes in a WellType\\nname and fetches the Well associated with that WellType, has the most recent\\npub_date in the past and doesn\\'t have an expires in the past.\\n\\nUsing a QuerySetBackedWellView\\nWells can also be backed by a queryset which will be used as a source of\\nadditional content after all items have been exhausted. Currently, this is not\\nconfigurable via the admin, but can be easily accomplished by using the\\nQuerySetBackedWellView. For example, in a urls.py:\\nurl(r\\'^$\\', QuerySetBackedWellView.as_view(well_title=\\'front_page\\',\\n                                          template_name=\"index.html\",\\n                                          queryset=Article.published.all()),\\n        name=\\'front_page\\'),\\n# get\\'s the current \\'front_page\\' well, backs it with Article.published.all()\\n# and renders the index.html page\\n\\nTo render a well we recommend using the armstrong.core.arm_layout module.\\nThis will allow simple templates to handle heterogenous content. For instance,\\nto render every item in a well using the \\'standard\\' layout:\\n{% load layout_helpers %}\\n{% for content in well.items %}\\n    {% render_model content \\'standard\\' %}\\n{% endfor %}\\n\\n\\nAdmin Customization\\nThe admin view for modifying a well has been customized to provide a drag and\\ndrop interface for ordering Node\\'s. To add a Node to a well, use the\\nVisualSearch box to first search for a ContentType and then for the title of\\nthe object you want to use. For instance, in the demo, type \\'art\\' into the box\\nwhich will provide \\'ARTICLE\\', press enter and search for articles with \\'Perry\\' in\\nthe title. The VisualSearch widget is from armstrong.hatband, which\\nprovides more information about customizing that process.\\nWhen a node is added, it displays simple (and not particularly helpful)\\ninformation about it\\'s ContentType id and object id. It also sends a request to\\nthe server for the html result of a render_model call with the \\'preview\\' name.\\nThis allows you to easily display the nodes in a manner similar to how those\\nobjects will display on the site.\\n\\nInstallation & Configuration\\nYou can install the latest release of armstrong.core.arm_wells using pip:\\npip install armstrong.core.arm_wells\\n\\nMake sure to add armstrong.core.arm_wells to your INSTALLED_APPS.  You\\ncan add this however you like.  This works as a copy-and-paste solution:\\nINSTALLED_APPS += [\"armstrong.core.arm_wells\", ]\\n\\nOnce installed, you have to run either syncdb or migrate if you are\\nusing South.\\n\\nNote\\narmstrong.core.arm_wells requires the package django-reversion\\nwhich does not support multiple versions of Django.  Because of this,\\nwe can\\'t specify a version of django-reversion for you to use.  Please\\nconsult the wiki page to determine which version you should use.\\n\\n\\nContributing\\n\\nCreate something awesome -- make the code better, add some functionality,\\nwhatever (this is the hardest part).\\nFork it\\nCreate a topic branch to house your changes\\nGet all of your commits in the new topic branch\\nSubmit a pull request\\n\\n\\nState of Project\\nArmstrong is an open-source news platform that is freely available to any\\norganization.  It is the result of a collaboration between the Texas Tribune\\nand Bay Citizen, and a grant from the John S. and James L. Knight\\nFoundation.\\nTo follow development, be sure to join the Google Group.\\narmstrong.core.arm_wells is part of the Armstrong project.  You\\'re\\nprobably looking for that.\\n\\nLicense\\nCopyright 2011-2012 Bay Citizen and Texas Tribune\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nNote for non-Texas Tribune users: We\\'re happy to answer questions about this app, but there are no current plans to adopt a release schedule or offer official support. We will certainly be making improvements as time goes on, though, so check back.\\nDot\\nThe Texas Tribune\\'s pixel tracking tool.\\nHow it works\\nA tiny script creates a 1x1 image on the hosting page. Attached to the image src are query parameters with meta about that page. Thus, when the client makes a request for the src, it sends along those parameters and records them in a PostgreSQL database. This means that, in addition to knowing who\\'s republishing Texas Tribune stories, we can get a rough page-view count of those reprinted stories.\\nThe script tag\\n<script\\n  data-tt-canonical=\"<CANONICAL>\"\\n  integrity=\"<SRI_HASH>\"\\n  src=\"https://dot.texastribune.org/static/dist/dot.min.[WEBPACK_HASH].js\"\\n  crossorigin=\"anonymous\"\\n>\\n</script>\\n\\nThe data-tt-canonical attribute should be a full URL that includes the protocol, domain and path. For example, https://www.texastribune.org/2018/03/09/slug-goes-here or https://apps.texastribune.org/path-goes-here. The important thing is that it contain \"texastribune\" somewhere.\\nSee \"Building the tracker script\" below for more information about the SRI hash.\\nThe image request\\nIf we need to track views in an environment that doesn\\'t support JavaScript, it\\'s possible to use an image tag directly:\\n<img src=\"https://dot.texastribune.org/dot.gif?url=<URL>&canonical=<CANONICAL>&query=<QUERY>&ref=<REF>\">\\n\\nurl: The republished page\\'s URL (it must include the protocol, domain and path)\\ncanonical: The Texas Tribune URL (it must include the protocol, domain and path, as well as \"texastribune\")\\nquery: Any query parameters attached to the republished page\\'s URL\\nref: The referrer to the republished page\\n\\nEven if a value is blank, include it as an empty query parameter:\\n<img src=\"https://dot.texastribune.org/dot.gif?query=&ref=&url=http://www.foo.com&canonical=https://texastribune.org/03/08/2018/slug-here\">\\nWhat\\'s inside\\nBelow are the main technologies used in the app. The only thing you need installed on your machine is Docker. The build process, detailed below in \"setup,\" handles the rest.\\n\\nExpress\\nNode Postgres\\nVue.js\\nApollo\\nGraphQL\\nSentry/Raven (this could be removed or swapped with a different error-handling service)\\n\\nSetup\\nThese steps assume you have Docker installed on your machine.\\n\\nCreate an env file in the root and fill it out according to what\\'s in env.sample.\\nmake db-refresh -f Makefile.dev. This pulls down a copy of the production database for use locally. Check out this repository for info about how to set up a database export (assuming you\\'re using AWS/S3).\\nmake -f Makefile.dev. This will trigger a Docker build and drop you inside a container. All dependencies will be installed with Yarn.\\n\\nWhere stuff lives\\n\\nserver/: The Express configuration, including routing, middleware and error handling\\ntracker/: The source files for the tracking script. This includes a Webpack configuration.\\ndashboard/: The visual dashboard, powered by Vue.js and Apollo. This includes a Webpack configuration.\\ngraphql/: The GraphQL API, powered by Express and Apollo\\n\\nImportant URLs\\n\\n/dashboard: The visual dashboard.\\n/test: An HTML page containing a development build of the tracker script (meaning it will not insert rows into the production database). Useful for debugging [development only].\\n/dot.gif?<params>: The endpoint for retrieving the image src and inserting republisher info into the database.\\n/graphql: The API. Given it\\'s GraphQL, all endpoints use POST.\\n\\nCommands\\nDevelopment\\n\\nyarn run dash:dev: Start development mode for the dashboard. Webpack\\'s hot module replacement is enabled, meaning you can make changes to files in dashboard/* and see them update live at /dashboard.\\nyarn run tracker:dev: Fire up development mode for the tracker-script portion of the app (which you can test at /test). Changes to files in tracker/* or server/* should trigger automatic rebuilds.\\n\\nProduction\\nCommands\\n\\nyarn run start:prod: Start the server.\\nyarn run dash:prod:webpack: Build the production bundle for the dashboard.\\nyarn run tracker:prod:webpack: Build a new production version of the tracker script. See section below.\\n\\nBuilding the tracker script\\nTo give republishers SRI protection, production builds of the tracker script are:\\n\\nVersion controlled\\nUniquely hashed\\nBuilt locally\\n\\nIf you change anything in tracker/src/, run yarn run tracker:prod:webpack. Webpack will notice the bundle has changed and thus build a new uniquely hashed file into tracker/public/dist. Make sure to commit it and its source map.\\nAfter you\\'ve deployed the new script, put its URL into this website to get an integrity key.\\nFinally, update the pixel-tracker variables for texastribune.org.\\nWhy this process? SRI integrity keys become invalid if the file they refer to has changed. So every time we update the logic of the tracker script, it\\'s important to build a new version of it and pair it with a new integrity key. This means we should also never delete old builds of the pixel script.\\nDeployment\\nDeployment assumes you have a Makefile on the production server that pulls master and starts a Docker build. Makefile.example in this project demonstrates a way to do this.\\nLicense\\nMIT\\nTODOs\\n\\nUse Webpack to put vendor code into a separate bundle.\\nUse an ORM for GraphQL resolvers.\\nGive more detail to Sentry when error occurs.\\nUpgrade to Node v8.x\\n\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\n=======================================\\nConcurrency control with django-locking\\nDjango has seen great adoption in the content management sphere, especially among the newspaper crowd. One of the trickier things to get right, is to make sure that nobody steps on each others toes while editing and modifying existing content. Newspaper editors might not always be aware of what other editors are up to, and this goes double for distributed teams. When different people work on the same content, the one who saves last will win the day, while the other edits are overwritten.\\ndjango-locking provides a system that makes concurrent editing impossible, and informs users of what other users are working on and for how long that content will remain locked. Users can still read locked content, but cannot modify or save it.\\ndjango-locking makes sure no two users can edit the same content at the same time, preventing annoying overwrites and lost time. Find the repository and download the code at http://github.com/stdbrouw/django-locking\\ndjango-locking has only been tested on Django 1.2 and 1.3, but probably works from 1.0 onwards.\\nDocumentation\\nForked from the Django Locking plugin at stdbrouw/django-locking, this code features the cream of the crop for django-locking combining features from over 4 repos!\\nNew features added to this fork\\nChanges on change list pages\\nUnlock content object from change list page by simply clicking on the lock icon\\n\\n\\nHover over the lock icon to see when the lock expires\\n\\n\\nHover over the username by the lock icon to see the full name of the person who has locked the content object\\n\\n\\nConsolidated username and lock icon into one column on change list page\\nChanges in settings:\\nAdded Lock warning and expiration flags in terms of seconds\\nLock messages:\\nAdded options to reload or save the object when lock expiration message is shown\\n\\nImproved look and feel for the lock messages\\nLock messages fade in and out seamlessly\\nAdded much more detail to let users know who the content object was locked by providing the username, first name and last name\\nAdded lock expiration warnings\\nShows how much longer the object is locked for in minutes\\nLocking:\\nAdded hard locking support using Django\\'s validation framework\\n\\nSet hard and soft locking as the default to ensure the integrity of locking\\nAdded seamless unlocking when lock expires\\n\\nArchitecture:\\n1 model tracks lock information and that\\'s it!  No messy migrations for each model that needs locking.\\nRefactored and cleaned up code for easier maintainability\\nSimplified installation by coupling common functionality into base admin/form/model classes\\n10 Minute Install\\n\\n\\nGet the code:\\ngit clone git@github.com:RobCombs/django-locking.git\\n\\n\\nInstall the django-locking python egg:\\ncd django-locking\\nsudo python setup.py install\\n\\n\\nAdd locking to the list of INSTALLED_APPS in project settings file:\\nINSTALLED_APPS = (\\'locking\\',)\\n\\n\\nAdd the following url mapping to your urls.py file:\\nurlpatterns = patterns(\\'\\',\\n(r\\'^admin/ajax/\\', include(\\'locking.urls\\')),\\n)\\n\\n\\nAdd locking to the admin files that you want locking for:\\nfrom locking.admin import LockableAdmin\\nclass YourAdmin(LockableAdmin):\\nlist_display = (\\'get_lock_for_admin\\')\\n\\n\\nAdd warning and expiration time outs to your Django settings file:\\nLOCKING = {\\'time_until_expiration\\': 120, \\'time_until_warning\\': 60}\\n\\n\\nBuild the Lock table in the database:\\ndjango-admin.py/manage.py migrate locking (For south users. Recommended approach) OR\\ndjango-admin.py/manage.py syncdb (For non south users)\\n\\n\\nInstall django-locking media:\\ncp -r django-locking/locking/media/locking $your static media directory\\n\\n\\nNote: This is the step where people usually get lost.\\nJust start up your django server and look for the 200/304s http responses when the server attempts to load the media\\nas you navigate to a model change list/view page where you\\'ve enabled django-locking. If you see 404s, you put the media in the wrong directory!\\nYou should see something like this in the django server console:\\n[02/May/2012 15:33:20] \"GET /media/static/locking/css/locking.css HTTP/1.1\" 304 0\\n[02/May/2012 15:33:20] \"GET /media/static/web/common/javascript/jquery-1.4.4.min.js HTTP/1.1\" 304 0\\n[02/May/2012 15:33:20] \"GET /media/static/locking/js/jquery.url.packed.js HTTP/1.1\" 304 0\\n[02/May/2012 15:33:21] \"GET /admin/ajax/variables.js HTTP/1.1\" 200 114\\n[02/May/2012 15:33:21] \"GET /media/static/locking/js/admin.locking.js?v=1 HTTP/1.1\" 304 0\\n[02/May/2012 15:33:21] \"GET /admin/ajax/redirects/medleyobjectredirect/14/is_locked/?_=1335987201245 HTTP/1.1\" 200 0\\n[02/May/2012 15:33:21] \"GET /admin/ajax/redirects/medleyobjectredirect/14/lock/?_=1335987201295 HTTP/1.1\" 200 0\\nYou can also hit the media directly for troubleshooting your django-locking media installation:\\nhttp://www.local.wsbradio.com:8000/media/static/locking/js/admin.locking.js\\nIf the url resolves, then you\\'ve completed this step correctly!\\nBasically, the code refers to the media like so.  That\\'s why you needed to do this step.\\nclass Media:\\njs = ( \\'http://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.min.js\\', \\n     \\'static/locking/js/jquery.url.packed.js\\',\\n     \"/admin/ajax/variables.js\",\\n     \"static/locking/js/admin.locking.js?v=1\")\\ncss = {\"all\": (\"static/locking/css/locking.css\",)\\n}\\n\\nThat\\'s it!\\nChecking the installation\\nSimulate a lock situation -> Open 2 browsers and hit your admin site with one user logged into the 1st browser and\\nother user logged into the other.  Go to the model in the admin that you\\'ve installed locking for with one browser.\\nOn the other browser, go to the change list/change view pages of the model that you\\'ve installed django-locking for.\\nYou\\'ll see locks in the interface similar to the screen shots above.\\nYou can also look at your server console and you\\'ll see the client making ajax calls to the django server checking for locks like so:\\n[04/May/2012 15:15:09] \"GET /admin/ajax/redirects/medleyobjectredirect/14/is_locked/?_=1336158909826 HTTP/1.1\" 200 0\\n[04/May/2012 15:15:09] \"GET /admin/ajax/redirects/medleyobjectredirect/14/lock/?_=1336158909858 HTTP/1.1\" 200 0\\n\\nOptional\\nIf you\\'d like to enforce hard locking(locking at the database level), then add the LockingForm class to the same admin pages\\nExample:\\nfrom locking.forms import LockingForm\\nclass YourAdmin(LockableAdmin):\\n list_display = (\\'get_lock_for_admin\\')\\n form = LockingForm\\n\\nNote: if you have an existing form and clean method, then call super to invoke the LockingForm\\'s clean method\\nExample:\\nfrom locking.forms import LockingForm\\nclass YourFormForm(LockingForm):\\n  def clean(self):\\n    self.cleaned_data = super(MedleyRedirectForm, self).clean()\\n    ...some code\\n    return self.cleaned_data\\n\\nCREDIT\\nThis code is basically a composition of the following repos with a taste of detailed descretion from me. Credit goes out to the following authors and repos for their contributions\\nand my job for funding this project:\\nhttps://github.com/stdbrouw/django-locking\\nhttps://github.com/runekaagaard/django-locking\\nhttps://github.com/theatlantic/django-locking\\nhttps://github.com/ortsed/django-locking\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': 'No readme file.', 'language': 'No language specified.'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nyourls\\nSimple Docker container for YOURLS.\\nUsage\\nAvailable on Docker Hub as texastribune/yourls.\\n$ docker run \\\\\\n    -e YOURLS_DB_USER=root \\\\\\n    -e YOURLS_DB_PASS=supersecureyo \\\\\\n    -e YOURLS_DB_NAME=yourls \\\\\\n    -e YOURLS_DB_HOST=localhost \\\\\\n    -e YOURLS_DB_PREFIX=yourls_ \\\\\\n    -e YOURLS_SITE=http://pvlv.io \\\\\\n    -e YOURLS_COOKIEKEY=evenmoresecure \\\\\\n    -e YOURLS_USERS=adminusername:adminpassword \\\\\\n    -it texastribune/yourls\\n\\nThe server will be listening on port 80. You'll need to set up a MariaDB/MySQL instance before getting started. Check out config.php for a few more configuration options. Define a /root short-link to set the root redirect.\\nDevelopment\\n$ make build\\n$ make push\\n\\nLicense & Acknowledgements\\nYOURLS is released under the MIT license. This Docker image is released under the BSD 3-Clause license.\\nThis image is inspired by [pavlov/docker-yourls] https://github.com/texastribune/yourls.git) Thanks for being awesome. :)\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ncsvkit is a suite of command-line tools for converting to and working with CSV, the king of tabular file formats.\\nIt is inspired by pdftk, GDAL and the original csvcut tool by Joe Germuska and Aaron Bycoffe.\\nImportant links:\\n\\nDocumentation: http://csvkit.rtfd.org/\\nRepository:    https://github.com/wireservice/csvkit\\nIssues:        https://github.com/wireservice/csvkit/issues\\nSchemas:       https://github.com/wireservice/ffs\\n\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nA Pass to Poison\\nTexas industrial polluters rarely face sanctions when they spew noxious chemicals into the air during malfunctions and other unplanned incidents, exceeding the emission limits of their state-issued air permits.\\nA Texas Tribune analysis of self-reported industry data shows that thousands of rogue releases occur at Texas industrial sites each year. Companies are often let off the hook.\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nSold Out Landing\\nA build-out of a single page presentation of the Sold Out project.\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nLethal Injection Drugs in Texas\\nThe Texas Tribune's project tracking the state's supply of lethal injection drugs.\\nThis project was produced and is maintained by Jolie McCullough using the Texas Tribune's Data Visuals kit.\\nQuickstart\\nPlease note - some static assets required to make this project work are only accessible to Texas Tribune developers.\\nClone the project, then run npm install. Then pull down the assets with npm run assets:pull, and the data with npm run data:fetch. Use npm run serve to run the local development server.\\nNow, get to work!\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTexas Tribune Data Visuals Guides\\nHere you will find a collection of guides for doing various Data Visuals tasks. Find yourself needing to describe how to setup something for a project? Try to document it here instead so it can be linked to later!\\n\\n\\nTexas Tribune Data Visuals Guides\\n\\n\\nSet up your News Apps computer environment\\n\\n\\nLicense\\n\\n\\nSet up your News Apps computer environment\\nThere are a few things we have to do to get your computer ready for all the cool stuff you're going to build.\\nLicense\\nAll guides are licensed under a Creative Commons BY-NC 4.0 license.\\n\\n\\n\",\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTHIS APP IS NO LONGER MAINTAINED.\\nDependencies\\n\\nNode 6.11.3\\nYarn 0.27.5\\n\\nThese versions are pinned in Dockerfile, so you shouldn\\'t have to worry about it.\\nSet-up\\n\\nCreate an env-docker file in the root. Fill it out according to what\\'s in env.sample.\\nRun make. This will take a few minutes the first time you run it as Docker has to build the image from scratch.\\n\\nFiles and directories of note\\nsource/\\nContains files that Middleman will eventually process and move to the build/ directory.\\n\\nEverything in source/scss gets compiled into source/css, which is .gitignored.\\nsource/javascripts contains ES6 modules that are eventually bundled into source/javascripts/bundle.js, which is ignored from version control.\\n\\nutils/\\n\\ndeploy.sh: A shell script for deployment to S3.\\ncritical.js: A Node script for inlining critical CSS.\\n\\ndata/\\nContains YAML files, the data of which is fed into some ERB templates.\\nbrowserslist\\nTells Autoprefixer what browsers we care about.\\npostcss.config.js\\nTells Webpack what PostCSS stuff we want it to do.\\nCommands\\n\\nyarn run dev: Fire up the development server. This will enable live reloading of templates, JavaScript and CSS.\\nyarn run build: Build for production.\\nyarn run clean: Clean out the build/ and .tmp/ directories.\\nyarn run middleman: Do the official Middleman build process.\\nyarn run critical: Inline critical CSS.\\nyarn run js:dev: Put Webpack in watch mode.\\nyarn run js:prod: Build the Webpack bundle for production.\\nyarn run deploy: Deploy to S3.\\nyarn run build:deploy: Build for production, then deploy.\\n\\nYou\\'ll typically only need these three: yarn run dev, yarn run build and yarn run deploy.\\nDeployment\\n\\nBuild for production: yarn run build\\nPush to S3: yarn run deploy (this also automatically busts the FB OG cache)\\n\\nYou can combine the build and deploy into one step with yarn run build:deploy.\\nCampaign IDs\\nMembership uses unique IDs to track specific campaigns (FMD, Giving Tuesday, etc.). If a campaign is in progress, we hard-code that ID into the donation-widget form (see markup below). This is to ensure every click of \"Go To Checkout\" passes the ID onto our checkout app.\\n<input id=\"campaign-id\" type=\"hidden\" value=\"<campaign-id>\" name=\"campaignId\">\\nIf a campaign is not in progress, that hidden field should not be present. However, we still want to pass campaign IDs to checkout if campaignId is included as a query parameter. (For campaigns, our membership folks create special URLs to support.texastribune.org that include that query parameter.) That\\'s why this code exists. The thinking is we want to ensure remnant donations spurred by campaign marketing materials still get recorded as part of that campaign, even if the date of giving is after we\\'ve removed the hidden field.\\nBrowser support\\nWe autoprefix for everything in browserslist but officially only support modern browsers plus Internet Explorer 9.\\n\\n\\n',\n",
       "  'language': 'HTML'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTexas Tribune Style Guides\\nThis is the project repo for the Texas Tribune style guides, living documents of Texas Tribune sites and platforms, including TribTalk, the Texas Tribune festival, and more. Check out the style guides on GitHub pages here.\\nGetting Started\\nTo add to the style guides, clone down the project repo.\\nYou'll need the Ruby gem bundler. To install the bundler, run:\\ngem install bundler\\n\\nInstall the necessary gems for the style guides from the Gemfile by running:\\nbundle install\\n\\nGrunt is used to watch and compile CSS files from Sass. Grunt requires that Node/npm be installed. Before installing Grunt, be sure the latest version of Node is installed. Using Homebrew is the recommended option here:\\nbrew install node\\n\\nTo install Grunt, run:\\nnpm install -g grunt-cli\\n\\nOnce Grunt is installed, install the necessary Node dependencies by running:\\nnpm install\\n\\nRunning Grunt Tasks\\nUse Grunt to compile the Sass into CSS by running:\\ngrunt build\\n\\nUse Grunt while developing to compile Sass into CSS, start a local server on\\nport 4000 and then watch by running:\\ngrunt dev\\n\\nBy default, compile Sass into CSS and compile the jekyll site by running:\\ngrunt\\n\\nContributing and deploying\\nContent for the project is stored on the gh-pages branch.\\nTo contribute to style guides, create a new branch, make your changes there, and submit a PR. When the PR is approved, merge it into the gh-pages branch. Then push the updated gh-pages branch to GitHub, and the new content will be deployed.\\nAdding a new style guide\\nTo add a new style guide to the project, add the Sass for the project to the _sass directory. Each style guide's Sass should go into its own appropriately named directory. Then, add a new HTML page to the root directory, following the established naming convention. Update the Gruntfile to build Sass for and watch the new project's style guide.\\nAdditional Resources\\nThe Jekyll docs provide a helpful overview on Jekyll and deploying GitHub pages.\\n\\n\\n\",\n",
       "  'language': 'CSS'},\n",
       " {'readme': \"\\n\\n\\n\\n        readme.md\\n      \\n\\n\\nBull's Eye analysis\\nThe files used to run the analysis for our Bull's Eye story.\\nTo run the processing tasks:\\nsh process.sh\\nThe first task merges Texas census tract shapefiles with population csvs. The raw data is in the raw_data directory. The shapefiles are made available by the U.S. Census. The population counts can be found on the American FactFinder.\\nThe second task filters out just the columns we want. We run these tasks for 2000, 2010, 2011, 2012, 2013 and 2015.\\nAll the merged data is put into the edits directory.\\nWe've also included QGIS file with a choropleth map made out of some of the shapefiles. That's within the qgis directory.\\n\\n\\n\",\n",
       "  'language': 'Shell'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nAnalysis of CD 27\\nThis repo contains data, code and analyis supporting the Texas Tribune's redistricting project on Congressional Districit 27.\\nAll the raw data is located in the data directory. All the edits that we made to the files are in the edits directory. All the topojson files for the maps are in the output directory.\\nThe analysis files are within the shell directory. Here's a quick run through of what's in each:\\n\\n\\nprocess.sh: We downloaded shapefiles of all of Texas's congressional districts going back to 1982 -- the year CD-27 was created -- from the Texas Legislative Council. This bash file simplifies those and turns them into topojoson files. It does the same for the county shapes, only we pull out just Nueces for them.\\n\\n\\nprocess-tracts.sh: This first pulls out CD-27 from the 2010 and 2012. It then finds all of the tracts that intersect with CD-27 in those years and merges Hispanic data into those shapefiles. Finally it converts to topojson for use with D3. These are used in the maps with the Hispanic population overlays. Tracts are made available by the U.S. Census.\\n\\n\\nprocess-results.sh: This takes voting district data for 2008, 2010, 2012 and 2014 and merges it with voting district shapefiles. It then does some math to decide which party won the voting district, depending on who got the most votes. Finally, it converts to topojson. The data is from the Texas Legislative Council.\\n\\n\\n\\n\\n\",\n",
       "  'language': 'Shell'},\n",
       " {'readme': \"\\n\\n\\n\\n        readme.md\\n      \\n\\n\\nHard to count analysis\\nThis repository includes the code we ran to get the data for our hard to count story.\\nThe raw data is made available by the U.S. Census, which releases low response scores for each tract in the United States. The scores are based on the likelihood of those living within a census tract will not respond to mailed questionnaires. The national average is 20.7. Scores in Texas range from 0 to 45.\\nWith this data, we first filtered out just Texas. We then downloaded census tract shapefiles and merged the shapefile with the response score data.\\nWe then downloaded urban area shapefiles and filtered out the 25 most-populous urban areas in Texas. We then ran a query using ogr2ogr to find all the census tracts that intersect with those 25 urban areas. This allowed us to find the census tracts in those urban areas and create the urban area dropdown in the first chart.\\nAfter the merge, we exported the results as a CSV, which was used to make the D3 charts. This is located in the output folder.\\nThe code for these tasks can be found here:\\n\\nprocess.sh\\n\\nTo help with the descriptions for these charts, we determined the percentage of Hispanic residents and poor residents who live in census tracts where the low response score is above the national average. We did this for the state of Texas, as well as Houston and Austin. We exported the results as a CSV to the output folder.\\nWe also determined how many cenus tracts are in each county for the chart's tooltip and exported those results as a CSV.\\nThe code for these tasks can be found here:\\n\\nanalysis.r\\n\\n\\n\\n\",\n",
       "  'language': 'Shell'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nWho's at the door?\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nSupported tags and respective Dockerfile links\\n\\n2, 2.1, 2.2, latest (Dockerfile)\\n\\noauth2_proxy on Docker\\nThis repository holds a build definition and supporting files for building a Docker image to run oauth2_proxy.\\nIt is published as automated build machinedata/oauth2_proxy on Docker Hub.\\nWhat is oauth2_proxy?\\noauth2_proxy is a reverse proxy and static file server that provides authentication using Providers (Google, GitHub, and others) to validate accounts by email, domain or group.\\nYet another oauth2_proxy container?\\nNot quite:\\n\\nBased on the official Alpine Linux image - super slim and lightweight.\\nNo magic. Straight config that follows upstream. Simple and clean configuration via environment variables or config file.\\nImage follows Dockerfile best practices (dropping root privileges, PID1 for proper signalling, logging,...)\\n\\nQuickstart\\nTo be able to start oauth2_proxy you need to configure an OAuth Provider first. Instructions for Google and others are found on the oauth2_proxy website. Note your client-id and client-secret.\\nIn the minimal configuration you also need to specify the upstream you are protecting.\\n$ docker run -d -p 4180:4180 \\\\\\n    -e OAUTH2_PROXY_CLIENT_ID=... \\\\\\n    -e OAUTH2_PROXY_CLIENT_SECRET=... \\\\\\n    -e OAUTH2_PROXY_UPSTREAM=... \\\\\\n    machinedata/oauth2_proxy\\nEnvironment variables\\nIt is very easy to configure oauth2_proxy via environment variables. If no config file is present, the docker-entrypoint.sh script will create one based on the passed environment variables.\\n\\n\\nOAUTH2_PROXY_CLIENT_ID: the OAuth Client ID: ie: \"123456.apps.googleusercontent.com\"\\n\\n\\nOAUTH2_PROXY_CLIENT_SECRET: the OAuth Client Secret\\n\\n\\nOAUTH2_PROXY_COOKIE_SECRET: the seed string for secure cookies. To generate a strong cookie secret just run python -c \\'import os,base64; print base64.b64encode(os.urandom(18))\\'.\\n\\n\\nOAUTH2_PROXY_EMAIL_DOMAIN: authenticate emails with the specified domain (may be given multiple times). The default is \"*\" and will authenticate any email.\\n\\n\\nOAUTH2_PROXY_UPSTREAM: the http url(s) of the upstream endpoint or file:// paths for static files. Routing is based on the path\\n\\n\\nYou can pass any variable that is specified on the command line options documentation.\\n\\nOAUTH2_PROXY_APPROVAL_PROMPT\\nOAUTH2_PROXY_AUTHENTICATED_EMAILS_FILE\\nOAUTH2_PROXY_AZURE_TENANT\\nOAUTH2_PROXY_BASIC_AUTH_PASSWORD\\nOAUTH2_PROXY_CONFIG\\nOAUTH2_PROXY_COOKIE_DOMAIN\\nOAUTH2_PROXY_COOKIE_EXPIRE\\nOAUTH2_PROXY_COOKIE_HTTPONLY\\nOAUTH2_PROXY_COOKIE_NAME\\nOAUTH2_PROXY_COOKIE_REFRESH\\nOAUTH2_PROXY_COOKIE_SECURE\\nOAUTH2_PROXY_CUSTOM_TEMPLATES_DIR\\nOAUTH2_PROXY_DISPLAY_HTPASSWD_FORM\\nOAUTH2_PROXY_GITHUB_ORG\\nOAUTH2_PROXY_GITHUB_TEAM\\nOAUTH2_PROXY_GOOGLE_ADMIN_EMAIL\\nOAUTH2_PROXY_GOOGLE_GROUP\\nOAUTH2_PROXY_GOOGLE_SERVICE_ACCOUNT_JSON\\nOAUTH2_PROXY_HTPASSWD_FILE\\nOAUTH2_PROXY_HTTP_ADDRESS\\nOAUTH2_PROXY_HTTPS_ADDRESS\\nOAUTH2_PROXY_LOGIN_URL\\nOAUTH2_PROXY_PASS_ACCESS_TOKEN\\nOAUTH2_PROXY_PASS_BASIC_AUTH\\nOAUTH2_PROXY_PASS_HOST_HEADER\\nOAUTH2_PROXY_PROFILE_URL\\nOAUTH2_PROXY_PROVIDER\\nOAUTH2_PROXY_PROXY_PREFIX\\nOAUTH2_PROXY_REDEEM_URL\\nOAUTH2_PROXY_REDIRECT_URL\\nOAUTH2_PROXY_RESOURCE\\nOAUTH2_PROXY_REQUEST_LOGGING\\nOAUTH2_PROXY_SCOPE\\nOAUTH2_PROXY_SIGNATURE_KEY\\nOAUTH2_PROXY_SKIP_AUTH_REGEX\\nOAUTH2_PROXY_SKIP_PROVIDER_BUTTON\\nOAUTH2_PROXY_TLS_CERT\\nOAUTH2_PROXY_TLS_KEY\\nOAUTH2_PROXY_VALIDATE_URL\\n\\nConfiguration file\\nThe container is configured to start oauth2_proxy with /config/oauth2_proxy.cfg as config file.\\nIf a config file is mounted (preferably read-only), the OAUTH2_PROXY_ environment variables will be ignored. Use the example config to start:\\n$ curl -O https://raw.githubusercontent.com/bitly/oauth2_proxy/master/contrib/oauth2_proxy.cfg.example\\n$ mv oauth2_proxy.cfg.example oauth2_proxy.cfg\\n$ sed -i -e \"s/# http_address = .*/http_address = \\\\\"0.0.0.0:4180\\\\\"/\" oauth2_proxy.cfg.example\\n$ docker run -d \\\\\\n             -v $(pwd)/oauth2_proxy.cfg.example:/config/oauth2_proxy.cfg:ro \\\\\\n             -p 4180:4180 machinedata/oauth2_proxy\\nVolumes\\n\\n/templates: Path to place custom templates sign_in.html and error.html. You also need to set custom-templates-dir via config file or the OAUTH2_PROXY_CUSTOM_TEMPLATES_DIR environment variable.\\n\\nPorts\\n\\n4180: The default port where oauth2_proxy is listening. Can be changed via http-address (and/or https_address) setting and corresponding OAUTH2_PROXY_ environment variable.\\n\\nLegal\\noauth2_proxy is a creation of bitly and was renamed from Google Auth Proxy in May 2015.\\nIt is licensed under the MIT license.\\ndocker-oauth2_proxy is licensed under the Apache 2.0 license, was created by Jodok Batlogg.\\nCopyright 2016-2018 Crate.io, Inc..\\nContributing\\nThanks for considering contributing to docker-oauth2_proxy!\\nThe easiest way to contribute is either by filing an issue on Github or to fork the repository to create a pull request.\\nIf you have any questions don\\'t hesitate to join us on Slack.\\n\\n\\n',\n",
       "  'language': 'Shell'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTribune Code Grabber\\nThis repo uses the latest version of the Data Visuals app kit.\\nTo start run, npm install. This project requires Node v4.0.0. If you hit a syntax error on your initial attempts to serve the project locally, it\\'s likely that you\\'re trying to use an older version of node. Update node, completely remove your node_modules folder and re-run npm install.\\nInstructions for adding or editing a component\\nThe HTML for each component in Code Grabber is built using a Nunjucks {% macro %} called {{ copyBlock(id, formOptions, preview) }} located in app/templates/layouts/macros.html. It takes three arguments:\\n\\nid: A string to identify the component and the targeted clipboard. For customizable components, it\\'s also used to locate the form in main.js and set its behaviors.\\nformOptions: The template will add a <form> tag around the options.\\npreview: Not required. If set to \\'load\\', the template will include a dashed frame to preview the component.\\n\\nEach group of components has a file in app/templates/includes/ in which the parameters for these templates are set and then called. For example, here\\'s how the {{ copyBlock() } is used in the read_more.html file:\\n<!-- formOptions  -->\\n{% set readmoreform %}\\n  <label>Headline</label>\\n  <input type=\"text\" id=\"readmore_headline\" value=\"\" required>\\n  <label>Link to the story</label>\\n  <input type=\"text\" id=\"readmore_link\" value=\"\" required>\\n{% endset %}\\n\\n<!-- id, formOptions, preview -->\\n{{ macro.copyBlock(\\'readmorecode\\', readmoreform, preview=\\'load\\')}}\\n\\n\\nEach component has a function to build its codeBlock. For some components (readmore and twitterinline) that function is also used to initialized a preview on load, but for most, it\\'s just called when the component\\'s form is submitted. When the user submits the form, it picks up the variables, calls that component function with the variables to build a new codeBlock, then runs returnCode(codeBlock, id). (The id must match the id assigned to the component in {% copyBlock() %}). The returnCode() function uses the id to push the updated codeBlock to the DOM in both the preview frame and the  block for that component, and then triggers a hidden clipboard copy button for that component. Afterwards, it triggers copied(this.id) to show the Copied! tooltip.\\n// build readmore codeBlock\\nfunction readmore(headlineSlug, link, headline) {\\n  var codeBlock = \\'<p class=\"readmore\" style=\"font-style: italic; padding-top: .5em; padding-bottom: .5em; vertically-align: middle;\"><span class=\"readmore--label\" style=\"color: #111111; font-family: Helvetica,Arial,sans-serif; font-size: .9em; font-style: italic; font-weight: 800; margin: 0 1em 1em 0; text-decoration: none; text-transform: uppercase;\">Read More</span><a onclick=\"ga(\\\\\\'send\\\\\\', \\\\\\'event\\\\\\', \\\\\\'codegrabber\\\\\\', \\\\\\'click\\\\\\', \\\\\\'readmore\\\\\\', \\\\\\'\\' + headlineSlug + \\'\\\\\\', {\\\\\\'nonInteraction\\\\\\': 1})\" class=\"readmore_link\" href=\"\\'+ link +\\'\">\\'+ headline +\\'</a></p>\\';\\n\\n  return codeBlock;\\n}\\n\\n// submit readmore form\\n$(\\'#readmorecode_form\\').submit(function(e) {\\n  var headline = $(\\'#readmore_headline\\').val(),\\n      link = $(\\'#readmore_link\\').val(),\\n      headlineSlug = slugify(headline),\\n      // assign output from readmore() to codeBlock\\n      codeBlock = readmore(headlineSlug, link, headline);\\n\\n  // update DOM & copy element\\n  returnCode(codeBlock, \\'readmorecode\\');\\n\\n  // provide user feedback\\n  copied(this.id);\\n\\n  e.preventDefault();\\n});\\n\\nDevelopment\\nRun the following command to start the development server:\\nnpm run serve\\nWebpack\\nThis kit uses the webpack module bundler.\\nConnect to S3\\nTo use the commands to deploy your project to Amazon S3, you\\'ll need to add a profile to your ~/.aws/config. It should look something like this:\\n[profile newsapps]\\naws_access_key_id=YOUR_UNIQUE_ID\\naws_secret_access_key=YOUR_SECRET_ACCESS_KEY\\n\\nDeployment\\nRun these commands to build and deploy:\\nnpm run build\\nnpm run deploy\\n\\nThe project will deploy using the S3 bucket and slug found in your config.js.\\nGoogle Sheets\\nCurrently, the 2016 Festival Talent lineup is brought into the project through a Google Doc. If this is your first time using Google sheets with a Data Visuals Kit, you\\'ll need to authorize your computer by saving a file named .tt_kit_google_client_secrets.json in your root directory. You can get the specific contents of that file from a fellow teammate.\\nThe config.js is already set up to pull in the information. If you need to make changes or add another Google sheet or document, this is where you do it. To update the data pulled into the project, run npm run data/fetch. You\\'ll see an updated .json file in the data folder, which you can then reference in your app/templates.\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nThe Texas Higher Education Data Project\\n\\nA very rough guide to starting development\\nExample .env file for environment variables:\\nDJANGO_SETTINGS_MODULE=exampleproject.settings.dev\\nDATABASE_URL=postgis:///tx_highered\\n\\nComplete guide to getting started (remove steps to suit you):\\n# install postgresql libpq-dev\\n\\ngit clone $REPOSITORY && cd $PATH\\nmkvirtualenv tx_higher_ed\\nsetvirtualenvproject\\nadd2virtualenv .\\npip install -r requirements.txt\\n\\n# if you need to create a database:\\n# `postdoc` greatly simplifies connecting to Docker databases\\npip install postdoc\\nphd createdb --encoding=UTF8 -T template0\\necho \"CREATE EXTENSION postgis;\" | phd psql\\necho \"CREATE EXTENSION postgis_topology;\" | phd psql\\n\\n# or if you need to reset your database:\\nmake resetdb\\n\\n# syncdb and load fixtures\\nmake syncdb\\n\\n#######################################################################\\n# You can stop at this point if you\\'re just playing with the project. #\\n#######################################################################\\n\\n# if using 2012 data, bump it up to 2014 standards\\npython tx_highered/scripts/2014_update.py\\n\\n# get ipeds data, requires https://github.com/texastribune/ipeds_reporter\\n../ipeds_reporter/csv_downloader/csv_downloader.py \\\\\\n  --uid data/ipeds/ipeds_institutions.uid --mvl data/ipeds\\nmv ~/Downloads/Data_*.csv data/ipeds\\n# get thecb data\\ncd data && make all\\n# load data\\n#   timing: 10m25.069s\\nmake load\\n# post-process the data\\npython exampleproject/manage.py tx_highered_process\\n\\n\\n####################################\\n# placeholder for post-2014 update #\\n####################################\\n# the 2012->2014 specific stuff can go out and the above importing\\n# instructions can get updated\\nDatabase\\nThis project currently requires a PostGIS database (hopefully not for long):\\n$ phd createdb\\n$ phd psql\\n\\nCREATE EXTENSION postgis;\\nCREATE EXTENSION postgis_topology;\\nMoving data between databases\\nYou can do a sql dump to move data from one postgres database to another\\n(excluding geo info):\\n$ phd SOURCE_DATABASE_URL pg_dump --no-owner --no-acl --table=tx_highered* --clean > tx_highered.sql\\n$ phd DEST_DATABASE_URL psql -f tx_highered.sql\\nAfter deploy\\n\\nFreeze the current data in a fixture\\n\\nEdit the tx_highered_YYYY.json.gz make task\\nRun the task to save the data\\n\\n\\nAdjust the loading scripts to reference the new fixture\\nDeprecate (or delete) any one-time data migration scripts, e.g.\\n2014_update.py won\\'t be necessary after 2015\\n\\nGetting Data from the IPEDS Data Center\\nWhen it asks you for an Institution, enter a list of UnitIDs generated by:\\nlist(Institution.objects.filter(ipeds_id__isnull=False).values_list(\\'ipeds_id\\', flat=True))\\n\\nGetting Data from the Texas Higher Education Coordinating Board\\nIf you want to regrab data from THECB\\'s web site, first find the data file that you want to re-grab.\\nIt will be named something like \"top_10_percent.html\". There will also be a file called \"top_10_percent.POST\". From that file you can recreate the report with the command:\\ncurl -X POST -d @top_10_percent.POST http://www.txhighereddata.org/interactive/accountability/InteractiveGenerate.cfm -s -v > blahblahblah.html\\n\\nIf you need to modify the report, you can reverse engineer it from the POST data and the form markup.\\n(c) 2012 The Texas Tribune\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTexas Legislative Districts\\nA reusable Django app for working with Texas legislative districts.\\nUsage\\nAdd tx_lege_districts to your INSTALLED_APPS. Then configure a GIS-enabled\\ndatabase and load the districts you want from a fixture:\\npython manage.py loaddata districts_2006\\n\\nHookup the urls in the tx_lege_districts namespace:\\nurlpatterns = patterns('',\\n    url('^lege\\\\-districts/', include('tx_lege_districts.urls',\\n            namespace='tx_lege_districts')),\\n)\\n\\nRepresentatives\\nDistricts provide a representative property that is None by default.\\nThe representative can be handled in your own project using a a configurable backend:\\n# myapp/backends.py\\nclass MyBackend(object):\\n    def get_representative(self, district):\\n        return MyRepresentativeModel.objects.get_for_district(district)\\n\\n# settings.py\\nTX_REPRESENTATIVE_BACKENDS = ['myapp.backends.MyBackend']\\n\\nTesting\\nSet the DATABASE_URL environment variable. Example:\\nexport DATABASE_URL=postgis:///tx_lege_districts\\n\\nChange template1 to be postgis enabled:\\npsql template1\\n\\nCREATE EXTENSION postgis;\\nCREATE EXTENSION postgis_topology;\\n\\nJust use DROP EXTENSION if you want to go back.\\nRun the test runner:\\npython runtests.py\\n\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nHoliday Hack Day\\nLet's design for voice!\\nSet-up\\n\\nMake sure you have an AWS account\\nMake sure you have an Amazon Developer account\\nInstall the Alexa Skill Kit CLI: npm install -g ask\\n\\nLearning how to build a skill\\nFor starters, I recommend following this tutorial. It shows how to build a skill interface in the Amazon Developer console and then connect app logic to it from AWS Lambda. We'll walk through this tutorial together in the morning.\\nActually building a skill\\nNode.js\\n\\nThe alexa-skills-kit-sdk-for-nodejs is probably the easiest way to get up and running. The README is quite thorough, and deployment to Lambda is made easy. Here are some sample skills created with this boilerplate:\\nskill-sample-nodejs-fact\\nskill-sample-nodejs-hello-world\\nskill-sample-nodejs-highlowgame\\nskill-sample-nodejs-howto\\nskill-sample-nodejs-trivia\\n\\nPython\\n\\nflask-ask\\npython-alexa\\nPython 3.6 + Alexa tutorial\\n\\nOther helpful resources:\\n\\nalexa-cookbook\\n\\n\\n\\n\",\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nAnalysis of maternal mortality data\\nThis repo contains data, code and analyis supporting the Texas Tribune's maternal mortality project. We used two main analysis in the story:\\n\\nThe mortality and prenatal rates in rural counties v. urban counties in Texas for 2010-14.\\nThe mortality and prenatal rate for counties with at least one hospital with an obstetric bed v. those that did not have one. This was for 2014.\\n\\nInstallation\\nThis project uses pipenv to manage dependencies. Once you have that set up, install the dependencies.\\npipenv install --three\\nOptional: Install extensions\\njupyter contrib nbextension install --user\\nOptional: Activate extensions\\njupyter nbextensions_configurator enable --user\\nThen you can run Jupyter notebooks.\\npipenv run jupyter notebook\\nNotebooks\\nSeveral Juypter notebooks were used to run the analysis. Here's a breakdown of each one:\\n01-population:\\n\\nThis is used to convert race data for each county into usable csv files. Race data probably won't be used in the initial story.\\n\\n01-prenatal:\\n\\nThis first takes prenatal data, which is broken down by year for 2010 to 2014 and sums it all. It also calculates how many eligible pregnant women there are for each county because all that is provided is the number of women who received and what percentage they were of all women.\\nIt then merges that data with race data, which probably won't be used in the initial story.\\nFinally, sums all the race data, which probably won't be used.\\n\\n01-prenatal-race:\\n\\nThis combines prenatal data broken down by race with each county's urban/rural designation. Race data probably won't be used in the initial story.\\n\\n02-mortality-race:\\n\\nThis merges five years of mortality and race data into one csv, with a row for each county. Race data probably won't be used in the initial story.\\nIt then merges the mortalty rates for each county with race data, which probably won't be used in the initial story.\\nIt then sums up each race and calculates the rate for each race, which probably won't be used in the initial story.\\nThose files now have maternal rates and race information for each county. It is combined with the county's urban/rural designation to determine if the county is rural or urban. It's also combined with border information to determine if the county is on the border or not.\\n\\n03-ob-hospitals:\\n\\nThis first joins hospital obstetric data with the spreadsheet that has the mortality rate, race data and urban/rural designation for each county. The hosital obstetric data is a spreadsheet that has each county listed and whether or not they have a hospital with an obstetric bed in the county. This sheet was created by hand, using the 2014 hospital obstetric reports.\\nPrenatal data is then added to the sheet that was just created.\\n\\n04-output:\\n\\nThis is where the actual calculations are run. We combine the urban and rural counties together and figure out mortality and prenatal rates, as well as race data, for each. We do the same for counties that have hospitals that have obstetric beds to those that don't. The final files are put in the input directory.\\n\\nData\\nTexas maternal mortality task force\\nThe task force studied maternality mortality across the state and put together a report. Data is current as of 2014. We were also emailed a new report in October 2017 that includes 2012-15 mortality rates, as well as causes of death and more.\\nTexas vital statistics\\nEvery year, Texas DSHS puts out vital statistics, which includes maternal mortality rates, prenatal care numbers, etc. Data is current as of 2014. Additionally, we were emailed some 2015 data, including prenatal rates.\\nCounty designations\\nTexas DSHS classifies counties based on whether they are rural or urban and border or non-border. Data is current as of 2013.\\nHospital obstetric reports\\nEvery year, DSHS puts out a hospital obstetric report, which tracks how many obstetric beds each hospital in Texas has. The 2014 report was emailed to us.\\nPRAMS\\nThis survey gives us postpartum care numbers for Texas. Data is current as of 2014.\\nAmerican Fact Finder\\nThis tool from the US Census Bureau provides demographic data. We're using 2015 ACS 5-year estimates.\\nAssociated Reporters\\n\\nMarissa Evans mevans@texastribune.org\\nChris Essig cessig@texastribune.org\\nBenjamin Din bdin@texastribune.org\\n\\n\\n\\n\",\n",
       "  'language': 'Jupyter Notebook'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTT Content Analytics\\nWhat is this?\\nScripts and a Dockerfile for running analytics. Currently contains two main scripts that we run at once: content analytics, and search analytics.\\nWhy the scripts?\\nContent analytics give regular updates on the quantity and quality of our content; how many stories get published, who they're authored by, how they're tagged, and so on. This script is subject to change if we decide we want different data or formats.\\nSearch analytics display query terms that users (and bots) have searched for on our main site. This is also subject to change if we decide to add richer data, such as metadata on the user and context.\\nWhy the Dockerfile?\\nThese scripts hook up to Google Docs (in order to upload analytics spreadsheets) and Slack (in order to post them to the #analytics channel). So the Dockerfile sets up an image that includes python packages to help with the Google and Slack auth and integrations.\\nHow can I use it?\\nYou can use it as-is, but you need a couple things:\\n\\nSupply Google service account credentials in the form of a JSON file that lives at /etc/ssl/certs/tt-googledrive-credentials.json inside Docker. For more details about the service account JSON file, see their docs (the Trib's credentials live in Rundeck).\\nSupply the Tribune's Scalyr API token as a Docker env var, which you can find at https://www.scalyr.com/keys, or in our Rundeck/Ansible repo.\\n\\nOnce you have the creds, you can run the Docker like so:\\ndocker pull texastribune/tt-content-analytics\\nexport SCALYR_API_TOKEN='<my-api-key-here>'\\ndocker run --rm --env=SCALYR_API_TOKEN --volume=/path/to/your/credentials.json:/etc/ssl/certs/tt-googledrive-credentials.json texastribune/tt-content-analytics\\n\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ndjangocms-text-ckeditor\\nText Plugin for django-cms with CK-Editor.\\nThe latest version of this package supports:\\n\\nDjango >= 1.8\\ndjango CMS >= 3.3\\n\\n\\nWarning\\n\\nFor django CMS 3.4.x use djangocms-text-ckeditor >= 3.2.x (e.g.: version 3.2.1).\\nFor django CMS 3.3.x use djangocms-text-ckeditor >= 3.1.x (e.g.: version 3.1.0).\\nFor django CMS 3.2.x use djangocms-text-ckeditor <= 2.9.x (e.g.: version 2.9.3).\\nFor django CMS 3.0 and 3.1 use djangocms-text-ckeditor <= 2.7 (e.g.: version 2.7.0).\\nFor django CMS 2.3 and 2.4 use the djangocms-text-ckeditor 1.x releases (e.g.: version 1.0.10).\\nFor Django 1.4 and 1.5 use djangocms-text-ckeditor < 2.7.\\ncms.plugins.text and djangocms-text-ckeditor can\\'t be used at the same time.\\n\\n\\n\\nInstallation\\nThis plugin requires django CMS 3.3 or higher to be properly installed.\\n\\nIn your projects virtualenv, run pip install djangocms-text-ckeditor.\\nAdd djangocms_text_ckeditor to your INSTALLED_APPS (the order does not matter).\\nRun manage.py migrate djangocms_text_ckeditor.\\n\\n\\nSome notes:\\n\\nIf upgrading from previous djangocms_text_ckeditor, be aware that the\\nnames of the migration modules have changed:\\nDjango 1.6: djangocms_text_ckeditor.migrations to\\ndjangocms_text_ckeditor.south_migrations\\nDjango 1.7: djangocms_text_ckeditor.migrations_django to\\ndjangocms_text_ckeditor.migrations\\n\\n\\nIf using Django 1.6 add \\'djangocms_text_ckeditor\\': \\'djangocms_text_ckeditor.south_migrations\\',\\nto SOUTH_MIGRATION_MODULES  (or define SOUTH_MIGRATION_MODULES if it does not exists);\\nIf using Django 1.7 and you were using version prior to 2.5, remove\\ndjangocms_text_ckeditor from MIGRATION_MODULES;\\n\\n\\nUpgrading from cms.plugins.text\\n\\nRemove cms.plugins.text from INSTALLED_APPS\\nAdd djangocms_text_ckeditor to INSTALLED_APPS\\nRun python manage.py migrate djangocms_text_ckeditor 0001 --fake\\n\\n\\nUsage\\n\\nDefault content in Placeholder\\nIf you use Django-CMS >= 3.0, you can use TextPlugin in \"default_plugins\"\\n(see docs about the CMS_PLACEHOLDER_CONF setting in Django CMS 3.0).\\nTextPlugin requires just one value: body where you write your default\\nHTML content. If you want to add some \"default children\" to your\\nautomagically added plugin (i.e. a LinkPlugin), you have to put children\\nreferences in the body. References are \"%(_tag_child_<order>)s\" with the\\ninserted order of chidren. For example:\\nCMS_PLACEHOLDER_CONF = {\\n    \\'content\\': {\\n        \\'name\\' : _(\\'Content\\'),\\n        \\'plugins\\': [\\'TextPlugin\\', \\'LinkPlugin\\'],\\n        \\'default_plugins\\':[\\n            {\\n                \\'plugin_type\\':\\'TextPlugin\\',\\n                \\'values\\':{\\n                    \\'body\\':\\'<p>Great websites : %(_tag_child_1)s and %(_tag_child_2)s</p>\\'\\n                },\\n                \\'children\\':[\\n                    {\\n                        \\'plugin_type\\':\\'LinkPlugin\\',\\n                        \\'values\\':{\\n                            \\'name\\':\\'django\\',\\n                            \\'url\\':\\'https://www.djangoproject.com/\\'\\n                        },\\n                    },\\n                    {\\n                        \\'plugin_type\\':\\'LinkPlugin\\',\\n                        \\'values\\':{\\n                            \\'name\\':\\'django-cms\\',\\n                            \\'url\\':\\'https://www.django-cms.org\\'\\n                        },\\n                    },\\n                ]\\n            },\\n        ]\\n    }\\n}\\n\\n\\nCKEDITOR_SETTINGS\\nYou can override the setting CKEDITOR_SETTINGS in your settings.py:\\nCKEDITOR_SETTINGS = {\\n    \\'language\\': \\'{{ language }}\\',\\n    \\'toolbar\\': \\'CMS\\',\\n    \\'skin\\': \\'moono-lisa\\',\\n}\\n\\nThis is the default dict that holds all CKEditor settings.\\n\\nCustomizing plugin editor\\nTo customize the plugin editor, use toolbar_CMS attribute, as in:\\nCKEDITOR_SETTINGS = {\\n    \\'language\\': \\'{{ language }}\\',\\n    \\'toolbar_CMS\\': [\\n        [\\'Undo\\', \\'Redo\\'],\\n        [\\'cmsplugins\\', \\'-\\', \\'ShowBlocks\\'],\\n        [\\'Format\\', \\'Styles\\'],\\n    ],\\n    \\'skin\\': \\'moono-lisa\\',\\n}\\n\\n\\nCustomizing HTMLField editor\\nIf you use HTMLField from djangocms_text_ckeditor.fields in your own\\nmodels, use toolbar_HTMLField attribute:\\nCKEDITOR_SETTINGS = {\\n    \\'language\\': \\'{{ language }}\\',\\n    \\'toolbar_HTMLField\\': [\\n        [\\'Undo\\', \\'Redo\\'],\\n        [\\'ShowBlocks\\'],\\n        [\\'Format\\', \\'Styles\\'],\\n    ],\\n    \\'skin\\': \\'moono-lisa\\',\\n}\\n\\nYou can further customize each HTMLField field by using different\\nconfiguration parameter in your settings:\\nmodels.py\\n\\nclass Model1(models.Model):\\n    text = HTMLField(configuration=\\'CKEDITOR_SETTINGS_MODEL1\\')\\n\\nclass Model2(models.Model):\\n    text = HTMLField(configuration=\\'CKEDITOR_SETTINGS_MODEL2\\')\\n\\nsettings.py\\n\\nCKEDITOR_SETTINGS_MODEL1 = {\\n    \\'toolbar_HTMLField\\': [\\n        [\\'Undo\\', \\'Redo\\'],\\n        [\\'ShowBlocks\\'],\\n        [\\'Format\\', \\'Styles\\'],\\n        [\\'Bold\\', \\'Italic\\', \\'Underline\\', \\'-\\', \\'Subscript\\', \\'Superscript\\', \\'-\\', \\'RemoveFormat\\'],\\n    ]\\n}\\n\\nCKEDITOR_SETTINGS_MODEL2 = {\\n    \\'toolbar_HTMLField\\': [\\n        [\\'Undo\\', \\'Redo\\'],\\n        [\\'Bold\\', \\'Italic\\', \\'Underline\\', \\'-\\', \\'Subscript\\', \\'Superscript\\', \\'-\\', \\'RemoveFormat\\'],\\n    ]\\n}\\n\\n\\nAdd configuration=\\'MYSETTING\\' to the HTMLField usage(s) you want to\\ncustomize;\\nDefine a setting parameter named as the string used in the configuration\\nargument of the HTMLField instance with the desidered configuration;\\n\\nValues not specified in your custom configuration will be taken from the global\\nCKEDITOR_SETTINGS.\\nFor an  overview of all the available settings have a look here:\\nhttp://docs.ckeditor.com/#!/api/CKEDITOR.config\\n\\nInline preview\\nThe child plugins of TextPlugin can be rendered directly inside CKEditor if\\ntext_editor_preview isn\\'t False. However there are few important points\\nto note:\\n\\nby default CKEditor doesn\\'t load CSS of your project inside the editing area\\nand has specific settings regarding empty tags, which could mean that things\\nwill not look as they should until CKEditor is configured correctly.\\nSee examples:\\n\\n\\nadd styles and js configuration\\nstop CKEditor from removing empty spans (useful for iconfonts)\\n\\n\\n\\nif you override widget default behaviour - be aware that it requires the\\nproperty \"allowedContent\" to contain cms-plugin[*] as this custom tag is\\nwhat allows the inline previews to be rendered\\n\\n\\n\\nDrag & Drop Images\\nIn IE and Firefox based browsers it is possible to drag and drop a picture into the text editor.\\nThis image is base64 encoded and lives in the \\'src\\' attribute as a \\'data\\' tag.\\nWe detect this images, encode them and convert them to picture plugins.\\nIf you want to overwirite this behavior for your own picture plugin:\\nThere is a setting called:\\nTEXT_SAVE_IMAGE_FUNCTION = \\'djangocms_text_ckeditor.picture_save.create_picture_plugin\\'\\n\\nyou can overwrite this setting in your settings.py and point it to a function that handles image saves.\\nHave a look at the function create_picture_plugin for details.\\nTo completely disable the feature, set TEXT_SAVE_IMAGE_FUNCTION = None.\\n\\nTranslations\\nIf you want to help translate the plugin please do it on transifex:\\nhttps://www.transifex.com/projects/p/django-cms/resource/djangocms-text-ckeditor/\\n\\nUsage as a model field\\nIf you want to use the widget on your own model fields, you can! Just import the provided HTMLField like so:\\nfrom djangocms_text_ckeditor.fields import HTMLField\\n\\nAnd use it in your models, just like a TextField:\\nclass MyModel(models.Model):\\n    myfield = HTMLField(blank=True)\\n\\nThis field does not allow you to embed any other CMS plugins within the text editor. Plugins can only be embedded\\nwithin Placeholder fields.\\nIf you need to allow additional plugins to be embedded in a HTML field, convert the HTMLField to a Placeholderfield\\nand configure the placeholder to only accept TextPlugin. For more information on using placeholders outside of the CMS see:\\nhttp://docs.django-cms.org/en/latest/introduction/templates_placeholders.html\\n\\nAuto Hyphenate Text\\nYou can hyphenate the text entered into the editor, so that the HTML entity &shy; (soft-hyphen)\\nautomatically is added in between words, at the correct syllable boundary.\\nTo activate this feature, pip install django-softhyphen. In settings.py add \\'softhyphen\\'\\nto the list of INSTALLED_APPS. django-softhyphen also installs hyphening dictionaries for 25\\nnatural languages.\\nIn case you already installed django-softhyphen but do not want to soft hyphenate, set\\nTEXT_AUTO_HYPHENATE to False.\\n\\nExtending the plugin\\n\\nNote\\nAdded in version 2.0.1\\n\\nYou can use this plugin as base to create your own CKEditor-based plugins.\\nYou need to create your own plugin model extending AbstractText:\\nfrom djangocms_text_ckeditor.models import AbstractText\\n\\nclass MyTextModel(AbstractText):\\n    title = models.CharField(max_length=100)\\n\\nand a plugin class extending TextPlugin class:\\nfrom djangocms_text_ckeditor.cms_plugins import TextPlugin\\nfrom .models import MyTextModel\\n\\n\\nclass MyTextPlugin(TextPlugin):\\n    name = _(u\"My text plugin\")\\n    model = MyTextModel\\n\\nplugin_pool.register_plugin(MyTextPlugin)\\n\\nNote that if you override the render method that is inherited from the base TextPlugin class, any child text\\nplugins will not render correctly. You must call the super render method in order for plugin_tags_to_user_html()\\nto render out all child plugins located in the body field. For example:\\nfrom djangocms_text_ckeditor.cms_plugins import TextPlugin\\nfrom .models import MyTextModel\\n\\n\\nclass MyTextPlugin(TextPlugin):\\n    name = _(u\"My text plugin\")\\n    model = MyTextModel\\n\\n    def render(self, context, instance, placeholder):\\n        context.update({\\n            \\'name\\': instance.name,\\n        })\\n        # Other custom render code you may have\\n    return super(MyTextPlugin, self).render(context, instance, placeholder)\\n\\nplugin_pool.register_plugin(MyTextPlugin)\\n\\nYou can further customize your plugin as other plugins.\\n\\nAdding plugins to the \"CMS Plugins\" dropdown\\nIf you have another plugin that you want to use inside texts you can make them appear in the dropdown by making them text_enabled.\\nCheck in django-cms doc how to do this.\\n\\nConfigurable sanitizer\\ndjangocms-text-ckeditor uses html5lib to sanitize HTML to avoid\\nsecurity issues and to check for correct HTML code.\\nSanitisation may strip tags usesful for some use cases such as iframe;\\nyou may customize the tags and attributes allowed by overriding the\\nTEXT_ADDITIONAL_TAGS and TEXT_ADDITIONAL_ATTRIBUTES settings:\\nTEXT_ADDITIONAL_TAGS = (\\'iframe\\',)\\nTEXT_ADDITIONAL_ATTRIBUTES = (\\'scrolling\\', \\'allowfullscreen\\', \\'frameborder\\')\\n\\nIn case you need more control on sanitisation you can extend AllowTokenParser class and define\\nyour logic into parse() method. For example, if you want to skip your donut attribute during\\nsanitisation, you can create a class like this:\\nfrom djangocms_text_ckeditor.sanitizer import AllowTokenParser\\n\\n\\nclass DonutAttributeParser(AllowTokenParser):\\n\\n    def parse(self, attribute, val):\\n        return attribute.startswith(\\'donut-\\')\\n\\nAnd add your class to ALLOW_TOKEN_PARSERS settings:\\nALLOW_TOKEN_PARSERS = (\\n    \\'mymodule.DonutAttributeParser\\',\\n)\\n\\nNOTE: Some versions of CKEditor will pre-sanitize your text before passing it to the web server,\\nrendering the above settings useless. To ensure this does not happen, you may need to add the\\nfollowing parameters to CKEDITOR_SETTINGS:\\n...\\n\\'basicEntities\\': False,\\n\\'entities\\': False,\\n...\\n\\nTo completely disable the feature, set TEXT_HTML_SANITIZE = False.\\nSee the html5lib documentation for further information.\\n\\nSearch\\ndjangocms-text-ckeditor works well with aldryn-search to make text content using Haystack.\\n\\nAbout CKEditor\\nThe current integrated Version of CKeditor is 4.6.2. For a full documentation visit: http://ckeditor.com/\\n\\nBuilding the JavaScript\\ndjangocms-text-ckeditor distributes a javascript bundle required for the\\nplugin to work, which contains CKEditor itself and all the necessary plugins for\\nfunctioning within CMS. To build the bundle you need to have to install\\ndependencies with npm install and then to run gulp bundle.\\nThis command also updates the file name loaded based on the file contents.\\n\\nUpdating the CKEditor\\nMake sure to use the url in build config\\n<https://github.com/divio/djangocms-text-ckeditor/blob/master/djangocms_text_ckeditor/static/djangocms_text_ckeditor/ckeditor/build-config.js#L16>_.\\n\\n\\n',\n",
       "  'language': 'HTML'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nInstall Docker\\nhttps://docs.docker.com/docker-for-mac/\\nor\\nhttps://docs.docker.com/docker-for-windows/\\nTo view slide deck (requires Docker)\\ngit clone git@github.com:texastribune/docker-workshop.git\\ncd presentation\\nmake\\n\\nLifecycle\\n\\nShakespeare\\ncd shakespeare\\nmake run\\n\\n\\nJupyter\\ncd juptyer\\nmake run\\n\\n\\n\\n',\n",
       "  'language': 'Jupyter Notebook'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nHarvey\\nAn apps page aggregating Harvey data.\\nQuickstart\\nyarn (if you have it, and you should!) or npm install.\\nNow, get to work!\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nReact External Boilerplate\\nTrying to develop a React app inside a code base that\\'s predominantly something else -- Django or Rails, for example - can be annoying. If you wan\\'t hot reloading and all that jazz, you\\'re likely going to have to find a way to get Node.js running on top of your existing server. It\\'s not impossible, but it\\'s also not fun.\\nThis boilerplate doesn\\'t fix that issue but rather gives you some tools for building your React app as a separate repository, then bringing it into your main code base during its build process.\\nIt\\'s all super basic stuff -- but it\\'ll save you a few minutes.\\nCommands\\n\\nnpm start: Start the development server, hot reloading included.\\nnpm run build: Build the bundle and stylesheet for production.\\n\\nHow to use it\\nDo your React-y development as you normally would. Go crazy. Build 100 components. We like to put them in the src/ directory, but that\\'s up to you.\\nThe important stuff lives in src/index.js (the Webpack entry point). During development, your app will simply attach to the DOM node specified in the isDev conditional block. But after doing npm run build, the bundle has three special qualities:\\n\\nIt exports App as a CommonJS module. That means you can put your React app on NPM and include it in your main code base\\'s package.json. This will allow for statements like import App from YourSeparateReactApp throughout your main code base.\\nIt exports the object baseProps, which contains props defined in src/index.js that are sent to <App/>. This is useful if your workflow is something like that described in No. 1. You can define additional props inside your main code base and send both those and baseProps to <App/>.\\nIt exports the method renderApp(). This method accepts a DOM element as its first argument -- to which your React app widget will eventually attach. You can also send an object of additional props as the second argument. renderApp() could be useful if, instead of putting your package on NPM, you\\'re putting the production bundle on a CDN.\\n\\nExamples\\nImporting your React app into your main code base\\n  import React from \\'react\\';\\n  import ReactDOM from \\'react-dom\\';\\n\\n  import { App, baseProps } from \\'YourSeparateReactApp\\';\\n\\n  const additionalProps = { bar: \\'foo\\' };\\n\\n  ReactDOM.render(\\n    <App {...additionalProps} {...baseProps} />,\\n    document.getElementById(\\'root\\')\\n  );\\nUsing a CDN\\n  <script src=\"https://mycdn.org/reactExternal.min.js\"></script>\\n  <script>\\n    var el = document.querySelector(\\'#foo\\');\\n    var additionalProps = { bar: \\'foo\\' };\\n\\n    reactExternal.renderApp(el, additionalProps);\\n    /**\\n      Your React widget will attach to <div id=\"foo\">.\\n      Nothing fancy at all -- just allows you to define\\n      your React app\\'s attachment point inside the\\n      document instead of that logic floating\\n      somewhere in the CDN code.\\n    */\\n  </script>\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTXLege Camera Status\\nA quick script that attempts to confirm whether a Texas House or Senate Granicus stream is live or not.\\nInstallation\\nnpm install [--save-dev] txlege-camera-status\\nUsage\\nconst { isCameraLive } = require('txlege-camera-status');\\n\\n// first parameter is chamber, second parameter is camera ID\\nisCameraLive('house', 3).then((cameraIsLive) => {\\n  // if camera is live, `cameraIsLive` is true\\n});\\nThere's also a tiny command line tool built in, too. (Mostly exists from when I was testing the script.)\\n> is-txlege-camera-live house 3\\ntrue\\nLicense\\nMIT\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\n\\ntt_social_auth\\nThis package is a custom python-social-auth backend for the Texas Tribune.\\nDrop it into your app of choice to get TT OAuth2 working (relatively) quickly.\\n\\nInstallation: Django\\nThe majority of this is the same as installing python-social-auth directly.\\nIf you run into problems or confusions, consult their docs.\\n\\nRegister your app in the main Texas Tribune project. Talk to Tech if you need help with this. This will give you a client ID and secret key.\\n\\nAdd tt_social_auth to requirements.txt\\n\\nAdd social.apps.django_app.default to INSTALLED_APPS\\n\\nAdd tt_social_auth.backends.texastribune.TribOAuth2 to AUTHENTICATION_BACKENDS (NOTE: you want to keep Django's default auth backend too; see Django docs for details)\\n\\nSet your client and secret from Step 1 in settings.py:\\nSOCIAL_AUTH_TEXASTRIBUNE_KEY = 'Your key here'\\nSOCIAL_AUTH_TEXASTRIBUNE_SECRET = 'Your secret here'\\n\\n\\nAdd the following to your TEMPLATE_CONTEXT_PROCESSORS (in Django 1.8 and higher, this goes in TEMPLATES['OPTIONS']['context_processors']):\\n'social.apps.django_app.context_processors.backends',\\n'social.apps.django_app.context_processors.login_redirect',\\n\\n\\nAdd the following to your urls.py:\\nurl('', include('social.apps.django_app.urls', namespace='social')),\\n\\n\\nOptional: if you don't want admins in Texas Tribune to be admins in this app, set AUTHENTICATE_TEXASTRIBUNE_STAFF=False in your Django settings or env var.\\n\\nOptional: if you are testing with a non-production version of the Texas Tribune app, you can set TEXASTRIBUNE_BASE_URL in your settings.py. For instance, if you want to test against a local server, you could set it to http://local.texastribune.org:8000/.\\n\\nOptional: you may want to set a LOGIN_REDIRECT_URL, which is where the login will redirect when complete.\\n\\n\\nTest it out by navigating to /login/texastribune and confirm that it goes through the whole handshake and drops you off at the LOGIN_REDIRECT_URL.\\n\\nDevelopment & Tests\\nTo install and run tests:\\npip install -e .[test]\\npy.test\\n\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ndjango-taggit\\n\\n\\ndjango-taggit a simpler approach to tagging with Django.  Add \"taggit\" to your\\nINSTALLED_APPS then just add a TaggableManager to your model and go:\\nfrom django.db import models\\n\\nfrom taggit.managers import TaggableManager\\n\\nclass Food(models.Model):\\n    # ... fields here\\n\\n    tags = TaggableManager()\\nThen you can use the API like so:\\n>>> apple = Food.objects.create(name=\"apple\")\\n>>> apple.tags.add(\"red\", \"green\", \"delicious\")\\n>>> apple.tags.all()\\n[<Tag: red>, <Tag: green>, <Tag: delicious>]\\n>>> apple.tags.remove(\"green\")\\n>>> apple.tags.all()\\n[<Tag: red>, <Tag: delicious>]\\n>>> Food.objects.filter(tags__name__in=[\"red\"])\\n[<Food: apple>, <Food: cherry>]\\nTags will show up for you automatically in forms and the admin.\\ndjango-taggit requires Django 1.7 or greater.\\nFor more info check out the documentation.  And for questions about usage or\\ndevelopment you can contact the\\nmailinglist.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nFaces of Death Row\\nFaces of Death Row was produced by Jolie McCullough for The Texas Tribune. It is a visualization of all inmates currently on death row in Texas filterable by length of stay, race, age, sex, county and execution date. It could easily be modified to show other filterable data.\\nThis app was built on the Texas Tribune's Data Visuals kit. It is assisted by the libraries filter.js and chosen.js.\\nData\\nThe data in this app was originally collected from the Texas Department of Criminal Justice (TDCJ) in April 2015 via an open records request. The conviction summaries are gathered from TDCJ records, court documents and news articles and summarized by the Texas Tribune. The data is regularly updated by Tribune staff.\\nUpdate Process\\nPlease note - some static assets required to make this project work are only accessible to Texas Tribune developers.\\nClone the project, then run npm install. Then pull down the assets with npm run assets:pull, and the data with npm run data:fetch. Use npm run serve to run the local development server.\\nThe data is stored in a Google spreadsheet which can be edited with anyone who has a Tribune email address. Here is a public version. To remove or add an inmate, simply edit the spreadsheet.\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\neslint-config-data-visuals\\nThis package serves as the base for Data Visuals\\' JavaScript standards.\\nUsage\\nFirst, install the package.\\nnpm install --save-dev eslint-config-data-visuals\\nThen add an eslint configuration file to your project and extend from this library.\\n{\\n  \"extends\": \"data-visuals\"\\n}\\n\\nWait... isn\\'t this just semistandard?\\nYou\\'re right! It is. But that package tends to be a little behind standard, and we wanted to keep things flexible if we decide to break away. But standard felt like a good starting point!\\nLicense\\nMIT\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nVarnish\\nSupported tags and respective Dockerfile links\\n\\n5.1.1, 5.1, 5, latest (5.1.1/Dockerfile)\\n\\nWhat is Varnish?\\nVarnish Cache is a web application accelerator also known as a caching HTTP reverse proxy. You install it in front of any server that speaks HTTP and configure it to cache the contents. Varnish Cache is really, really fast. It typically speeds up delivery with a factor of 300 - 1000x, depending on your architecture.\\n\\nwikipedia.org/wiki/Varnish_(software)\\n\\nHow to use this image.\\nThis image is intended as a base image for other images to built on.\\nCreate a Dockerfile in your Varnish project\\nFROM texastribune/varnish:5.1.1\\nCreate a default.vcl in your Varnish project\\nvcl 4.0;\\n\\nbackend default {\\n    .host = \"www.nytimes.com\";\\n    .port = \"80\";\\n}\\nThen, run the commands to build and run the Docker image:\\n$ docker build -t my-varnish .\\n$ docker run -it --rm --name my-running-varnish my-varnish\\nCustomize configuration\\nYou can override the port Varnish serves in your Dockerfile.\\nFROM texastribune/varnish:5.1.1\\n\\nENV VARNISH_PORT 8080\\nENV VARNISH_DAEMON_OPTS \"additional varnish options here\"\\nEXPOSE 8080\\nFor valid VARNISH_DAEMON_OPTS, see the varnish options documentation.\\nYou can override the size of the cache.\\nFROM texastribune/varnish:5.1.1\\n\\nENV VARNISH_MEMORY 1G\\nHow to install VMODs (Varnish Modules)\\nVarnish Modules are extensions written for Varnish Cache.\\nTo install Varnish Modules, you will need the Varnish source to compile against. This is why we install Varnish from source in this image rather than using a package manager.\\nInstall VMODs in your Varnish project\\'s Dockerfile. For example, to install the Querystring module:\\nFROM texastribune/varnish:5.1.1\\n\\n# Install Querystring Varnish module\\nENV QUERYSTRING_VERSION=0.3\\nRUN \\\\\\n  cd /usr/local/src/ && \\\\\\n  curl -sfL https://github.com/Dridi/libvmod-querystring/archive/v$QUERYSTRING_VERSION.tar.gz -o libvmod-querystring-$QUERYSTRING_VERSION.tar.gz && \\\\\\n  tar -xzf libvmod-querystring-$QUERYSTRING_VERSION.tar.gz && \\\\\\n  cd libvmod-querystring-$QUERYSTRING_VERSION && \\\\\\n  ./autogen.sh && \\\\\\n  ./configure VARNISHSRC=/usr/local/src/varnish-$VARNISH_VERSION && \\\\\\n  make install && \\\\\\n  rm -r ../libvmod-querystring-$QUERYSTRING_VERSION*\\nLicense\\nView license information for the software contained in this image.\\nSupported Docker versions\\nThis image is supported on Docker version 1.9.1.\\nSupport for older versions (down to 1.6) is provided on a best-effort basis.\\nPlease see the Docker installation documentation for details on how to upgrade your Docker daemon.\\nIssues\\nIf you have any problems with or questions about this image, please contact us through a GitHub issue.\\nContributing\\nYou are invited to contribute new features, fixes, or updates, large or small; we are always thrilled to receive pull requests, and do our best to process them as fast as we can.\\nBefore you start to code, we recommend discussing your plans through a GitHub issue, especially for more ambitious contributions. This gives other contributors a chance to point you in the right direction, give you feedback on your design, and help you find out if someone else is working on the same thing.\\n\\n\\n',\n",
       "  'language': 'Shell'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nEmission events database\\nScraper tools for the following databases:\\n\\nAir Emission Event Report Database\\nCentral Registry Query - Regulated Entity Search\\nCommission Issued Orders\\n\\nSetup\\nYou will need python, some dependencies, a postgres database and an environmental variable pointing to it and that\\'s it:\\nexport EMISSIONS_DATABASE=\"postgres://malev@localhost:5432/emission_events\"\\npip install -r requirements.txt\\nmake setup\\n\\nAnd to start the party, you will need to populate the database. So far we have 4 different commands to start working:\\npython emission_events/manage.py COMMAND\\n\\nWhere command can be:\\n\\ndownloadbatch to download a batch of emission events starting with --initial.\\nupdateemissions will download 100 emissions from the last one stored in the database. If you run this command daily, you will always have the latest data on your system.\\nupdateregulatedentities Regulated Entities don\\'t change too ofter. But from time to time you can check wheather you have a new kid on the block.\\ndownloadissuedorders will download the issued orders emited for every regulated entity on your database.\\n\\nEmission events type\\nFor now, we are only focusing on the following emission events:\\n\\nair-shutdown\\nair-startup\\nemissions-event\\nemissions-event-emergency-resp\\nexcess-opacity\\nmaintenance\\n\\nCommands\\npsql -h 127.0.0.1 -U postgres -p 5433\\npg_dump -h 127.0.0.1 -U postgres -p 5433 emissions_project > database.sql\\npsql emission_events < database.sql\\n\\nBackup\\nexport EMISSIONS_DATABASE=postgres://malev@localhost/emission_events\\nphd pg_dump > marcosdb-today.sql\\n\\nDocker\\ndocker build -t emissions .\\nEMISSIONS_DATABASE=postgres://malev@localhost:5433/emission_events\\\\\\ndocker run -e EMISSIONS_DATABASE --rm emissions\\n\\nReferences\\n\\nhttps://www.tceq.texas.gov/assets/public/comm_exec/agendas/comm/backup/Agendas/2014/9-24-2014/1447PST.pdf\\nhttp://www11.tceq.texas.gov/oce/ch/index.cfm?fuseaction=main.search&RequestTimeout=90&principalname=&rename=DOW%20TEXAS%20OPERATIONS%20FREEPORT&rern=&aid=&progid=&county=&region=&startdate=09/01/2006&endate=&principalid=&reid=203518192001135\\nhttp://www2.tceq.texas.gov/oce/penenfac/index.cfm?fuseaction=home.details&rn=346523962013242\\nhttp://www11.tceq.texas.gov/oce/ch/index.cfm?fuseaction=main.search&RequestTimeout=90&principalname=&rename=DOW%20TEXAS%20OPERATIONS%20FREEPORT&rern=&aid=&progid=&county=&region=&startdate=09/01/2006&endate=&principalid=&reid=203518192001135\\nhttp://www.tceq.state.tx.us/assets/public/comm_exec/agendas/comm/marked/2014/140115.Mrk.pdf\\nhttp://www2.tceq.texas.gov/oce/penenfac/index.cfm?fuseaction=home.details&rn=904547452014029\\nhttp://www7.tceq.state.tx.us/uploads/eagendas/Agendas/2014/9-24-2014/0158PWS.pdf\\nhttps://www.tceq.texas.gov/assets/public/compliance/enforcement/enf_reports/AER/FY13/enfrptfy13.pdf\\nhttp://www11.tceq.state.tx.us/oce/eer/index.cfm?fuseaction=main.getDetails&target=205264\\n\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nAudiogram\\nThe Texas Tribune\\'s fork of New York Public Radio\\'s excellent social-audio tool.\\nPreferred installation is with Docker.\\nLicense\\nCopyright (c) 2016 New York Public Radio\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': 'No readme file.', 'language': 'Makefile'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\narmstrong.core.tt_sections\\nProvides the basic concept of sections within an Armstrong site.\\nYou can use Section models to organize your content into a group.  Sections\\ncan have a parent section to allow you to create a hierarchy.  For example, the\\nTexas Tribune has an Immigration section which in turns has Sanctuary Cities\\nand Dream Act as children sections.\\nYou are not limited to a hierarchical structure---you can create a flat\\nstructure as well.\\n\\nUsage\\nYou need to add a section field to any model that you would like to show up\\nin a given section.  For example:\\n# your models.py\\nfrom django.db import models\\nfrom armstrong.core.tt_sections.models import Section\\n\\n\\nclass MyArticle(models.Model):\\n    title = models.CharField(max_length=100)\\n    body = models.TextField()\\n\\n    section = models.ForeignKey(Section)\\n\\nYou can also relate to multiple sections as well through a ManyToManyField:\\nclass MyArticle(models.Model):\\n    # other fields\\n    sections = models.ManyToManyField(Section)\\n\\n\\nDisplaying Sections\\nYou can display a section through the SimpleSectionView class-based-view\\n(CBV).  The standard project template in Armstrong provides an example of how\\nto configure this view.\\nurl(r\\'^section/(?P<full_slug>[-\\\\w/]+)\\',\\n        SimpleSectionView.as_view(template_name=\\'section.html\\'),\\n        name=\\'section_view\\'),\\n\\nYou can use the {% section menu %} template tag to display list of all\\nsections inside your template.  You must load the section_helpers template\\ntags to use this.  You must provide it with a section_view kwarg that is\\nassociated with the section view you configure inside your URL routes.  For\\nexample, to display a list of sections that link to the section view created\\nabove, you would put this in your template.\\n{% load section_helpers %}\\n{% section_menu section_view=\\'section_view\\' %}\\n\\nWith the following sections in your database:\\nPolitics\\nSports\\n    Football\\n    Basketball\\nFashion\\n\\nUsing all of the example we have so far, the output from your template would\\nlook like this:\\n<ul class=\"root\">\\n    <li>\\n        <a href=\\'/section/politics/\\'>Politics</a>\\n    </li>\\n    <li>\\n        <a href=\\'/section/sports/\\'>Sports</a>\\n        <ul class=\"children\">\\n            <li>\\n                <a href=\\'/section/sports/football/\\'>Football</a>\\n            </li>\\n            <li>\\n                <a href=\\'/section/sports/basketball/\\'>Basketball</a>\\n            </li>\\n        </ul>\\n    </li>\\n    <li>\\n        <a href=\\'/section/fashion/\\'>Fashion</a>\\n    </li>\\n</ul>\\n\\n\\nInstallation & Configuration\\nWe recommend installing this through the Cheese Shop.\\npip install armstrong.core.tt_sections\\n\\nThis gets you the latest released version of armstrong.core.tt_sections.\\n\\nConfiguration\\nThere are two setting that you can use to change the behavior of this\\ncomponent.\\n\\nARMSTRONG_SECTION_ITEM_BACKEND\\nThis is used to configure which backend is used to find the items\\nassociated with a given Section.  (default:\\narmstrong.core.tt_sections.backend.ItemFilter)\\nARMSTRONG_SECTION_ITEM_MODEL\\nThis is used by the default find_related_models backend to determine\\nwhich model has a section associated with it. (default:\\narmstrong.apps.content.models.Content)\\n\\n\\nContributing\\n\\nCreate something awesome -- make the code better, add some functionality,\\nwhatever (this is the hardest part).\\nFork it\\nCreate a topic branch to house your changes\\nGet all of your commits in the new topic branch\\nSubmit a pull request\\n\\n\\nState of Project\\nArmstrong is an open-source news platform that is freely available to any\\norganization.  It is the result of a collaboration between the Texas Tribune\\nand Bay Citizen, and a grant from the John S. and James L. Knight\\nFoundation.\\nTo follow development, be sure to join the Google Group.\\narmstrong.core.arm_section is part of the Armstrong project.  You\\'re\\nprobably looking for that.\\n\\nLicense\\nCopyright 2011 Bay Citizen and Texas Tribune\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': 'No readme file.', 'language': 'Makefile'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nData Visuals Kit\\nA work in progress. Do not use yet.\\nI don't know if this will be the final resting place for this, but it needs somewhere to live.\\nQuickstart\\nRun these commands to create your project's folder:\\ngit clone --depth=1 https://github.com/texastribune/idv-kit.git <name-of-project>\\ncd <name-of-project> && rm -rf .git\\nNext, npm install.\\nNow, get to work!\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTexas Tribune - Front End Code Test\\nImagine that you\\'ve been tasked with redesigning our membership sign-up page. When the user decides to give money, we want to offer them a range of membership levels by payment rate with the option to pay annually, monthly or give a one-time donation. You\\'ll find a sketch prototype of the page in the root directory in a file titled Support Prototype.png. Here\\'s a link to our existing donation site. You can use this as inspiration, but please don\\'t copy it verbatim. Consider this an opportunity to refactor the design and code.\\nWhen the user selects to give \"Annually\" or \"One-Time\" the payment options should become $35, $60, $150, $250, $500 and $[input]. If they switch to \"Monthly\", the payment options should become $4, $9, $17, $31, $63 and $[input].\\nWe want you to create a functioning page that console logs or alerts accurate user input when you hit the submit button. We\\'ll be considering:\\n\\nHow you organize the project\\nHow you translate the desktop prototype into a responsive mobile version\\nHow your JavaScript functions\\n\\nPlease don\\'t use images other than what we\\'ve provided. Forms should use valid form elements.\\nHere\\'s a link to our style guide to help you. We don\\'t expect your project to be fully stylized, but it should have basic visual styles applied and be generally consistent with our brand.\\nDownload this repo and commit locally. When you\\'re done send us a zip file that includes the .git folder so we can view your commit history.\\nStarter tips: You\\'ll find a logo and background image to use inside app/assets/img. The project compiles scripts and styles from src/scss and src/js into app/ when you run the grunt-build command. The HTML file that you\\'ll edit is app/index.html.\\nCheck out the FireShell documentation for instructions on how to run the local development server. You may need to install a few assets before you can get started, such as Node, Git, Grunt.\\nOriginal Boilerplate Documentation\\nFireShell \\nFiercely quick front-end boilerplate and workflows.\\nThe opinionated FireShell framework. Built for the modern developer. For teams and the individual, encouraging a better workflow. JavaScript task running, build processes, autominification and file concatenation, wrapped with an enhanced HTML5 boilerplated framework.\\n\\nSource: github.com/toddmotto/fireshell\\nHomepage: getfireshell.com\\nTwitter: @getfireshell\\n\\nJump start\\nGet started with FireShell:\\n\\nDownload the latest stable release from\\ngetfireshell.com.\\nClone the git repo — git clone https://github.com/toddmotto/fireshell.git\\n\\nPlatform support\\nFireShell runs on both Mac OS X, Linux and Windows. Automated command-line scripts are only supported on Mac OS X and Windows.\\nDocumentation\\nRead the developer documentation on FireShell for further reading and learning. You may need to install a few assets before you can get started, such as Node, Git, Grunt.\\nFeatures\\nHere are some of the main features of FireShell:\\n\\nHTML5 framework, WAI-ARIA roles and HTML5 semantics\\nBaseline HTML5 features, DNS prefetching, responsive meta\\nEncourages one-file CSS/JS in the view (HTML) for performance\\nIncludes jQuery CDN and relative fallback\\nIncludes Modernizr and HTML5 Shiv\\nGoogle Universal Analytics snippet\\nOpen source workflow with Grunt.js running on Node.js\\nTwo .command (Mac OS X) and .bat (Windows) files for double-click command-line execution of FireShell\\nAutomatic Grunt dependency installation, directory relocation and grunt tasks\\nDynamically appended copyright for JS/CSS\\nLivereloading the browser and file injection upon changes\\nAnnotated Gruntfile.js for extending\\nBuilt-in build script for auto-minification of CSS and JavaScript files for production\\nPre-setup Sass/SCSS files and folders for baseline project structure and imports\\nIncludes .editorconfig for consistent coding styles in IDEs\\nStandard .gitignore to ignore minified files and standard ignorables such as .DS_Store\\nJSHint .jshintrc file for configuring JavaScript linting\\nNo superfluous code comments\\nExtremely lightweight footprint\\n\\nScaffolding\\n├── app\\n│   ├── apple-touch-icon-precomposed.png\\n│   ├── assets\\n│   │   ├── css\\n│   │   ├── fonts\\n│   │   ├── img\\n│   │   └── js\\n│   ├── favicon.ico\\n│   └── index.html\\n├── src\\n│   ├── js\\n│   │   └── scripts.js\\n│   └── scss\\n│       ├── mixins\\n│       ├── modules\\n│       ├── partials\\n│       ├── vendor\\n│       └── style.scss\\n├── docs\\n├── grunt-build.command\\n├── grunt-build.bat\\n├── grunt-dev.command\\n├── grunt-dev.bat\\n├── package.json\\n├── README.md\\n├── .editorconfig\\n├── .gitignore\\n├── .jshintrc\\n└── .travis.yml\\n\\nRoadmap\\nProjected roadmap for FireShell and it\\'s subsets builds.\\n\\nIntegrate Grunt-init to allow for initial project naming (for dynamic CSS/JS banners)\\nLESS.css variant (less dir inside src), keeping Sass as default but providing Gruntfile.js setup\\nAngularJS FireShell build with MVC scaffolding\\nBower as package manager\\nStatic HTML Includes FireShell build (emulates server-side includes)\\nPHP FireShell spawning a localhost with relevant includes\\nCreate a Yeoman generator for FireShell\\nAdd grunt-autoprefixer in place of vendor Sass mixin.\\n\\nContributors\\nFireShell is maintained by Todd Motto and it\\'s contributors; Jean-Philippe Sirois, Noah Bass, Bernard Chhun, Chris Missal, Mihai Ionut Vilcu, Octavio Amuchastegui.\\nLicense\\nThe MIT License (MIT)\\nCopyright (c) FireShell\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the \"Software\"), to deal in\\nthe Software without restriction, including without limitation the rights to\\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\\nof the Software, and to permit persons to whom the Software is furnished to do\\nso, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n\\n',\n",
       "  'language': 'CSS'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nUnholstered\\nThe Texas Tribune's Unholstered project on police shootings in Texas.\\nThis project was produced by Ryan Murphy and Jolie McCullough using the Texas Tribune's Data Visuals kit. D3 and Swiper were used to create the visuals and landing page experience.\\nQuickstart\\nPlease note - some static assets required to make this project work are only accessible to Texas Tribune developers.\\nClone the project, then run npm install. Then pull down the assets with npm run assets:pull, and the data with npm run data:fetch. Use npm run serve to run the local development server.\\nNow, get to work!\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nPrice of Admission\\nThe Price of Admission series was published in March 2016. This website was built by Becca Aaronson using a variable of The Texas Tribune\\'s app kit.\\nQuickstart\\nRun this command in your project\\'s folder:\\ncurl -fsSL https://github.com/texastribune/newsapps-app-kit/archive/price-of-admission-template.tar.gz | tar -xz --strip-components=1\\nNext, npm install.\\nIf this is your first time to ever use the kit, you need to follow the steps in your terminal for Google authorization, i.e. go to the given link and paste the token into the terminal.\\nCreating Templates with Google Docs & Spreadsheets\\nThis kit allows you to pull in content from Google Docs and Spreadsheets. To use the story.html template file, you\\'ll need to set up a Google doc using our basic template and follow these steps.\\nSet up config.js\\nUpdate the config.js, and add the unique Google Tokens for your  document(s) and spreadsheet(s). For spreadsheets, you\\'ll need to designate how to process the data, as either a keyvalue or objectlist. You\\'ll also need to assign a \"name\" in the config.js for each doc/sheet, which will become the name of a json file in your /data folder with the data pulled in from that document. You\\'ll then be able to refer to that name throughout your project to pull data from that file.\\nUpdate project data with npm run data/fetch\\nTo pull or update the data from your docs/sheets in your project, run the command npm run data/fetch. You can double-check the /data folder to make sure your files and data are updated.\\nCreate index.html / additional story pages\\nAll of the hard work to build your story page occurs in /templates/layouts/story.html. To create a new page, add an HTMl file inside /app, starting with index.html and insert the following code:\\n{% extends \\'layouts/story.html\\' %}\\n{% set context = data.story_one %}\\n\\n{% block styles %}\\n{{ super() }}\\n{% endblock %}\\n\\n{% block script %}\\n{{ super()}}\\n{% endblock %}\\n\\nThe line {% set context = data.story_one %} tells the project which data file to pull in to build the template. In this case, the file will look for the story_one.json file, which hopefully you just set up in your config.js with the name story_name and pulled in with the command npm run data/fetch.\\nAdditional HTML pages set up in your app will set the name of the file as the slug. For example, when you deploy, index.html will be bucket.org/project-name and story-two.html will be bucket.org/project-name/story-two/.\\nSample Google Doc ArchieML template\\n\\nid — This is the Part Number for the series. It\\'s used to set up the story navigation in the .nav__aside and .nav__footer modules found in /app/styles/_nav.scss. It\\'s also used to indicate which lead art class to use, which you define in /app/styles/_utils/ > @mixin story-header.\\nscript — Sets which script file to load in the base.html. You don\\'t need to include the extension .js. Default: main\\nheadline - Your story headline. Appears in the .storytop, .nav__aside and .nav__footer\\npub_date - Publish date. Use AP style :)\\nslug - URL slug for your story. It must match the filename of the HTML file for that story.\\nshort_head - Shorter headline, in case you need it.\\n[authors] - Currently supports up 2 individual author names. If you include email, the name will be linked. See /app/templates/macros/authors.html\\n{lead_art}\\n\\nphoto_url: test.jpg // file name with extension type. Looks for file in /app/assets/images\\nthumbnail_url: test.jpg // for thumbnail version in .nav__aside\\ncaption: Vestibulum ullamcorper mauris at ligula. Sed hendrerit. // Photo caption\\n-credit: Person McPhotographer // Credit\\n\\n\\nfb_art - Specific image file for Facebook OG meta data. Include extension; looks for file in /app/assets/images\\nsummary - Summary description for Facebook OG meta description.\\ntwitter_text - Text for Twitter share button\\n[+prose] — ALL THE MAGIC STUFF. This will convert regular story text into HTML. Paragraphs in prose must be separated by an empty return space to be read as new <p>. Hyperlinked text will automatically become <a href=\"link.html\">. Inside [+prose] you can also add ad units, the nav__aside, and a variety of other components. See full list and make additional components in /app/template/macros/prose.html.\\n\\n{.ad} - Insert ad unit into prose.\\n\\ntype - Options: inside [468, 60], sidebar [350, 200], banner [728, 90].\\nid - Define ad units in /app/scripts/includes/adLoader.js and set id to indicate which ad unit\\nalignment - Optional. Will add alignment class. Options: right-lock, left-lock, basic.\\n\\n\\nsubhead - Insert subheader into story text with bold styles.\\n{.image} - Insert image.\\n\\nphoto_url - file.jpg // Include file type extension. Looks for file in /app/assets/images\\ncaption - Photo caption text\\ncredit - Photo credit\\nalignment - Options: basic (full width), right-lock (float right on Desktop), left-lock(float left on Desktop)\\n\\n\\ndisclosure - Italicizes text / adds .disclosure styles.\\nrepublish - Adds prompt/link to additional republish page. Requires you to separately set up an HTML page with the story_republish.html template.\\n\\n\\n\\nWebpack\\nThis kit uses the webpack module bundler. You can see an example of how to import files in app/scripts/main.js - as the kit comes preloaded with an import of the app/scripts/includes/adLoader.js partial script. If you\\'re using a big library like JQuery or D3, we recommend downloading the node module, and adding it to the webpack.config.js file as a plugin.\\nHere\\'s an example:\\n  plugins: [\\n    new webpack.ProvidePlugin({\\n      $: \\'jquery\\',\\n      jQuery: \\'jquery\\',\\n      \\'window.jQuery\\': \\'jquery\\',\\n      d3: \\'d3\\'\\n    })\\n  ],\\n\\nThen you can set global variables in your /app/scripts/main.js and reference the libraries throughout your project. When you compile the project later for deployment, it will include the libraries.\\nDevelopment\\nRun the following command to start the development server:\\nnpm run serve\\nThen visit http://localhost:3000 to see your work.\\nConnect to S3\\nTo use the commands to deploy your project to Amazon S3, you\\'ll need to add a profile to your ~/.aws/config. It should look something like this:\\n[profile newsapps]\\naws_access_key_id=YOUR_UNIQUE_ID\\naws_secret_access_key=YOUR_SECRET_ACCESS_KEY\\n\\nDeployment\\nRun these commands to build and deploy:\\nnpm run build\\nnpm run deploy\\n\\nThe project will deploy using the S3 bucket and slug found in your config.js.\\nAssets\\nThe graphics kit comes with an empty app/assets folder for you to store images, fonts and data files. The kit works best if you add these files to app/assets/images, app/assets/fonts and app/assets/data. These files will automatically be ignored by git hub, if added to the proper folders, to prevent a storage overload and to keep files locally that may have sensitive information in an open source project.\\nAvailable Commands\\nnpm run data/fetch\\nPulls down the project\\'s spreadsheet and/or documents and creates data files in the data folder. Will authenticate you with your Google account if needed.\\nnpm run serve\\nStarts the development server.\\nnpm run build\\nBuild the project for production.\\nnpm run deploy/dev\\nDeploys the project.\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\n\\n=============\\n\\nWhat is this?\\nAssumptions?\\nWhat\\'s in here?\\nQuick start\\nConfiguration\\nDeploy the desktop app\\nAbout\\n\\nWhat is this?\\nLunchbox is a suite of tools to create images intended for social media sharing. It includes:\\n\\nQuotable: Converts quoted text into a branded image.\\nFactlist: Produces a branded image with a list of items.\\nWaterbug: Creates a watermarked image with attribution.\\n\\nAssumptions\\nLunchbox is a customizable toolset deployable as a desktop app. The following instructions are meant for developers setting up and customizing the app for their organization. For end-users of the tools, see usage guidelines.\\nThe following things are assumed to be true in this documentation.\\n\\nYou are running OSX.\\nYou are using Python 2.7. (Probably the version that came OSX.)\\nYou have virtualenv and virtualenvwrapper installed and working.\\n\\nWhat\\'s in here?\\n\\nfabfile -- Fabric commands for automating setup and deployment.\\nless -- Application styles and Bootstrap less files.\\ntemplates -- HTML (Jinja2) templates, to be compiled locally.\\nwww -- App assets and rendered files.\\nLunchbox Setup.exe -- Lunchbox Demo installer for Windows.\\nLunchbox.dmg -- Lunchbox Demo installer for OSX.\\napp.py -- A Flask app for rendering the project locally.\\napp_config.py -- Configuration variables for the Flask app.\\npackage.json -- Node dependencies and scripts for building Electron app.\\npackager-config.json -- Configuration for create installers with Electron.\\nrender_utils.py -- Helper functions for baking out Flask app.\\nrequirements.txt -- Python requirements.\\nstatic.py -- Routes for static files in Flask app.\\n\\nQuick Start\\nBootstrap the project by forking this repo and installing the following:\\nmkvirtualenv lunchbox\\ncd lunchbox\\npip install -r requirements.txt\\nnpm install\\n\\nThen run the app:\\nfab app\\n\\nVisit localhost:8000 in your browser to see the app.\\nConfiguration\\nYou can skip configuration if you just want to deploy Lunchbox and start using it with the application\\'s default branding (or you can download the Demo ). Configuration options allow you to tailor the app to match your organization\\'s branding and theme.\\nAssets\\nIf you are customizing the branding of the apps, you will probably want to use your organization\\'s web fonts and logos.\\nFor fonts, we provide a Jinja template at templates/_fonts.html using Typekit\\'s webfontloader for loading fonts from Google, Typekit, or custom stylesheets. Then, the fonts will be available in the CSS and JavaScript in all of the apps.\\nFor your organization\\'s logos, you can provide SVGs or PNGs. Make sure that there is no whitespace around the logo so that the padding performs properly. You can place them anywhere in the www folder as long as you link them correctly when you define your global variables, but we recommend www/img.\\nFor Waterbug, you will want to have a white version and a black version of your logo so that you can choose the appropriate version for light and dark photos.\\nDefine global variables\\nThere are two places where variables are defined, one place for Quotable and Factlist and one place for Waterbug.\\nQuotable/Factlist\\nFor Quotable and Factlist, all configuration takes places in less/variables.less. You can define font families, establish the default background color/text color and define the logo used on the images.\\nImportantly, if you use a custom logo, you will also need to explicitly define the width and height of the logo in both square crop and 16:9 crop scenarios. The variables at the top of the file will do this:\\n@logo-path: url(\\'../path/to/logo.svg\\');\\n@logo-sq-width: 145px;\\n@logo-sq-height: 48px;\\n@logo-16x9-width: 121px;\\n@logo-16x9-height: 40px;\\n\\nAdditionally, you can fine-tune various aspects of Quotable and Factlist using the app-specific variables also listed in the file. The defaults should work well out of the box, but your organization\\'s logo or font may require tweaks.\\nWaterbug\\nWaterbug has a different configuration system because it cannot be controlled through CSS. To customize Waterbug, go to www/js/waterbug-config.js and customize the variables at the top of the file.\\nIn this file, you can define the logos used and the sizes with which they render by editing the logos object.\\nvar logos = {\\n    \\'name-of-logo\\': {\\n        whitePath: \\'../path/to/logo-white.svg\\', // path to white logo\\n        blackPath: \\'../path/to/logo-black.svg\\', // path to black logo\\n        w: 200, // width of logo\\n        h: 67, // height of logo\\n        display: \\'Name of logo\\' // how the button toggle will appear in the UI\\n    },\\n    \\'name-of-second-logo\\': {\\n        whitePath: \\'../path/to/second-logo-white.svg\\',\\n        blackPath: \\'../path/to/second-logo-black.svg\\',\\n        w: 150,\\n        h: 51,\\n        display: \\'Name of second logo\\'\\n    }\\n};\\n\\nIf you have more than one logo, the UI will automatically add toggle buttons so that you can switch between logos on the fly.\\nAdditionally, You can change every property of the font rendering (font face, size, shadow, etc.) as well as the padding around all of the elements (elementPadding) in the image and the export width of the image (canvasWidth).\\nYou will want to configure the copyright options for Waterbug based on the photo providers your news organization can use. This is defined in an large object that contains an object for each copyright option. The boolean values control the behavior of the form:\\n// copyright options\\nvar orgName = \\'Your News Organization\\';\\nvar freelanceString = \\'for \\' + orgName;\\n\\nvar copyrightOptions = {\\n    \\'internal\\': {\\n        showPhotographer: true, // show the photographer input box\\n        showSource: false, // show the source input box\\n        photographerRequired: false, // require a photographer\\n        sourceRequired: false, // require a source\\n        source: orgName, // How the source should appear on the image, e.g. \\'NPR\\'\\n        display: orgName, // How the option will appear in the dropdown menu\\n    },\\n    \\'freelance\\': {\\n        showPhotographer: true,\\n        showSource: false,\\n        photographerRequired: true,\\n        sourceRequired: false,\\n        source: freelanceString,\\n        display: \\'Freelance\\' \\n    },\\n    \\'ap\\': {\\n        showPhotographer: true,\\n        showSource: false,\\n        photographerRequired: false,\\n        sourceRequired: false,\\n        source: \\'AP\\',\\n        display: \\'AP\\' \\n    },\\n    \\'getty\\': {\\n        showPhotographer: true,\\n        showSource: false,\\n        photographerRequired: false,\\n        sourceRequired: false,\\n        source: \\'Getty Images\\',\\n        display: \\'Getty\\' \\n    },\\n    \\'thirdParty\\': {\\n        showPhotographer: true,\\n        showSource: true,\\n        photographerRequired: false,\\n        sourceRequired: true,\\n        source: \\'\\',\\n        display: \\'Third Party/Courtesy\\' \\n    }\\n}\\n\\nThe app will automatically add all of your copyright options to the dropdown menu. Also, it will perform form validation based on the boolean values above.\\nFinally, you can configure the application defaults. Ensure that the logo and image paths point to existing files:\\n// app load defaults\\nvar currentCrop = \\'twitter\\'; // default crop size\\nvar currentLogo = \\'lunchbox\\'; // default logo slug\\nvar currentLogoColor = \\'white\\'; // default logo color\\nvar currentTextColor = \\'white\\'; // default text color\\nvar defaultImage = \\'../img/test-kitten.jpg\\'; // path to image to load as test image\\nvar defaultLogo = logos[currentLogo][\\'whitePath\\'] // path to default logo\\n\\nAt the bottom of the form, you will notice a Sharing Guidelines section. To edit that section, you can just update the list in templates/waterbug.html.\\nMultiple Themes\\nFor Quotable and Factlist, you can provide up to three themes in addition to the default theme if your news organization requires different branding for different accounts (think NPR vs. NPR Music).\\nIn less/variables.less, you can define themes at the bottom of the file. For each theme, you can change the background color, text color, and logo:\\n@theme2-bg-color: #41474E;\\n@theme2-text-color: #dbe0e6;\\n@theme2-logo-path: url(\\'../img/icon-socializr-white.svg\\');\\n@theme2-sq-logo-width: 145px;\\n@theme2-sq-logo-height: 48px;\\n@theme2-16x9-logo-width: 121px;\\n@theme2-16x9-logo-height: 40px;\\n\\nIn the form UI, you can change the display of the theme selection buttons in each app\\'s HTML template (templates/quotable.html, templates/factlist.html). Be sure not to change the ID attribute of the button, as these IDs control the JavaScript that adds and removes classes on the image.\\nDeploy the desktop app\\nThe project uses Electron to create desktop apps for OSX and Windows.\\nTo ensure you will be able to properly build the applications, read the prerequisites section for electron-builder. Specifically, run brew install wine makensis to get the proper libraries for building application installers.\\nOnce you have the prerequisites, build an electron app by running:\\nfab electron master deploy\\n\\nInstallers for Windows and Mac OSX can be found in the root level folder after this runs.\\nDeploy to Amazon S3\\nWhile Lunchbox was designed to be deployed as a desktop app, it may make more sense for your news organization to deploy to Amazon S3 or a file server.\\nFor Amazon S3, ensure that you have your AWS Access Key ID and Secret Access Key stored as environment variables as such:\\nexport AWS_ACCESS_KEY_ID=\"YOURACCESSKEYID\"\\nexport AWS_SECRET_ACCESS_KEY=\"YOURSECRETACCESSKEY\"\\n\\nThen, in app_config.py, change your staging and production S3 targets:\\nPRODUCTION_S3_BUCKET = \\'your.bucket.org\\'\\nSTAGING_S3_BUCKET = \\'stage-your.bucket.org\\'\\n\\nWith these variables set, you can run fab [production/staging] master deploy to deploy Lunchbox to your S3 bucket.\\nDeploy to other file server\\nFor other file servers, you can change the following app_config variables:\\nFILE_SERVER_USER = \\'ubuntu\\' # set this to the user you use to SSH onto the server\\nFILE_SERVER = \\'your.fileserver.org\\' # set this to either the hostname or IP address of your file server\\nFILE_SERVER_PATH = \\'~/www\\' # set this to the path that your server serves files to the web from\\n\\nThen, you can run fab fileserver master deploy. This will rsync the rendered files to FILE_SERVER_PATH/lunchbox.\\nAbout\\nLunchbox consolidates NPR’s Quotable, Factlist and Waterbug, apps into a stand-alone desktop suite of tools for the newsroom.\\nIt was worked on during the OpenNews Portland Code Convening on July 23-24, 2015.\\nAdditional contributors:\\n\\nJason Emory Parker\\n\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nThe News Apps App Kit\\nThis package helps jump start special Tribune features and series that are built outside the regular CMS. It's Tribune-centric, but easy to update and transform to fit your needs. If you're working on a graphic, use the News Apps Graphic Kit. It is similar to the App Kit, but comes with NPR's pym.js to help you embed with a responsive <iframe>.\\nFeatures\\n\\nLive reloading and viewing powered by BrowserSync\\nCompiling of Sass/SCSS with Ruby Sass\\nCSS prefixing with autoprefixer\\nCSS sourcemaps with gulp-sourcemaps\\nCSS compression with csso\\nJavaScript linting with jshint\\nJavaScript compression with uglifyjs\\nTemplate compiling with nunjucks\\nImage compression with gulp-imagemin\\nAsset revisioning with gulp-rev and gulp-rev-replace\\n\\nQuickstart\\nRun this command in your project's folder:\\ncurl -fsSL https://github.com/texastribune/newsapps-app-kit/archive/master.tar.gz | tar -xz --strip-components=1\\nNext, npm install.\\nIf this is your first time to ever use the kit, you need to authorize your computer: npm run spreadsheet/authorize\\nAdd your Google sheet's ID to the config.json, and override any sheets that need to be processed differently. (keyvalue or objectlist)\\nNow get to work!\\nDevelopment\\nRun the following command to start the development server:\\ngulp serve\\nThen visit http://localhost:3000 to see your work.\\nConnect to S3\\nTo use the commands to deploy your project to Amazon S3, you'll need to add a profile to your ~/.aws/config. It should look something like this:\\n[profile newsapps]\\naws_access_key_id=YOUR_UNIQUE_ID\\naws_secret_access_key=YOUR_SECRET_ACCESS_KEY\\n\\nDeployment\\nRun these commands to build and deploy:\\ngulp\\nnpm run deploy\\n\\nThe project will deploy using the S3 bucket and slug found in your package.json.\\nAssets\\nThe graphics kit comes with an empty app/assets folder for you to store images, fonts and data files. The kit works best if you add these files to app/assets/images, app/assets/fonts and app/assets/data. These files will automatically be ignored by git hub, if added to the proper folders, to prevent a storage overload and to keep files locally that may have sensitive information in an open source project.\\nAvailable Commands\\nnpm run spreadsheet/authorize\\nAllows your computer to interact with the scraper. Only needs to be done once. Any future uses of the graphic kit can skip this.\\nnpm run spreadsheet/fetch\\nPulls down the project's spreadsheet and processes it into the data.json file.\\nnpm run spreadsheet/edit\\nOpens the project's spreadsheet in your browser.\\nnpm run deploy\\nDeploys the project.\\nnpm run assets/push\\nPushes the raw assets to the S3 bucket.\\nnpm run assets/pull\\nPulls the raw assets down to the local environment.\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nGuns on Campus\\nThis project was built using the News Apps Kit Price of Admission template, a package that helps jump start special Tribune features and series that are built outside the regular CMS. It\\'s Tribune-centric, but easy to update and transform to fit your needs. If you\\'re working on a single-page graphic and plan to embed it with an iframe, use the News Apps Graphic Kit.\\nQuickstart\\nClone this repo and run npm install.\\nIf this is your first time to ever use the kit, you need to follow the steps in your terminal for Google authorization, i.e. go to the given link and paste the token into the terminal. You\\'ll also need a .tt_kit_google_client_secrets.json file with a secret code in your root directory. Talk to a teammate to get this file. It will look something like this, but with your client_id and client_secret:\\n{\"installed\":{\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\"client_secret\":\"rAnDoML3tt#rs\",\"token_uri\":\"https://accounts.google.com/o/oauth2/token\",\"client_email\":\"\",\"redirect_uris\":[\"urn:ietf:wg:oauth:2.0:oob\",\"oob\"],\"client_x509_cert_url\":\"\",\"client_id\":\"rAnDoML3tt#rs.apps.googleusercontent.com\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\"}}\\n\\nCreating Templates with Google Docs & Spreadsheets\\nThis kit pulls in content from Google Docs and Spreadsheets. Here is a Google doc using our basic template.\\nSet up config.js\\nThe config.js contains the unique Google Tokens for the documents loading content into this project. The assigned \"name\" in the config.js for each doc/sheet becomes the name of a json file in your /data folder with the data pulled in from that document. That way, you can refer to the data by name throughout the project to pull data from that file.\\nUpdate project data with npm run data/fetch\\nTo pull or update the data from your docs/sheets in your project, run the command npm run data/fetch. You can double-check the /data folder to make sure your files and data are updated.\\nCreate index.html / additional story pages\\nBuilding a story page occurs in /templates/layouts/story.html. To create a new page, add an HTMl file inside /app, starting with index.html and insert the following code:\\n{% extends \\'layouts/story.html\\' %}\\n{% set context = data.story_one %}\\n\\n{% block styles %}\\n{{ super() }}\\n{% endblock %}\\n\\n{% block script %}\\n{{ super()}}\\n{% endblock %}\\n\\nThe line {% set context = data.story_one %} tells the project which data file to pull in to build the template. In this case, the file will look for the story_one.json file, which hopefully you just set up in your config.js with the name story_name and pulled in with the command npm run data/fetch.\\nAdditional HTML pages set up in your app will set the name of the file as the slug. For example, when you deploy, index.html will be bucket.org/project-name and story-two.html will be bucket.org/project-name/story-two/.\\nSample Google Doc ArchieML template\\n\\nid — This is the Part Number for the series. It\\'s used to set up the story navigation in the .nav__aside and .nav__footer modules found in /app/styles/_nav.scss. It\\'s also used to indicate which lead art class to use, which you define in /app/styles/_utils/ > @mixin story-header.\\nscript — Sets which script file to load in the base.html. You don\\'t need to include the extension .js. Default: main\\nheadline - Your story headline. Appears in the .storytop, .nav__aside and .nav__footer\\nseo - This will be fed to the Page Title tag. If not specified, the <title> will grab Headline.\\npub_date - Publish date. Use AP style :)\\nslug - URL slug for your story. It must match the filename of the HTML file for that story.\\nshort_head - Shorter headline, in case you need it.\\n[authors] - Currently supports up 2 individual author names. If you include email, the name will be linked. See /app/templates/macros/authors.html\\n{lead_art}\\n\\nphoto_url: test.jpg // file name with extension type. Looks for file in /app/assets/images\\nthumbnail_url: test.jpg // for thumbnail version in .nav__aside\\ncaption: Vestibulum ullamcorper mauris at ligula. Sed hendrerit. // Photo caption\\n-credit: Person McPhotographer // Credit\\n\\n\\nfb_art - Specific image file for Facebook OG meta data. Include extension; looks for file in /app/assets/images\\nsummary - Summary description for Facebook OG meta description.\\ntwitter_text - Text for Twitter share button\\n[+prose] — ALL THE MAGIC STUFF. This will convert regular story text into HTML. Paragraphs in prose must be separated by an empty return space to be read as new <p>. Hyperlinked text will automatically become <a href=\"link.html\">. Inside [+prose] you can also add ad units, the nav__aside, and a variety of other components. See full list and make additional components in /app/template/macros/prose.html.\\n\\n{.ad} - Insert ad unit into prose.\\n\\ntype - Options: inside [468, 60], sidebar [350, 200], banner [728, 90].\\nid - Define ad units in /app/scripts/includes/adLoader.js and set id to indicate which ad unit\\nalignment - Optional. Will add alignment class. Options: right-lock, left-lock, basic.\\n\\n\\nsubhead - Insert subheader into story text with bold styles.\\n{.image} - Insert image.\\n\\nphoto_url - file.jpg // Include file type extension. Looks for file in /app/assets/images\\ncaption - Photo caption text\\ncredit - Photo credit\\nalignment - Options: basic (full width), right-lock (float right on Desktop), left-lock(float left on Desktop)\\n\\n\\n{video}\\n\\nid: youTubeID\\ncaption: Vestibulum ullamcorper mauris at ligula. Sed hendrerit.\\n\\n\\ndisclosure - Italicizes text / adds .disclosure styles.\\nrepublish - Adds prompt/link to additional republish page. Requires you to separately set up an HTML page with the story_republish.html template.\\n\\n\\n\\nWebpack\\nThis kit uses the webpack module bundler. You can see an example of how to import files in app/scripts/main.js - as the kit comes preloaded with an import of the app/scripts/includes/adLoader.js partial script. If you\\'re using a big library like JQuery or D3, we recommend downloading the node module, and adding it to the webpack.config.js file as a plugin.\\nHere\\'s an example:\\n  plugins: [\\n    new webpack.ProvidePlugin({\\n      $: \\'jquery\\',\\n      jQuery: \\'jquery\\',\\n      \\'window.jQuery\\': \\'jquery\\',\\n      d3: \\'d3\\'\\n    })\\n  ],\\n\\nThen you can set global variables in your /app/scripts/main.js and reference the libraries throughout your project. When you compile the project later for deployment, it will include the libraries.\\nDevelopment\\nRun the following command to start the development server:\\nnpm run serve\\nThen visit http://localhost:3000 to see your work.\\nConnect to S3\\nTo use the commands to deploy your project to Amazon S3, you\\'ll need to add a profile to your ~/.aws/config. It should look something like this:\\n[profile newsapps]\\naws_access_key_id=YOUR_UNIQUE_ID\\naws_secret_access_key=YOUR_SECRET_ACCESS_KEY\\n\\nDeployment\\nRun these commands to build and deploy:\\nnpm run build\\nnpm run deploy\\n\\nThe project will deploy using the S3 bucket and slug found in your config.js.\\nAssets\\nThe kit comes with an empty app/assets folder for you to store images, fonts and data files. The kit works best if you add these files to app/assets/images, app/assets/fonts and app/assets/data. These files will automatically be ignored by git hub, if added to the proper folders, to prevent a storage overload and to keep files locally that may have sensitive information in an open source project.\\nAvailable Commands\\nnpm run data/fetch\\nPulls down the project\\'s spreadsheet and/or documents and creates data files in the data folder. Will authenticate you with your Google account if needed.\\nnpm run serve\\nStarts the development server.\\nnpm run build\\nBuild the project for production.\\nnpm run deploy/dev\\nDeploys the project.\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nPops: Bootstrapping Django\\'s Admin\\nIntroducing Django\\'s admin to Twitter\\'s Bootstrap.\\nThis is built on top of the work of django-admintools-bootstrap and utilizes\\ndjango-admin-tools.\\n\\nWarning\\nThis is development software and prone to constant change.  Tread carefully.\\n\\n\\nInstallation & Configuration\\nYou can install the development version of this package using pip:\\n$ git clone git://github.com/tswicegood/pops.git\\n$ cd pops && pip install .\\n\\nNext, you must add pops to the list of INSTALLED_APPS in your Django\\nsettings before django-admin-tools.  Note, you have to have a properly\\ninstalled and configured admin-tools package in order to use pops.\\n\\nLicense\\nCopyright 2012-2013 Texas Tribune\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\nOther Software\\nPops is a fork of django-admintools-bootstrap.  That code is available under\\nan MIT license.  All code listed below is copyrighted by their original owners\\nand distributed under their accompanying licenses.  See the various directories\\nin vendor/ for full licensing information.\\n\\nhttp://addyosmani.github.com/jquery-ui-bootstrap/\\nhttp://twitter.github.com/bootstrap/\\nhttps://bitbucket.org/izi/django-admin-tools/\\nhttp://www.crummy.com/software/BeautifulSoup/\\nhttps://github.com/jezdez/django-appconf\\n\\n\\nWhat\\'s in a name?\\n\"Pops\" is one of Louis Armstrong\\'s nicknames.  This project is being done\\nfor integration with the Armstrong Project, which is named after Mr.\\nArmstrong.  Pops is completely independent of the project, however, so we\\ndecided to give it a different, albeit still related, name.\\n\\n\\n',\n",
       "  'language': 'CSS'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nNews Apps Graphic Kit\\nThe News Apps Graphic Kit is a boilerplate for embeddable graphics. It was built for the Texas Tribune's News Apps team, but could easily be altered to cater to other organization's/individual's needs.\\nFeatures\\n\\nLive reloading and viewing powered by BrowserSync\\nCompiling of Sass/SCSS with Ruby Sass\\nCSS prefixing with autoprefixer\\nCSS sourcemaps with gulp-sourcemaps\\nCSS compression with csso\\nJavaScript linting with jshint\\nJavaScript compression with uglifyjs\\nTemplate compiling with nunjucks\\nImage compression with gulp-imagemin\\nAsset revisioning with gulp-rev and gulp-rev-replace\\npym.js included by default for easy embedding in hostile CMS environments\\n\\nQuickstart\\nRun this command in your project's folder:\\ncurl -fsSL https://github.com/texastribune/newsapps-graphic-kit/archive/master.tar.gz | tar -xz --strip-components=1\\nNext, npm install.\\nIf this is your first time to ever use the kit, you need to authorize your computer: npm run spreadsheet/authorize\\nAdd your Google sheet's ID to the config.json, and override any sheets that need to be processed differently. (keyvalue or objectlist)\\nGet to work!\\nConnect to S3\\nTo use the commands to deploy your project to Amazon S3, you'll need to add a [profile newsapps] to ~/.aws/config. It should look something like this:\\n[profile newsapps]\\naws_access_key_id=YOUR_UNIQUE_ID\\naws_secret_access_key=YOUR_SECRET_ACCESS_KEY\\n\\nThen you can run these commands to build and deploy:\\ngulp\\nnpm run deploy\\n\\nThe package will deploy to graphics.texastribune.org/donor-wall. To change the location, update the package.json file.\\nAssets\\nThe graphics kit comes with an empty app/assets folder for you to store images, fonts and data files. The kit works best if you add these files to app/assets/images, app/assets/fonts and app/assets/data. These files will automatically be ignored by git hub, if added to the proper folders, to prevent a storage overload and to keep files locally that may have sensitive information in an open source project.\\nYeoman is being considered.\\nAvailable Commands\\nnpm run spreadsheet/authorize\\nAllows your computer to interact with the scraper. Only needs to be done once! Any future uses of the graphic kit can skip this.\\nnpm run spreadsheet/fetch\\nPulls down the project's spreadsheet and processes it into the data.json file.\\nnpm run spreadsheet/edit\\nOpens the project's spreadsheet in your browser.\\nnpm run deploy\\nDeploys the project.\\nnpm run assets/push\\nPushes the raw assets to the S3 bucket.\\nnpm run assets/pull\\nPulls the raw assets down to the local environment.\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nNews Apps Styles v2.0.1\\nBase styles for all your News Apps needs. Comes with versioning and is installable via npm.\\nDependencies\\n\\nBourbon\\nNeat\\n\\nInstalling\\nnpm install --save-dev newsapps-styles\\nOur Philosophy\\nThe goal of this style guide is to create a front-end framework to maintain consistency across News Apps projects, while giving each designer and developer the flexibility to code in their own unique and creative way.\\nOur style files are written in SCSS — a Sass syntax that builds on the existing CSS syntax. With SCSS, we can use all the great features of Sass, such as @mixins, %placeholders and $variables, but can write in the more familiar (and more expressive) CSS syntax.\\nIf you're unfamiliar with Sass or SCSS, read this guide to catch up on the basics.\\nCurrently, our partial style files are broken into three folders — helpers, theme, and components. You'll find a thorough description of the partials in the helpers folder below. If you're creating a feature with additional prose styles or making a graphic, you may also want to use those partials from the theme folder. And you'll find particular styles for components, like a masthead, and footer, in the components folder.\\nThe Helpers\\nEverything you need to jump start a project is contained in the helpers folder. Here's an explanation of each of the partials:\\n_BASE.SCSS\\nThese are the only styles applied directly to HTML elements. If you really need a clean slate, don't use this partial. But it's helpful for normalizing styles and inheriting basic news apps prose styles without using additional classes.\\n_BUTTONS.SCSS:\\nCreates a button mixin, to include buttons in any color. Also allows for solid color buttons or ghost buttons, based on the arguments you insert.\\n_COLORS.SCSS:\\nAll of our colors are assigned to $variables, most of which have a descriptive name to help you decide when to use it. For example, $rep-red and $dem-blue are used for typical political party designations.\\n_GRID.SCSS:\\nEverything you need to build a responsive layout is included in the grid partial. Jump to Grid Specs to learn more about the basic structure and default grid settings, and Grid Mixins to learn more about the @mixins we've created to help you implement a grid design.\\n_TABLES.SCSS:\\nThis partial contains one mixin to rule them all — i.e. all the variations of mobile table styles. Skip to Tables to learn more.\\n_TYPOGRAPHY.SCSS:\\nOur style guide has a modular typography system. By changing the size $variables, you can change font-sizes across the entire project. The system also allows you to designate font-size with .classes in your HTML or @extend %font-sizes on new classes.\\n\\n\\n\",\n",
       "  'language': 'CSS'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ntt_disposal_wells\\nThis Django application was generated using the Texas Tribune's Generic\\nDjango app template.\\n\\nInstallation & Configuration\\nYou can install this using pip like this:\\npip install tt_disposal_wells\\n\\nOnce installed, you need to add it to your INSTALLED_APPS.  You can do that\\nhowever you like or you can copy-and-paste this in after your\\nINSTALLED_APPS are defined.\\nINSTALLED_APPS += ['tt_disposal_wells', ]\\n\\nNow you're ready to start using tt_disposal_wells.\\n\\nUsage\\nTODO\\n\\nExample\\nAll of the usage is outlined, along with tests inside the example\\ndirectory.  See that directory for more information on how to run the tests and\\nexample project.\\n\\n\\n\",\n",
       "  'language': 'HTML'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nMeme v.2\\nContributors: Yuri Victor, Joshua Benton, Matt Montgomery, Ivar Vong, Steve Peters, Flip Stewart, Greg MacWilliam.\\nMeme is a generator that Vox Media uses to create social sharing images. See working version at http://www.sbnation.com/a/meme.\\n\\nWhat\\'s new in version 2.0?\\n\\nRefactored into a formal MV* app.\\nFixed bugs with rendering state and repeat drag-n-drop images.\\nImproved initial rendering with loaded web fonts.\\nImproved cross-origin options: both for base64 images and CORS.\\nHighly (and easily!) customizable editor and theme options.\\nWatermark selector.\\n\\nInstall\\n\\ngit clone https://github.com/voxmedia/meme.git\\nbundle install\\nbundle exec middleman\\n\\nThis will start a local web server running at: http://localhost:4567/\\nCustomization\\nConfiguration\\nSettings and controls are configured through source/javascripts/settings.js.erb. The settings file has ample comments to document configuration.\\nFonts\\nInclude your own fonts in stylesheets/_fonts.scss, then add your font options into the settings file.\\nEditor theme\\nSet the theme-color variable in source/stylesheets/_vars.scss. That one color will be tinted across all editor controls.\\nCross-Origin Resources (CORS)\\nThis is an HTML5 Canvas-based application, and thus comes with some security restrictions when loading graphics across domains (ex: a canvas element on http://tatooine.com cannot export with an image hosted on http://dagobah.com).\\nIf you\\'re hosting this application on the same domain that serves your images, then congratulations! You have no problems. However, if you\\'re going through a CDN, then you\\'ll probably encounter some cross-domain security issues; at which time you have two options:\\n\\n\\nFollow this excellent MDN article about configuring \"Access-Control-Allow-Origin\" headers. You\\'ll need to enable these headers on your CDN, at which time the Meme app should be able to request images from it.\\n\\n\\nEmbed all of your watermark images as base64 data URIs within the settings.js.erb file. The asset pipeline\\'s asset_data_uri helper method makes this very easy, and effectively embeds all image data within your JavaScript. The downside here is that your JavaScript will become a very large payload as you include more images. In the long term, getting CORS headers configured will be a better option.\\n\\n\\nExamples\\n\\nhttp://www.sbnation.com/a/meme\\nhttps://twitter.com/voxdotcom/status/481671889094340608\\nhttps://twitter.com/voxdotcom/status/479228288221470721\\nhttps://twitter.com/voxdotcom/status/481619042545844225\\n\\nContributing\\n\\nFork it ( https://github.com/voxmedia/meme/fork )\\nCreate your feature branch (git checkout -b my-new-feature)\\nCommit your changes (git commit -am \\'Add some feature\\')\\nPush to the branch (git push origin my-new-feature)\\nCreate a new Pull Request\\n\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\narmstrong.esi\\nDjango application for handling edge side include (ESI)\\n\\nUsage\\nESI allows you to specify sections of the site that require different caching\\nstrategies and can be sent to a smart caching layer for rendering.\\nFor example, if you want to send a page that is identical for every user except\\nfor a welcome message, you could render that message like:\\n<html>\\n  <body>\\n    <esi:include \"/esi/welcome-message\" />\\n    ... the rest of the page ...\\n  </body>\\n</html>\\n\\nA smart proxy such as Varnish and the middleware included with\\narmstrong.esi can cache this page, and send a request for /esi/welcome-message\\nfor personalization. The next user hitting the page would get the cached version and\\nyour application server would only need to render /esi/welcome-message\\narmstrong.esi provides a template tag for rendering the correct urls with the same\\nsyntax as django\\'s url tag. For example, the above example becomes:\\n{% load esi %}\\n<html>\\n  <body>\\n    {% esi welcome_message %}\\n    ... the rest of the page ...\\n  </body>\\n</html>\\n\\nThis replaces our {% esi %} tag with a <esi:include> tag pointing to\\nthe URL for that view.\\n\\nUsing with Varnish\\nVarnish integrates fairly easily with armstrong.esi. The EsiHeaderMiddleware\\nsets the \\'X-ESI\\' header to \\'true\\' if the page request has esi tags on it.  To\\nenable esi processing in varnish for pages that need it, add the following to\\nyour vcl_fetch method:\\nif (beresp.http.X-ESI) {\\n    set beresp.do_esi = true;\\n}\\n\\n\\nLoading without ESI\\nThe template tag reads the DEBUG settings value and if set to True\\nrenders the view with the current request rather than including the\\n<esi:include> tag. This makes it easy to see fully rendered pages in development.\\n\\nInstallation & Configuration\\nYou can install the latest release of armstrong.esi using pip:\\npip install armstrong.apps.articles\\n\\nMake sure to add armstrong.esi to your INSTALLED_APPS.  You can\\nadd this however you like.  This works as a copy-and-paste solution:\\nINSTALLED_APPS += [\"armstrong.esi\"]\\n\\nYou must also enable the armstrong.esi middleware. To do this, add the following\\nline to your MIDDLEWARE_CLASSES:\\n\\'armstrong.esi.middleware.EsiHeaderMiddleware\\'\\n\\nIf you want to use the {% esi %} template tag mentioned above please also\\nadd the esi context processor to your TEMPLATE_CONTEXT_PROCESSORS\\nsetting:\\n\\'armstrong.esi.context_processors.esi\\'\\n\\n\\nContributing\\n\\nCreate something awesome -- make the code better, add some functionality,\\nwhatever (this is the hardest part).\\nFork it\\nCreate a topic branch to house your changes\\nGet all of your commits in the new topic branch\\nSubmit a pull request\\n\\n\\nState of Project\\nArmstrong is an open-source news platform that is freely available to any\\norganization.  It is the result of a collaboration between the Texas Tribune\\nand Bay Citizen, and a grant from the John S. and James L. Knight\\nFoundation.  The first release is scheduled for June, 2011.\\nTo follow development, be sure to join the Google Group.\\narmstrong.apps.articles is part of the Armstrong project.  You\\'re\\nprobably looking for that.\\n\\nLicense\\nCopyright 2011-2012 Bay Citizen and Texas Tribune\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': 'No readme file.', 'language': 'CSS'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTexas Reservoir Levels\\nA revamped version of one of the Texas Tribune's most popular data visualizations — a live tracker of the current state of Texas' reservoirs.\\nThis project was mostly an experiment with ECMAScript 6 with the help of the transpiler Babel. Browserify was also thrown in there for some extra fun because the JavaScript wasn't complicated enough!\\nBuilt using a greatly modified variation of News Apps' App Kit.\\nRequirements\\n\\nNode.js + npm\\nGulp installed globally\\n\\nInstallation\\nClone the repo, then install the dependencies.\\nnpm install\\nTo launch the test server, run:\\ngulp serve\\nTo build for production, run:\\ngulp --production\\n\\nWhere's the data?\\nThe app is being powered via a separate API that shoves the json file it uses where it expects it on S3. As a result, the data will not be available in this repo.\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': 'No readme file.', 'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nIPEDS Importer\\nThe purpose of this is to make it easier to interact with the IPEDS Data\\nCenter\\nThis project consists of several components:\\n\\n\\nMVL Generator -- If you\\'ve built your variables in the report builder,\\nyou\\'ll find that your reports are limited to 250 variables. This can be a\\nproblem because just getting a report with test scores will take up 207\\nvariables. Juggling the MVL files to upload reports can be a pain, but\\nthat\\'s still easier than dealing with creating a new set of variables.\\n\\n\\nCSV Downloader -- Once you have a lot of MVL variable files, running reports\\ngets tedious very quickly. The CSV Downloader automates running \"Compare\\nInstitution\" reports.\\n\\n\\nMVL Generator\\nUsage\\nRunning this project assumes you\\'re familiar with running Python projects.\\nBeyond that, it\\'s not too bad.\\nIn your virtualenv:\\npip install -r requirements.txt\\nmake resetdb\\npython manage.py createsuperuser\\nmake start\\n\\nBootstrapping some data:\\n\\nNavigate to \"Import MVL\" http://localhost:8000/ipeds_reporter/importer/\\nImport the sample fixture located at ipeds/reporter/fixtures/sample.mvl\\n\\nExporting all variables:\\n\\nNavigate to http://localhost:8000/ipeds_reporter/variable/\\nSelect all\\nClick the \"Select all ... variables\" link to select more than the first page\\nSelect the \"Make MVL\" admin action\\nSave that file someplace special\\n\\nHeroku\\nYou can play with a public Heroku install. Log in with the username guest\\nand the password guest. You\\'ll be able to browse and make MVL files, but not\\ncreate or upload new variables.\\nCSV Downloader\\nA CLI that launches a browser (using Selenium):\\ncsvdownloader/csv_downloader.py --help\\n\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nNews Apps Style Guide\\nThe Texas Tribune's framework for basic styles used on news apps projects.\\nCurrent Features\\n\\nColors\\nUtils\\nTypography\\nProse / Graphics Themes\\nButtons\\nTables\\nLogos\\n\\nQuickstart\\nWe've built this style guide using the News Apps' app kit. The project pulls styles from the News Apps' Styles repo.\\n\\n\\n\",\n",
       "  'language': 'HTML'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nSentry on Heroku\\n\\nSentry is a realtime event logging and aggregation platform.  At its core\\nit specializes in monitoring errors and extracting all the information\\nneeded to do a proper post-mortem without any of the hassle of the\\nstandard user feedback loop.\\n\\nQuick setup\\nClick the button below to automatically set up the Sentry in an app running on\\nyour Heroku account.\\n\\nFinally, you need to setup your first user:\\nheroku run \"sentry --config=sentry.conf.py createuser\" --app YOURAPPNAME\\n\\n\\nManual setup\\nFollow the steps below to get Sentry up and running on Heroku:\\n\\nCreate a new Heroku application. Replace \"APP_NAME\" with your\\napplication\\'s name:\\nheroku apps:create APP_NAME\\n\\n\\nAdd PostgresSQL to the application:\\nheroku addons:create heroku-postgresql:hobby-dev\\n\\n\\nAdd Redis to the application:\\nheroku addons:create heroku-redis:premium-0\\n\\n\\nSet Django\\'s secret key for cryptographic signing and Sentry\\'s shared secret\\nfor global administration privileges:\\nheroku config:set SECRET_KEY=$(python -c \"import base64, os; print(base64.b64encode(os.urandom(40)).decode())\")\\n\\n\\nSet the absolute URL to the Sentry root directory. The URL should not include\\na trailing slash. Replace the URL below with your application\\'s URL:\\nheroku config:set SENTRY_URL_PREFIX=https://sentry-example.herokuapp.com\\n\\n\\nDeploy Sentry to Heroku:\\ngit push heroku master\\n\\n\\nRun Sentry\\'s database migrations:\\nheroku run \"sentry --config=sentry.conf.py upgrade --noinput\"\\n\\n\\nCreate a user account for yourself:\\nheroku run \"sentry --config=sentry.conf.py createuser\"\\n\\n\\n\\nThat\\'s it!\\n\\nEmail notifications\\nFollow the steps below, if you want to enable Sentry\\'s email notifications:\\n\\nAdd SendGrid add-on to your Heroku application:\\nheroku addons:create sendgrid\\n\\n\\nSet the reply-to email address for outgoing mail:\\nheroku config:set SERVER_EMAIL=sentry@example.com\\n\\n\\n\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nCKEditor 4\\nThis is the Texas Tribune's dev version of CKEditor used for a minimalist editor\\nwithin the Django Admin.\\nWe're currently giving it a test drive on our flat pages.\\nIt includes a custom TT plugin for auto-linking politicians to their directory pages. If you want to test the\\nplugin, you'll need to run this editor instance on a simple server and proxy the port to domain name that\\nends in .texastribune.org.\\nDocumentation\\nThe Texas Tribune CKeditor wiki is available at: https://wiki.texastribune.org/x/OI0c.\\nThe full editor documentation is available online at the following address:\\nhttp://docs.ckeditor.com\\nDeployment\\nWe're serving the editor source from our CDN bucket\\non S3, which various templates connect to. Jenkins will automatically deploy changes to the master branch to the CDN bucket, and they will therefore be immediately(ish) effective on the production system, so be careful when uploading changes!\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nChartbuilder 📈\\nChartbuilder is a front-end charting application that facilitates easy creation of simple beautiful charts.\\nChartbuilder is the user and export interface. D4 is the default charting framework.\\nChartbuilder powers all chart creation on Atlas, a\\ncharting platform developed by Quartz.\\nWhat\\'s new in Chartbuilder 2.0\\n\\nThe Chart Grid type. Use it to create small multiples of bars, lines, dots, or columns.\\nThe app has been rewritten in React.js, making it easier to add new chart types or use\\nthird-party rendering libraries, like we are with D4.\\nChart edits are automatically saved to localStorage, and a chart can be\\nrecovered by clicking the \"load previous chart\" button on loading the page.\\nData input now accepts csv formatted data as well as tsv formated data\\nAll UI elements belong to Chartbuilder UI,\\na framework of flexible React components that you can use in other projects.\\n\\nWhat Chartbuilder is not\\n\\nA replacement for Excel\\nA replacement for Google Spreadsheet\\nA data analysis tool\\nA data transformation tool\\n\\nWhat Chartbuilder is\\nChartbuilder is the final step in charting to create charts in a consistent predefined style. Paste data into it and export the code to draw a mobile friendly responsive chart or a static svg or png chart.\\nWho is using Chartbuilder\\nOther than Quartz, customized Chartbuilder created charts have been seen in many publications:\\n\\nNPR\\nThe Wall Street Journal\\nCNBC\\nThe New Yorker\\nThe Press-Enterprise\\nNew Hampshire Public Radio\\nCFO Magazine\\nAustralian Broadcasting Corporation\\nDigiday\\n\\nGetting started with Chartbuilder\\nIf you are not interested in customizing the styles of your charts use the hosted version: http://quartz.github.io/Chartbuilder\\nTo work on the Chartbuilder code, first download the project and install\\ndependencies:\\n####Download via github\\n\\nMake sure you have a recent version of node.js (0.12 or above) (or io.js)\\nDownload source (and unzip or clone using git)\\nfrom the terminal navigate to the source folder (on a Mac: cd ~/Downloads/Chartbuilder-master/)\\nInstall the dependencies automatically by running npm install\\nStart the included web server by running npm run dev\\nPoint your browser to http://localhost:3000/\\nWhen you\\'re done developing, build and deploy your Chartbuilder!\\n\\n####Making a chart with Charbuilder\\n\\nHow to make a line chart with time series data\\nHow to make a bar chart with ranking data\\nHow to make a column chart with ordinal data\\n\\n####Customizing your Chartbuilder\\n\\nGetting to know the Chartbuilder code\\nCustomizing chartbuilder\\nTest things out\\nWhen you\\'re done developing, build and deploy your Chartbuilder!\\nKeep your customized version in sync with the master\\n\\nDocumentation\\n\\nThe Chartbuilder API docs\\ndocument most of the React components, classes, and utilities in the code base.\\n\\nDocumentation for Chartbuilder\\'s dependencies:\\n\\nD3\\nD4\\nReact\\n\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nThe News Apps App Kit\\nThis package helps jump start special Tribune features and series that are built outside the regular CMS. It's Tribune-centric, but easy to update and transform to fit your needs. If you're working on a graphic, use the News Apps Graphic Kit.\\nFeatures\\n\\nLive reloading and viewing powered by BrowserSync\\nCompiling of Sass/SCSS with libSass\\nCSS prefixing with autoprefixer\\nCSS sourcemaps with gulp-sourcemaps\\nCSS compression with clean-css\\nJavaScript linting with jshint\\nJavaScript compression with uglifyjs\\nTemplate compiling with nunjucks\\nImage compression with gulp-imagemin\\nAsset revisioning with gulp-rev and gulp-rev-replace\\n\\nQuickstart\\nRun this command in your project's folder:\\ncurl -fsSL https://github.com/texastribune/newsapps-app-kit/archive/master.tar.gz | tar -xz --strip-components=1\\nNext, npm install.\\nIf this is your first time to ever use the kit, you need to authorize your computer: npm run spreadsheet/authorize\\nAdd your Google sheet's ID to the config.json, and override any sheets that need to be processed differently. (keyvalue or objectlist)\\nNow get to work!\\nDevelopment\\nRun the following command to start the development server:\\ngulp serve\\nThen visit http://localhost:3000 to see your work.\\nConnect to S3\\nTo use the commands to deploy your project to Amazon S3, you'll need to add a profile to your ~/.aws/config. It should look something like this:\\n[profile newsapps]\\naws_access_key_id=YOUR_UNIQUE_ID\\naws_secret_access_key=YOUR_SECRET_ACCESS_KEY\\n\\nDeployment\\nRun these commands to build and deploy:\\ngulp\\nnpm run deploy\\n\\nThe project will deploy using the S3 bucket and slug found in your config.json.\\nAssets\\nThe graphics kit comes with an empty app/assets folder for you to store images, fonts and data files. The kit works best if you add these files to app/assets/images, app/assets/fonts and app/assets/data. These files will automatically be ignored by git hub, if added to the proper folders, to prevent a storage overload and to keep files locally that may have sensitive information in an open source project.\\nAvailable Commands\\nnpm run spreadsheet/authorize\\nAllows your computer to interact with the scraper. Only needs to be done once. Any future uses of the graphic kit can skip this.\\nnpm run spreadsheet/fetch\\nPulls down the project's spreadsheet and processes it into the data.json file.\\nnpm run spreadsheet/edit\\nOpens the project's spreadsheet in your browser.\\nnpm run serve\\nStarts the development server.\\nnpm run build\\nBuild the project for production.\\nnpm run deploy\\nDeploys the project.\\nnpm run assets/push\\nPushes the raw assets to the S3 bucket.\\nnpm run assets/pull\\nPulls the raw assets down to the local environment.\\n\\n\\n\",\n",
       "  'language': 'CSS'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nThe Shale Life\\nThe Texas Tribune's 15-part Shale Life series — the result of more than six months of reporting from the state's most active shale plays — to see how surging energy production is changing lives and fortunes across Texas. Check out the live version here. And read more on the significance of this project for The Texas Tribune.\\nThis project was built using the tt-newsapps-generator.\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': 'No readme file.', 'language': 'HTML'},\n",
       " {'readme': 'No readme file.', 'language': 'HTML'},\n",
       " {'readme': 'No readme file.', 'language': 'HTML'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nNews Shaman\\nNews Shaman is a hack from The Huffington Post and Change.org Editors Lab, April 8-9 2016.\\nBuilt by Liam Andrew, Kathryn Beaty, and Ben Hasson of The Texas Tribune.\\n\\n\\n',\n",
       "  'language': 'CSS'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nadstyles\\nThis is the style guide for Texas Tribune ads.\\nGetting started\\nTo get started, clone down the project repo.\\nIf you don't already have Ruby installed, you'll need to install it. This project uses Ruby 2.1.4.\\nYou'll need the Ruby gem bundler. If you need to install the bundler, run:\\ngem install bundler\\n\\nInstall the necessary gems from the Gemfile by running:\\nbundle install\\n\\nYou might find it helpful to use rbenv to manage your Ruby environment.\\nInitial setup\\nYou'll need to configure your authentication with Google docs for the middleman-google_drive gem. See the Setup section in the README for the gem here.\\nYou'll also need to set the GOOGLE_DRIVE_KEY environment variable to load data from Google docs.\\nDevelopment\\nMiddleman is configured to live reload as changes are made to files. To start up the Middleman server, run:\\nbundle exec middleman\\n\\nTo build the site, run:\\nbundle exec middleman build\\n\\nWhen Middleman builds, it creates a static file for each file located in the source folder. The build process is configured in config.rb.\\nDeploying\\nThe style guide deploys to Heroku.\\n\\n\\n\",\n",
       "  'language': 'Ruby'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nWARNING: If not careful you could configure an open relay with this.  Limit what can speak to port 25 of this container.\\nNotes:\\n\\n/var/log is exported as a VOLUME so you can view the postfix logs\\nuses SMTP_USER, SMTP_PASSWORD and SMTP_HOST as environment variables\\nsee Makefile\\n\\nhttps://www.linode.com/docs/email/postfix/postfix-smtp-debian7\\n\\n\\n\\n',\n",
       "  'language': 'Shell'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTedTracker\\nA single page app for tracking the travels of 2016 presidential candidate Ted Cruz. Built and maintained with the News Apps App Kit.\\nRequirements\\n\\nnode.js/io.js + npm\\ngulp\\nAWS CLI\\n\\nGetting Started\\ngit clone texastribune/tedtracker\\nnpm install\\nTo update the data from the Google spreadsheet, run npm run spreadsheet/fetch. Please note –\\xa0if you do not have access to that spreadsheet and/or have not been authenticated against access to Google's Drive API with the Texas Tribune's graphics app, this will not work for you. (This should only apply to those of you playing a long at home/not members of News Apps.)\\nYou'll then need to pull down the raw assets from S3. We don't commit images and the like.\\nnpm run assets/pull\\nTo test the site locally, run gulp serve.\\nDeployment\\nAlways be sure to get the latest data first!\\nnpm run spreadsheet/fetch\\nThen build the site:\\ngulp\\nAnd finally deploy:\\nnpm run deploy\\n(Similar caveats as above –\\xa0if you do not have clearance to access the Texas Tribune's S3 buckets this step will break for you.)\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\n500 Page\\nCloudflare lets us customize our 500 page by giving it the address of a static page. In order to show any changes to\\nthe branch, you\\'ll have to republish the page in the Cloudflare.\\nIf readers are seeing this page, the site\\'s in pretty bad shape, so we shouldn\\'t include a nav bar or direct\\nthem anywhere that\\'s just going to error out again. Hence the Twitter feed.\\nFrom the Cloudflare docs: The maximum customized page size is 1.5 MB.\\ndeploying\\nFrom the Cloudflare admin, find the 500 page section. Choose \\'customize\\' and put the url of the raw\\nindex file in the admin.\\nstyling\\nAn example of the HTML generated by the 500 token is as follows (presumably the content changes according to which 500 error):\\n<div class=\"cf-error-details cf-error-522\">\\n  <h1>Connection timed out</h1>\\n  <p data-translate=\"connection_timed_out\">The initial connection between CloudFlare\\'s network and the origin web server timed out. As a result, the web page can not be displayed.</p>\\n  <ul class=\"cferror_details\">\\n    <li>Ray ID: 000000000000000</li>\\n    <li>Your IP address: 71.41.155.50</li>\\n    <li>Error reference number: 522</li>\\n    <li>CloudFlare Location: San Jose</li>\\n  </ul>\\n</div>\\n\\n\\n',\n",
       "  'language': 'HTML'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTinypass API example in Python\\nThis is just an example use of the Tinypass REST API in python.\\nFor more information see here: http://developer.tinypass.com/main/restapi\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nCKEditor 4 - Texas Tribune branch\\nNotes\\nThe purpose of this repo is to put the editor used in Tribtalk into our version control since we will likely add features as time goes one.\\nWe may want to consider using it in the main TT project as well.\\nTODO: create a deployment procedure. Right now it lives on our S3 cdn.\\n\\nThis repository contains the development version of CKEditor.\\nAttention: The code in this repository should be used locally and for\\ndevelopment purposes only. We do not recommend using it in production environment\\nbecause the user experience will be very limited. For that purpose, you should\\neither build the editor (see below) or use an official release available on the\\nCKEditor website.\\nCode Installation\\nThere is no special installation procedure to install the development code.\\nSimply clone it to any local directory and you are set.\\nAvailable Branches\\nThis repository contains the following branches:\\n\\nmaster – Development of the upcoming minor release.\\nmajor – Development of the upcoming major release.\\nstable – Latest stable release tag point (non-beta).\\nlatest – Latest release tag point (including betas).\\nrelease/A.B.x (e.g. 4.0.x, 4.1.x) – Release freeze, tests and tagging.\\nHotfixing.\\n\\nNote that both master and major are under heavy development. Their\\ncode did not pass the release testing phase, though, so it may be unstable.\\nAdditionally, all releases have their respective tags in the following form: 4.4.0,\\n4.4.1, etc.\\nSamples\\nThe samples/ folder contains some examples that can be used to test your\\ninstallation. Visit CKEditor SDK for plenty of samples\\nshowcasing numerous editor features, with source code readily available to view, copy\\nand use in your own solution.\\nCode Structure\\nThe development code contains the following main elements:\\n\\nMain coding folders:\\n\\ncore/ – The core API of CKEditor. Alone, it does nothing, but\\nit provides the entire JavaScript API that makes the magic happen.\\nplugins/ – Contains most of the plugins maintained by the CKEditor core team.\\nskin/ – Contains the official default skin of CKEditor.\\ndev/ – Contains some developer tools.\\ntests/ – Contains the CKEditor tests suite.\\n\\n\\n\\nBuilding a Release\\nA release-optimized version of the development code can be easily created\\nlocally. The dev/builder/build.sh script can be used for that purpose:\\n> ./dev/builder/build.sh\\n\\nA \"release ready\" working copy of your development code will be built in the new\\ndev/builder/release/ folder. An Internet connection is necessary to run the\\nbuilder, for its first time at least.\\nTesting Environment\\nRead more on how to set up the environment and execute tests in the CKEditor Testing Environment guide.\\nReporting Issues\\nPlease use the CKEditor Developer Center to report bugs and feature requests.\\nLicense\\nCopyright (c) 2003-2016, CKSource - Frederico Knabben. All rights reserved.\\nFor licensing, see LICENSE.md or http://ckeditor.com/license\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nav-tools\\ntools for audio, video, podcasting\\nlivestream-to-mp3.sh\\nTo use this first copy the intro.wav file to the current directory. Then grab the URL to the video file. This can be obtained on livestream.com from the share button underneath the video. Then run it like so:\\n./livestream-to-mp3.sh <video url>\\n\\nFor example:\\n./livestream-to-mp3.sh http://livestream.com/texastribune/events/4773607/videos/111858691\\n\\nIt will run for a while and if it all works you should have an output.mp3 that has the intro appended to the front of the audio from the video.\\n\\n\\n',\n",
       "  'language': 'Shell'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nPen Brackets\\nA snapshot of who's running in the 2016 election.\\n\\n\\n\",\n",
       "  'language': 'HTML'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nHospital Explorer (webapp)\\nRequirements\\nTo run this app you will need on your computer:\\n\\nRuby 2.1.2\\nNode.js\\nBower\\nbundler\\n\\nI recommend: rbenv and nvm.\\nBuilding\\nbundle install\\nbower install\\nmiddleman server\\n\\nDon't forget to run rbenv rehash after installing gems to enable the binaries. Only if you are using rbenv.\\nBefore deploying\\nmiddleman build\\nmiddleman s3_sync\\n\\nKeep in mind that you will need to change BUCKET y config.rb to point the deploy to your bucket. Also you will need to have AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY defined as environmental variables on your system. And if you want special configurations on your deploy, check first this.\\njshint\\nTo check how is your JavaScript going, just run:\\nrake jshint\\n\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nRSS to JSON\\nA quick fix while The Texas Tribune's API catches up! Converts our summary RSS feeds into usable JSON and pushes them up to S3.\\nSetup and Usage (without Docker)\\nFirst, install dependencies.\\nnpm install\\nThen, you'll need to make the aws-sdk library happy with AWS credentials. There are multiple ways to do this, so pick your poison. (I recommend the env variables if you do not already have the ~/.aws/credentials file set up.)\\nYou'll also need to set the AWS_S3_BUCKET environmental variable so the library knows where to push the files.\\nThe feeds that will be pulled in are listed in source.js. Add or edit to that if necessary, then run the start command.\\nnpm start\\nSetup and Usage (with Docker)\\nBuild the container using the handy Make command.\\nmake docker/build\\nThen you need to figure out how to get the AWS credentials and bucket location in. You can either go the env variable file route, or pass them in directly when you docker run.\\nIf you went the file route:\\ndocker run -it --rm --env-file=env-docker rsstojson\\nAnd the direct env variable route:\\ndocker run -it --rm --env AWS_ACCESS_KEY_ID=... --env AWS_SECRET_ACCESS_KEY=... --env AWS_S3_BUCKET=... rsstojson\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\n===\\ninfoNavigator\\n===\\ninfoNavigator takes your models and makes denormalized tables for all of them so that internal users can explore them\\n\\nQuick start\\n\\n\\nAdd \"infoNavigator\" to your INSTALLED_APPS setting like this::\\n\\nINSTALLED_APPS = (\\n...\\n\\'infoNavigator\\',\\n\\n\\n)\\n\\n\\n\\n\\nGet a url to all those models within your django project like this:\\nfrom infoNavigator.views import RecordsView\\nurl(r\\'^navigate/\\', RecordsView(models.Keyword, models.SICCode, models.NAICSCode,\\n\\nmodels.OSHA170Form, models.ReportingJurisdiction,\\nmodels.Accident, models.AccidentAbstract,\\nmodels.AccidentInjury, models.Inspection,\\nmodels.OptionalInspectionInfo, models.RelatedActivity,\\nmodels.StrategicCode, models.Violation,\\nmodels.ViolationEvent,\\nmodels.ViolationGeneralDuty).as_view()),\\n\\n\\n\\n\\nVisit the url you set up\\n\\n\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nHospitals Importer\\nRequirements\\n\\ncsv2json\\nYou will need hospitals-201X.json. Old json data file.\\nYou will need a Hospital_Data_Merge_TX.csv file with all the hospitals information on it.\\nYou should have all the addresses and hospital's names edited before running the script.\\nAll the csv files should be in a folder like data/12-18-2014.\\nIf you have a missing file check the spelling and the case. You should update manifest.json.\\n\\nGenerate JSON files\\npython import.py data/WHEREVER_YOU_HAVE_YOUR_CSV_FILES\\n\\nThe output will be copied into output. Once you have the json and geojson files you should copy them into hospitals.texastribune.org/data.\\nKeep in mind that you will need the old data files to show old info.\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nreveal.js \\nA framework for easily creating beautiful presentations using HTML. Check out the live demo.\\nreveal.js comes with a broad range of features including nested slides, Markdown contents, PDF export, speaker notes and a JavaScript API. It\\'s best viewed in a modern browser but fallbacks are available to make sure your presentation can still be viewed elsewhere.\\nMore reading:\\n\\nInstallation: Step-by-step instructions for getting reveal.js running on your computer.\\nChangelog: Up-to-date version history.\\nExamples: Presentations created with reveal.js, add your own!\\nBrowser Support: Explanation of browser support and fallbacks.\\nPlugins: A list of plugins that can be used to extend reveal.js.\\n\\nOnline Editor\\nPresentations are written using HTML or Markdown but there\\'s also an online editor for those of you who prefer a graphical interface. Give it a try at http://slides.com.\\nInstructions\\nMarkup\\nMarkup hierarchy needs to be <div class=\"reveal\"> <div class=\"slides\"> <section> where the <section> represents one slide and can be repeated indefinitely. If you place multiple <section>\\'s inside of another <section> they will be shown as vertical slides. The first of the vertical slides is the \"root\" of the others (at the top), and it will be included in the horizontal sequence. For example:\\n<div class=\"reveal\">\\n\\t<div class=\"slides\">\\n\\t\\t<section>Single Horizontal Slide</section>\\n\\t\\t<section>\\n\\t\\t\\t<section>Vertical Slide 1</section>\\n\\t\\t\\t<section>Vertical Slide 2</section>\\n\\t\\t</section>\\n\\t</div>\\n</div>\\nMarkdown\\nIt\\'s possible to write your slides using Markdown. To enable Markdown, add the data-markdown attribute to your <section> elements and wrap the contents in a <script type=\"text/template\"> like the example below.\\nThis is based on data-markdown from Paul Irish modified to use marked to support Github Flavoured Markdown. Sensitive to indentation (avoid mixing tabs and spaces) and line breaks (avoid consecutive breaks).\\n<section data-markdown>\\n\\t<script type=\"text/template\">\\n\\t\\t## Page title\\n\\n\\t\\tA paragraph with some text and a [link](http://hakim.se).\\n\\t</script>\\n</section>\\nExternal Markdown\\nYou can write your content as a separate file and have reveal.js load it at runtime. Note the separator arguments which determine how slides are delimited in the external file. The data-charset attribute is optional and specifies which charset to use when loading the external file.\\nWhen used locally, this feature requires that reveal.js runs from a local web server.\\n<section data-markdown=\"example.md\"  \\n         data-separator=\"^\\\\n\\\\n\\\\n\"  \\n         data-separator-vertical=\"^\\\\n\\\\n\"  \\n         data-separator-notes=\"^Note:\"  \\n         data-charset=\"iso-8859-15\">\\n</section>\\nElement Attributes\\nSpecial syntax (in html comment) is available for adding attributes to Markdown elements. This is useful for fragments, amongst other things.\\n<section data-markdown>\\n\\t<script type=\"text/template\">\\n\\t\\t- Item 1 <!-- .element: class=\"fragment\" data-fragment-index=\"2\" -->\\n\\t\\t- Item 2 <!-- .element: class=\"fragment\" data-fragment-index=\"1\" -->\\n\\t</script>\\n</section>\\nSlide Attributes\\nSpecial syntax (in html comment) is available for adding attributes to the slide <section> elements generated by your Markdown.\\n<section data-markdown>\\n\\t<script type=\"text/template\">\\n\\t<!-- .slide: data-background=\"#ff0000\" -->\\n\\t\\tMarkdown content\\n\\t</script>\\n</section>\\nConfiguration\\nAt the end of your page you need to initialize reveal by running the following code. Note that all config values are optional and will default as specified below.\\nReveal.initialize({\\n\\n\\t// Display controls in the bottom right corner\\n\\tcontrols: true,\\n\\n\\t// Display a presentation progress bar\\n\\tprogress: true,\\n\\n\\t// Display the page number of the current slide\\n\\tslideNumber: false,\\n\\n\\t// Push each slide change to the browser history\\n\\thistory: false,\\n\\n\\t// Enable keyboard shortcuts for navigation\\n\\tkeyboard: true,\\n\\n\\t// Enable the slide overview mode\\n\\toverview: true,\\n\\n\\t// Vertical centering of slides\\n\\tcenter: true,\\n\\n\\t// Enables touch navigation on devices with touch input\\n\\ttouch: true,\\n\\n\\t// Loop the presentation\\n\\tloop: false,\\n\\n\\t// Change the presentation direction to be RTL\\n\\trtl: false,\\n\\n\\t// Turns fragments on and off globally\\n\\tfragments: true,\\n\\n\\t// Flags if the presentation is running in an embedded mode,\\n\\t// i.e. contained within a limited portion of the screen\\n\\tembedded: false,\\n\\n\\t// Flags if we should show a help overlay when the questionmark\\n\\t// key is pressed\\n\\thelp: true,\\n\\n\\t// Flags if speaker notes should be visible to all viewers\\n\\tshowNotes: false,\\n\\n\\t// Number of milliseconds between automatically proceeding to the\\n\\t// next slide, disabled when set to 0, this value can be overwritten\\n\\t// by using a data-autoslide attribute on your slides\\n\\tautoSlide: 0,\\n\\n\\t// Stop auto-sliding after user input\\n\\tautoSlideStoppable: true,\\n\\n\\t// Enable slide navigation via mouse wheel\\n\\tmouseWheel: false,\\n\\n\\t// Hides the address bar on mobile devices\\n\\thideAddressBar: true,\\n\\n\\t// Opens links in an iframe preview overlay\\n\\tpreviewLinks: false,\\n\\n\\t// Transition style\\n\\ttransition: \\'default\\', // none/fade/slide/convex/concave/zoom\\n\\n\\t// Transition speed\\n\\ttransitionSpeed: \\'default\\', // default/fast/slow\\n\\n\\t// Transition style for full page slide backgrounds\\n\\tbackgroundTransition: \\'default\\', // none/fade/slide/convex/concave/zoom\\n\\n\\t// Number of slides away from the current that are visible\\n\\tviewDistance: 3,\\n\\n\\t// Parallax background image\\n\\tparallaxBackgroundImage: \\'\\', // e.g. \"\\'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg\\'\"\\n\\n\\t// Parallax background size\\n\\tparallaxBackgroundSize: \\'\\', // CSS syntax, e.g. \"2100px 900px\"\\n\\n\\t// Amount to move parallax background (horizontal and vertical) on slide change\\n\\t// Number, e.g. 100\\n\\tparallaxBackgroundHorizontal: \\'\\',\\n\\tparallaxBackgroundVertical: \\'\\'\\n\\n});\\nThe configuration can be updated after initialization using the configure method:\\n// Turn autoSlide off\\nReveal.configure({ autoSlide: 0 });\\n\\n// Start auto-sliding every 5s\\nReveal.configure({ autoSlide: 5000 });\\nDependencies\\nReveal.js doesn\\'t rely on any third party scripts to work but a few optional libraries are included by default. These libraries are loaded as dependencies in the order they appear, for example:\\nReveal.initialize({\\n\\tdependencies: [\\n\\t\\t// Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/\\n\\t\\t{ src: \\'lib/js/classList.js\\', condition: function() { return !document.body.classList; } },\\n\\n\\t\\t// Interpret Markdown in <section> elements\\n\\t\\t{ src: \\'plugin/markdown/marked.js\\', condition: function() { return !!document.querySelector( \\'[data-markdown]\\' ); } },\\n\\t\\t{ src: \\'plugin/markdown/markdown.js\\', condition: function() { return !!document.querySelector( \\'[data-markdown]\\' ); } },\\n\\n\\t\\t// Syntax highlight for <code> elements\\n\\t\\t{ src: \\'plugin/highlight/highlight.js\\', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },\\n\\n\\t\\t// Zoom in and out with Alt+click\\n\\t\\t{ src: \\'plugin/zoom-js/zoom.js\\', async: true },\\n\\n\\t\\t// Speaker notes\\n\\t\\t{ src: \\'plugin/notes/notes.js\\', async: true },\\n\\n\\t\\t// Remote control your reveal.js presentation using a touch device\\n\\t\\t{ src: \\'plugin/remotes/remotes.js\\', async: true },\\n\\n\\t\\t// MathJax\\n\\t\\t{ src: \\'plugin/math/math.js\\', async: true }\\n\\t]\\n});\\nYou can add your own extensions using the same syntax. The following properties are available for each dependency object:\\n\\nsrc: Path to the script to load\\nasync: [optional] Flags if the script should load after reveal.js has started, defaults to false\\ncallback: [optional] Function to execute when the script has loaded\\ncondition: [optional] Function which must return true for the script to be loaded\\n\\nReady Event\\nA \\'ready\\' event is fired when reveal.js has loaded all non-async dependencies and is ready to start navigating. To check if reveal.js is already \\'ready\\' you can call Reveal.isReady().\\nReveal.addEventListener( \\'ready\\', function( event ) {\\n\\t// event.currentSlide, event.indexh, event.indexv\\n} );\\nPresentation Size\\nAll presentations have a normal size, that is the resolution at which they are authored. The framework will automatically scale presentations uniformly based on this size to ensure that everything fits on any given display or viewport.\\nSee below for a list of configuration options related to sizing, including default values:\\nReveal.initialize({\\n\\n\\t...\\n\\n\\t// The \"normal\" size of the presentation, aspect ratio will be preserved\\n\\t// when the presentation is scaled to fit different resolutions. Can be\\n\\t// specified using percentage units.\\n\\twidth: 960,\\n\\theight: 700,\\n\\n\\t// Factor of the display size that should remain empty around the content\\n\\tmargin: 0.1,\\n\\n\\t// Bounds for smallest/largest possible scale to apply to content\\n\\tminScale: 0.2,\\n\\tmaxScale: 1.5\\n\\n});\\nAuto-sliding\\nPresentations can be configured to progress through slides automatically, without any user input. To enable this you will need to tell the framework how many milliseconds it should wait between slides:\\n// Slide every five seconds\\nReveal.configure({\\n  autoSlide: 5000\\n});\\nWhen this is turned on a control element will appear that enables users to pause and resume auto-sliding. Alternatively, sliding can be paused or resumed by pressing »a« on the keyboard. Sliding is paused automatically as soon as the user starts navigating. You can disable these controls by specifying autoSlideStoppable: false in your reveal.js config.\\nYou can also override the slide duration for individual slides and fragments by using the data-autoslide attribute:\\n<section data-autoslide=\"2000\">\\n\\t<p>After 2 seconds the first fragment will be shown.</p>\\n\\t<p class=\"fragment\" data-autoslide=\"10000\">After 10 seconds the next fragment will be shown.</p>\\n\\t<p class=\"fragment\">Now, the fragment is displayed for 2 seconds before the next slide is shown.</p>\\n</section>\\nWhenever the auto-slide mode is resumed or paused the autoslideresumed and autoslidepaused events are fired.\\nKeyboard Bindings\\nIf you\\'re unhappy with any of the default keyboard bindings you can override them using the keyboard config option:\\nReveal.configure({\\n  keyboard: {\\n    13: \\'next\\', // go to the next slide when the ENTER key is pressed\\n    27: function() {}, // do something custom when ESC is pressed\\n    32: null // don\\'t do anything when SPACE is pressed (i.e. disable a reveal.js default binding)\\n  }\\n});\\nTouch Navigation\\nYou can swipe to navigate through a presentation on any touch-enabled device. Horizontal swipes change between horizontal slides, vertical swipes change between vertical slides. If you wish to disable this you can set the touch config option to false when initializing reveal.js.\\nIf there\\'s some part of your content that needs to remain accessible to touch events you\\'ll need to highlight this by adding a data-prevent-swipe attribute to the element. One common example where this is useful is elements that need to be scrolled.\\nLazy Loading\\nWhen working on presentation with a lot of media or iframe content it\\'s important to load lazily. Lazy loading means that reveal.js will only load content for the few slides nearest to the current slide. The number of slides that are preloaded is determined by the viewDistance configuration option.\\nTo enable lazy loading all you need to do is change your \"src\" attributes to \"data-src\" as shown below. This is supported for image, video, audio and iframe elements. Lazy loaded iframes will also unload when the containing slide is no longer visible.\\n<section>\\n  <img data-src=\"image.png\">\\n  <iframe data-src=\"http://hakim.se\"></iframe>\\n  <video>\\n    <source data-src=\"video.webm\" type=\"video/webm\" />\\n    <source data-src=\"video.mp4\" type=\"video/mp4\" />\\n  </video>\\n</section>\\nAPI\\nThe Reveal object exposes a JavaScript API for controlling navigation and reading state:\\n// Navigation\\nReveal.slide( indexh, indexv, indexf );\\nReveal.left();\\nReveal.right();\\nReveal.up();\\nReveal.down();\\nReveal.prev();\\nReveal.next();\\nReveal.prevFragment();\\nReveal.nextFragment();\\n\\n// Toggle presentation states, optionally pass true/false to force on/off\\nReveal.toggleOverview();\\nReveal.togglePause();\\nReveal.toggleAutoSlide();\\n\\n// Change a config value at runtime\\nReveal.configure({ controls: true });\\n\\n// Returns the present configuration options\\nReveal.getConfig();\\n\\n// Fetch the current scale of the presentation\\nReveal.getScale();\\n\\n// Retrieves the previous and current slide elements\\nReveal.getPreviousSlide();\\nReveal.getCurrentSlide();\\n\\nReveal.getIndices(); // { h: 0, v: 0 } }\\nReveal.getProgress(); // 0-1\\nReveal.getTotalSlides();\\n\\n// Returns the speaker notes for the current slide\\nReveal.getSlideNotes();\\n\\n// State checks\\nReveal.isFirstSlide();\\nReveal.isLastSlide();\\nReveal.isOverview();\\nReveal.isPaused();\\nReveal.isAutoSliding();\\nSlide Changed Event\\nA \\'slidechanged\\' event is fired each time the slide is changed (regardless of state). The event object holds the index values of the current slide as well as a reference to the previous and current slide HTML nodes.\\nSome libraries, like MathJax (see #226), get confused by the transforms and display states of slides. Often times, this can be fixed by calling their update or render function from this callback.\\nReveal.addEventListener( \\'slidechanged\\', function( event ) {\\n\\t// event.previousSlide, event.currentSlide, event.indexh, event.indexv\\n} );\\nPresentation State\\nThe presentation\\'s current state can be fetched by using the getState method. A state object contains all of the information required to put the presentation back as it was when getState was first called. Sort of like a snapshot. It\\'s a simple object that can easily be stringified and persisted or sent over the wire.\\nReveal.slide( 1 );\\n// we\\'re on slide 1\\n\\nvar state = Reveal.getState();\\n\\nReveal.slide( 3 );\\n// we\\'re on slide 3\\n\\nReveal.setState( state );\\n// we\\'re back on slide 1\\nSlide States\\nIf you set data-state=\"somestate\" on a slide <section>, \"somestate\" will be applied as a class on the document element when that slide is opened. This allows you to apply broad style changes to the page based on the active slide.\\nFurthermore you can also listen to these changes in state via JavaScript:\\nReveal.addEventListener( \\'somestate\\', function() {\\n\\t// TODO: Sprinkle magic\\n}, false );\\nSlide Backgrounds\\nSlides are contained within a limited portion of the screen by default to allow them to fit any display and scale uniformly. You can apply full page backgrounds outside of the slide area by adding a data-background attribute to your <section> elements. Four different types of backgrounds are supported: color, image, video and iframe. Below are a few examples.\\n<section data-background=\"#ff0000\">\\n\\t<h2>All CSS color formats are supported, like rgba() or hsl().</h2>\\n</section>\\n<section data-background=\"http://example.com/image.png\">\\n\\t<h2>This slide will have a full-size background image.</h2>\\n</section>\\n<section data-background=\"http://example.com/image.png\" data-background-size=\"100px\" data-background-repeat=\"repeat\">\\n\\t<h2>This background image will be sized to 100px and repeated.</h2>\\n</section>\\n<section data-background-video=\"https://s3.amazonaws.com/static.slid.es/site/homepage/v1/homepage-video-editor.mp4,https://s3.amazonaws.com/static.slid.es/site/homepage/v1/homepage-video-editor.webm\" data-background-video-loop>\\n\\t<h2>Video. Multiple sources can be defined using a comma separated list. Video will loop when the data-background-video-loop attribute is provided.</h2>\\n</section>\\n<section data-background-iframe=\"https://slides.com\">\\n\\t<h2>Embeds a web page as a background. Note that the page won\\'t be interactive.</h2>\\n</section>\\nBackgrounds transition using a fade animation by default. This can be changed to a linear sliding transition by passing backgroundTransition: \\'slide\\' to the Reveal.initialize() call. Alternatively you can set data-background-transition on any section with a background to override that specific transition.\\nParallax Background\\nIf you want to use a parallax scrolling background, set the first two config properties below when initializing reveal.js (the other two are optional).\\nReveal.initialize({\\n\\n\\t// Parallax background image\\n\\tparallaxBackgroundImage: \\'\\', // e.g. \"https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg\"\\n\\n\\t// Parallax background size\\n\\tparallaxBackgroundSize: \\'\\', // CSS syntax, e.g. \"2100px 900px\" - currently only pixels are supported (don\\'t use % or auto)\\n\\n\\t// Amount of pixels to move the parallax background per slide step,\\n\\t// a value of 0 disables movement along the given axis\\n\\t// These are optional, if they aren\\'t specified they\\'ll be calculated automatically\\n\\tparallaxBackgroundHorizontal: 200,\\n\\tparallaxBackgroundVertical: 50\\n\\n});\\nMake sure that the background size is much bigger than screen size to allow for some scrolling. View example.\\nSlide Transitions\\nThe global presentation transition is set using the transition config value. You can override the global transition for a specific slide by using the data-transition attribute:\\n<section data-transition=\"zoom\">\\n\\t<h2>This slide will override the presentation transition and zoom!</h2>\\n</section>\\n\\n<section data-transition-speed=\"fast\">\\n\\t<h2>Choose from three transition speeds: default, fast or slow!</h2>\\n</section>\\nYou can also use different in and out transitions for the same slide:\\n<section data-transition=\"slide\">\\n    The train goes on … \\n</section>\\n<section data-transition=\"slide\"> \\n    and on … \\n</section>\\n<section data-transition=\"slide-in fade-out\"> \\n    and stops.\\n</section>\\n<section data-transition=\"fade-in slide-out\"> \\n    (Passengers entering and leaving)\\n</section>\\n<section data-transition=\"slide\">\\n    And it starts again.\\n</section>\\nNote that this does not work with the page and cube transitions.\\nInternal links\\nIt\\'s easy to link between slides. The first example below targets the index of another slide whereas the second targets a slide with an ID attribute (<section id=\"some-slide\">):\\n<a href=\"#/2/2\">Link</a>\\n<a href=\"#/some-slide\">Link</a>\\nYou can also add relative navigation links, similar to the built in reveal.js controls, by appending one of the following classes on any element. Note that each element is automatically given an enabled class when it\\'s a valid navigation route based on the current slide.\\n<a href=\"#\" class=\"navigate-left\">\\n<a href=\"#\" class=\"navigate-right\">\\n<a href=\"#\" class=\"navigate-up\">\\n<a href=\"#\" class=\"navigate-down\">\\n<a href=\"#\" class=\"navigate-prev\"> <!-- Previous vertical or horizontal slide -->\\n<a href=\"#\" class=\"navigate-next\"> <!-- Next vertical or horizontal slide -->\\nFragments\\nFragments are used to highlight individual elements on a slide. Every element with the class fragment will be stepped through before moving on to the next slide. Here\\'s an example: http://lab.hakim.se/reveal-js/#/fragments\\nThe default fragment style is to start out invisible and fade in. This style can be changed by appending a different class to the fragment:\\n<section>\\n\\t<p class=\"fragment grow\">grow</p>\\n\\t<p class=\"fragment shrink\">shrink</p>\\n\\t<p class=\"fragment fade-out\">fade-out</p>\\n\\t<p class=\"fragment current-visible\">visible only once</p>\\n\\t<p class=\"fragment highlight-current-blue\">blue only once</p>\\n\\t<p class=\"fragment highlight-red\">highlight-red</p>\\n\\t<p class=\"fragment highlight-green\">highlight-green</p>\\n\\t<p class=\"fragment highlight-blue\">highlight-blue</p>\\n</section>\\nMultiple fragments can be applied to the same element sequentially by wrapping it, this will fade in the text on the first step and fade it back out on the second.\\n<section>\\n\\t<span class=\"fragment fade-in\">\\n\\t\\t<span class=\"fragment fade-out\">I\\'ll fade in, then out</span>\\n\\t</span>\\n</section>\\nThe display order of fragments can be controlled using the data-fragment-index attribute.\\n<section>\\n\\t<p class=\"fragment\" data-fragment-index=\"3\">Appears last</p>\\n\\t<p class=\"fragment\" data-fragment-index=\"1\">Appears first</p>\\n\\t<p class=\"fragment\" data-fragment-index=\"2\">Appears second</p>\\n</section>\\nFragment events\\nWhen a slide fragment is either shown or hidden reveal.js will dispatch an event.\\nSome libraries, like MathJax (see #505), get confused by the initially hidden fragment elements. Often times this can be fixed by calling their update or render function from this callback.\\nReveal.addEventListener( \\'fragmentshown\\', function( event ) {\\n\\t// event.fragment = the fragment DOM element\\n} );\\nReveal.addEventListener( \\'fragmenthidden\\', function( event ) {\\n\\t// event.fragment = the fragment DOM element\\n} );\\nCode syntax highlighting\\nBy default, Reveal is configured with highlight.js for code syntax highlighting. Below is an example with clojure code that will be syntax highlighted. When the data-trim attribute is present surrounding whitespace is automatically removed.\\n<section>\\n\\t<pre><code data-trim>\\n(def lazy-fib\\n  (concat\\n   [0 1]\\n   ((fn rfib [a b]\\n        (lazy-cons (+ a b) (rfib b (+ a b)))) 0 1)))\\n\\t</code></pre>\\n</section>\\nSlide number\\nIf you would like to display the page number of the current slide you can do so using the slideNumber configuration value.\\n// Shows the slide number using default formatting\\nReveal.configure({ slideNumber: true });\\n\\n// Slide number formatting can be configured using these variables:\\n//  h: current slide\\'s horizontal index\\n//  v: current slide\\'s vertical index\\n//  c: current slide index (flattened)\\n//  t: total number of slides (flattened)\\nReveal.configure({ slideNumber: \\'c / t\\' });\\n\\nOverview mode\\nPress \"Esc\" or \"o\" keys to toggle the overview mode on and off. While you\\'re in this mode, you can still navigate between slides,\\nas if you were at 1,000 feet above your presentation. The overview mode comes with a few API hooks:\\nReveal.addEventListener( \\'overviewshown\\', function( event ) { /* ... */ } );\\nReveal.addEventListener( \\'overviewhidden\\', function( event ) { /* ... */ } );\\n\\n// Toggle the overview mode programmatically\\nReveal.toggleOverview();\\nFullscreen mode\\nJust press »F« on your keyboard to show your presentation in fullscreen mode. Press the »ESC« key to exit fullscreen mode.\\nEmbedded media\\nEmbedded HTML5 <video>/<audio> and YouTube iframes are automatically paused when you navigate away from a slide. This can be disabled by decorating your element with a data-ignore attribute.\\nAdd data-autoplay to your media element if you want it to automatically start playing when the slide is shown:\\n<video data-autoplay src=\"http://clips.vorwaerts-gmbh.de/big_buck_bunny.mp4\"></video>\\nAdditionally the framework automatically pushes two post messages to all iframes, slide:start when the slide containing the iframe is made visible and slide:stop when it is hidden.\\nStretching elements\\nSometimes it\\'s desirable to have an element, like an image or video, stretch to consume as much space as possible within a given slide. This can be done by adding the .stretch class to an element as seen below:\\n<section>\\n\\t<h2>This video will use up the remaining space on the slide</h2>\\n    <video class=\"stretch\" src=\"http://clips.vorwaerts-gmbh.de/big_buck_bunny.mp4\"></video>\\n</section>\\nLimitations:\\n\\nOnly direct descendants of a slide section can be stretched\\nOnly one descendant per slide section can be stretched\\n\\npostMessage API\\nThe framework has a built-in postMessage API that can be used when communicating with a presentation inside of another window. Here\\'s an example showing how you\\'d make a reveal.js instance in the given window proceed to slide 2:\\n<window>.postMessage( JSON.stringify({ method: \\'slide\\', args: [ 2 ] }), \\'*\\' );\\nWhen reveal.js runs inside of an iframe it can optionally bubble all of its events to the parent. Bubbled events are stringified JSON with three fields: namespace, eventName and state. Here\\'s how you subscribe to them from the parent window:\\nwindow.addEventListener( \\'message\\', function( event ) {\\n\\tvar data = JSON.parse( event.data );\\n\\tif( data.namespace === \\'reveal\\' && data.eventName =\\'slidechanged\\' ) {\\n\\t\\t// Slide changed, see data.state for slide number\\n\\t}\\n} );\\nThis cross-window messaging can be toggled on or off using configuration flags.\\nReveal.initialize({\\n\\t...,\\n\\n\\t// Exposes the reveal.js API through window.postMessage\\n\\tpostMessage: true,\\n\\n\\t// Dispatches all reveal.js events to the parent window through postMessage\\n\\tpostMessageEvents: false\\n});\\nPDF Export\\nPresentations can be exported to PDF via a special print stylesheet. This feature requires that you use Google Chrome or Chromium.\\nHere\\'s an example of an exported presentation that\\'s been uploaded to SlideShare: http://www.slideshare.net/hakimel/revealjs-300.\\n\\nOpen your presentation with print-pdf included anywhere in the query string. This triggers the default index HTML to load the PDF print stylesheet (css/print/pdf.css). You can test this with lab.hakim.se/reveal-js?print-pdf.\\nOpen the in-browser print dialog (CMD+P).\\nChange the Destination setting to Save as PDF.\\nChange the Layout to Landscape.\\nChange the Margins to None.\\nClick Save.\\n\\n\\nAlternatively you can use the decktape project.\\nTheming\\nThe framework comes with a few different themes included:\\n\\nblack: Black background, white text, blue links (default theme)\\nwhite: White background, black text, blue links\\nleague: Gray background, white text, blue links (default theme for reveal.js < 3.0.0)\\nbeige: Beige background, dark text, brown links\\nsky: Blue background, thin dark text, blue links\\nnight: Black background, thick white text, orange links\\nserif: Cappuccino background, gray text, brown links\\nsimple: White background, black text, blue links\\nsolarized: Cream-colored background, dark green text, blue links\\n\\nEach theme is available as a separate stylesheet. To change theme you will need to replace black below with your desired theme name in index.html:\\n<link rel=\"stylesheet\" href=\"css/theme/black.css\" id=\"theme\">\\nIf you want to add a theme of your own see the instructions here: /css/theme/README.md.\\nSpeaker Notes\\nreveal.js comes with a speaker notes plugin which can be used to present per-slide notes in a separate browser window. The notes window also gives you a preview of the next upcoming slide so it may be helpful even if you haven\\'t written any notes. Press the \\'s\\' key on your keyboard to open the notes window.\\nNotes are defined by appending an <aside> element to a slide as seen below. You can add the data-markdown attribute to the aside element if you prefer writing notes using Markdown.\\nAlternatively you can add your notes in a data-notes attribute on the slide. Like <section data-notes=\"Something important\"></section>.\\nWhen used locally, this feature requires that reveal.js runs from a local web server.\\n<section>\\n\\t<h2>Some Slide</h2>\\n\\n\\t<aside class=\"notes\">\\n\\t\\tOh hey, these are some notes. They\\'ll be hidden in your presentation, but you can see them if you open the speaker notes window (hit \\'s\\' on your keyboard).\\n\\t</aside>\\n</section>\\nNotes are only visible to you in the speaker view. If you wish to share your notes with the audience initialize reveal.js with the showNotes config value set to true.\\nIf you\\'re using the external Markdown plugin, you can add notes with the help of a special delimiter:\\n<section data-markdown=\"example.md\" data-separator=\"^\\\\n\\\\n\\\\n\" data-separator-vertical=\"^\\\\n\\\\n\" data-separator-notes=\"^Note:\"></section>\\n\\n# Title\\n## Sub-title\\n\\nHere is some content...\\n\\nNote:\\nThis will only display in the notes window.\\nServer Side Speaker Notes\\nIn some cases it can be desirable to run notes on a separate device from the one you\\'re presenting on. The Node.js-based notes plugin lets you do this using the same note definitions as its client side counterpart. Include the required scripts by adding the following dependencies:\\nReveal.initialize({\\n\\t...\\n\\n\\tdependencies: [\\n\\t\\t{ src: \\'socket.io/socket.io.js\\', async: true },\\n\\t\\t{ src: \\'plugin/notes-server/client.js\\', async: true }\\n\\t]\\n});\\nThen:\\n\\nInstall Node.js\\nRun npm install\\nRun node plugin/notes-server\\n\\nMultiplexing\\nThe multiplex plugin allows your audience to view the slides of the presentation you are controlling on their own phone, tablet or laptop. As the master presentation navigates the slides, all client presentations will update in real time. See a demo at http://revealjs-51546.onmodulus.net/.\\nThe multiplex plugin needs the following 3 things to operate:\\n\\nMaster presentation that has control\\nClient presentations that follow the master\\nSocket.io server to broadcast events from the master to the clients\\n\\nMore details:\\nMaster presentation\\nServed from a static file server accessible (preferably) only to the presenter. This need only be on your (the presenter\\'s) computer. (It\\'s safer to run the master presentation from your own computer, so if the venue\\'s Internet goes down it doesn\\'t stop the show.) An example would be to execute the following commands in the directory of your master presentation:\\n\\nnpm install node-static\\nstatic\\n\\nIf you want to use the speaker notes plugin with your master presentation then make sure you have the speaker notes plugin configured correctly along with the configuration shown below, then execute node plugin/notes-server in the directory of your master presentation. The configuration below will cause it to connect to the socket.io server as a master, as well as launch your speaker-notes/static-file server.\\nYou can then access your master presentation at http://localhost:1947\\nExample configuration:\\nReveal.initialize({\\n\\t// other options...\\n\\n\\tmultiplex: {\\n\\t\\t// Example values. To generate your own, see the socket.io server instructions.\\n\\t\\tsecret: \\'13652805320794272084\\', // Obtained from the socket.io server. Gives this (the master) control of the presentation\\n\\t\\tid: \\'1ea875674b17ca76\\', // Obtained from socket.io server\\n\\t\\turl: \\'revealjs-51546.onmodulus.net:80\\' // Location of socket.io server\\n\\t},\\n\\n\\t// Don\\'t forget to add the dependencies\\n\\tdependencies: [\\n\\t\\t{ src: \\'//cdn.socket.io/socket.io-1.3.5.js\\', async: true },\\n\\t\\t{ src: \\'plugin/multiplex/master.js\\', async: true },\\n\\n\\t\\t// and if you want speaker notes\\n\\t\\t{ src: \\'plugin/notes-server/client.js\\', async: true }\\n\\n\\t\\t// other dependencies...\\n\\t]\\n});\\nClient presentation\\nServed from a publicly accessible static file server. Examples include: GitHub Pages, Amazon S3, Dreamhost, Akamai, etc. The more reliable, the better. Your audience can then access the client presentation via http://example.com/path/to/presentation/client/index.html, with the configuration below causing them to connect to the socket.io server as clients.\\nExample configuration:\\nReveal.initialize({\\n\\t// other options...\\n\\n\\tmultiplex: {\\n\\t\\t// Example values. To generate your own, see the socket.io server instructions.\\n\\t\\tsecret: null, // null so the clients do not have control of the master presentation\\n\\t\\tid: \\'1ea875674b17ca76\\', // id, obtained from socket.io server\\n\\t\\turl: \\'revealjs-51546.onmodulus.net:80\\' // Location of socket.io server\\n\\t},\\n\\n\\t// Don\\'t forget to add the dependencies\\n\\tdependencies: [\\n\\t\\t{ src: \\'//cdn.socket.io/socket.io-1.3.5.js\\', async: true },\\n\\t\\t{ src: \\'plugin/multiplex/client.js\\', async: true }\\n\\n\\t\\t// other dependencies...\\n\\t]\\n});\\nSocket.io server\\nServer that receives the slideChanged events from the master presentation and broadcasts them out to the connected client presentations. This needs to be publicly accessible. You can run your own socket.io server with the commands:\\n\\nnpm install\\nnode plugin/multiplex\\n\\nOr you use the socket.io server at http://revealjs-51546.onmodulus.net/.\\nYou\\'ll need to generate a unique secret and token pair for your master and client presentations. To do so, visit http://example.com/token, where http://example.com is the location of your socket.io server. Or if you\\'re going to use the socket.io server at http://revealjs-51546.onmodulus.net/, visit http://revealjs-51546.onmodulus.net/token.\\nYou are very welcome to point your presentations at the Socket.io server running at http://revealjs-51546.onmodulus.net/, but availability and stability are not guaranteed. For anything mission critical I recommend you run your own server. It is simple to deploy to nodejitsu, heroku, your own environment, etc.\\nsocket.io server as file static server\\nThe socket.io server can play the role of static file server for your client presentation, as in the example at http://revealjs-51546.onmodulus.net/. (Open http://revealjs-51546.onmodulus.net/ in two browsers. Navigate through the slides on one, and the other will update to match.)\\nExample configuration:\\nReveal.initialize({\\n\\t// other options...\\n\\n\\tmultiplex: {\\n\\t\\t// Example values. To generate your own, see the socket.io server instructions.\\n\\t\\tsecret: null, // null so the clients do not have control of the master presentation\\n\\t\\tid: \\'1ea875674b17ca76\\', // id, obtained from socket.io server\\n\\t\\turl: \\'example.com:80\\' // Location of your socket.io server\\n\\t},\\n\\n\\t// Don\\'t forget to add the dependencies\\n\\tdependencies: [\\n\\t\\t{ src: \\'//cdn.socket.io/socket.io-1.3.5.js\\', async: true },\\n\\t\\t{ src: \\'plugin/multiplex/client.js\\', async: true }\\n\\n\\t\\t// other dependencies...\\n\\t]\\nIt can also play the role of static file server for your master presentation and client presentations at the same time (as long as you don\\'t want to use speaker notes). (Open http://revealjs-51546.onmodulus.net/ in two browsers. Navigate through the slides on one, and the other will update to match. Navigate through the slides on the second, and the first will update to match.) This is probably not desirable, because you don\\'t want your audience to mess with your slides while you\\'re presenting. ;)\\nExample configuration:\\nReveal.initialize({\\n\\t// other options...\\n\\n\\tmultiplex: {\\n\\t\\t// Example values. To generate your own, see the socket.io server instructions.\\n\\t\\tsecret: \\'13652805320794272084\\', // Obtained from the socket.io server. Gives this (the master) control of the presentation\\n\\t\\tid: \\'1ea875674b17ca76\\', // Obtained from socket.io server\\n\\t\\turl: \\'example.com:80\\' // Location of your socket.io server\\n\\t},\\n\\n\\t// Don\\'t forget to add the dependencies\\n\\tdependencies: [\\n\\t\\t{ src: \\'//cdn.socket.io/socket.io-1.3.5.js\\', async: true },\\n\\t\\t{ src: \\'plugin/multiplex/master.js\\', async: true },\\n\\t\\t{ src: \\'plugin/multiplex/client.js\\', async: true }\\n\\n\\t\\t// other dependencies...\\n\\t]\\n});\\nMathJax\\nIf you want to display math equations in your presentation you can easily do so by including this plugin. The plugin is a very thin wrapper around the MathJax library. To use it you\\'ll need to include it as a reveal.js dependency, find our more about dependencies here.\\nThe plugin defaults to using LaTeX but that can be adjusted through the math configuration object. Note that MathJax is loaded from a remote server. If you want to use it offline you\\'ll need to download a copy of the library and adjust the mathjax configuration value.\\nBelow is an example of how the plugin can be configured. If you don\\'t intend to change these values you do not need to include the math config object at all.\\nReveal.initialize({\\n\\n\\t// other options ...\\n\\n\\tmath: {\\n\\t\\tmathjax: \\'https://cdn.mathjax.org/mathjax/latest/MathJax.js\\',\\n\\t\\tconfig: \\'TeX-AMS_HTML-full\\'  // See http://docs.mathjax.org/en/latest/config-files.html\\n\\t},\\n\\t\\n\\tdependencies: [\\n\\t\\t{ src: \\'plugin/math/math.js\\', async: true }\\n\\t]\\n\\n});\\nRead MathJax\\'s documentation if you need HTTPS delivery or serving of specific versions for stability.\\nInstallation\\nThe basic setup is for authoring presentations only. The full setup gives you access to all reveal.js features and plugins such as speaker notes as well as the development tasks needed to make changes to the source.\\nBasic setup\\nThe core of reveal.js is very easy to install. You\\'ll simply need to download a copy of this repository and open the index.html file directly in your browser.\\n\\n\\nDownload the latest version of reveal.js from https://github.com/hakimel/reveal.js/releases\\n\\n\\nUnzip and replace the example contents in index.html with your own\\n\\n\\nOpen index.html in a browser to view it\\n\\n\\nFull setup\\nSome reveal.js features, like external Markdown and speaker notes, require that presentations run from a local web server. The following instructions will set up such a server as well as all of the development tasks needed to make edits to the reveal.js source code.\\n\\n\\nInstall Node.js\\n\\n\\nInstall Grunt\\n\\n\\nClone the reveal.js repository\\n$ git clone https://github.com/hakimel/reveal.js.git\\n\\n\\nNavigate to the reveal.js folder\\n$ cd reveal.js\\n\\n\\nInstall dependencies\\n$ npm install\\n\\n\\nServe the presentation and monitor source files for changes\\n$ grunt serve\\n\\n\\nOpen http://localhost:8000 to view your presentation\\nYou can change the port by using grunt serve --port 8001.\\n\\n\\nFolder Structure\\n\\ncss/ Core styles without which the project does not function\\njs/ Like above but for JavaScript\\nplugin/ Components that have been developed as extensions to reveal.js\\nlib/ All other third party assets (JavaScript, CSS, fonts)\\n\\nLicense\\nMIT licensed\\nCopyright (C) 2015 Hakim El Hattab, http://hakim.se\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nThis is intended to be a collection of tools for use with AWS all packaged together in one tidy container.\\nSo far it just has one script that refreshes and RDS database (removes and restores from a snapshot).\\nThis will remove whatever database you specify with TARGET_DB. Use caution!\\nUse it like this:\\ndocker run \\\\\\n\\t--env=AWS_ACCESS_KEY_ID \\\\\\n\\t--env=AWS_SECRET_ACCESS_KEY \\\\\\n\\t--env=SOURCE_DB=my-prod-db \\\\\\n\\t--env=TARGET_DB=my-test-db \\\\\\n\\t--env=DB_SUBNET_GROUP=my-subnet-group \\\\\\n\\t--env=SECURITY_GROUP=sg-123456 \\\\\\n\\t--env=TARGET_PASS=mynewpassword \\\\\\n\\t--rm --entrypoint=python texastribune/aws-tools /app/refresh-db.py\\n\\nIt assumes an AWS region of 'us-east-1' but you can override that by setting AWS_REGION\\nThe DB_SUBNET_GROUP is optional and only applies to instances residing in a VPC. If not provided\\nthe new database will not be in a VPC.\\nThe security group designation for VPC instances is an ID (e.g 'sg-123456'), for non-VPC it's a name (e.g. 'test-db-sg')\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nDebt Project\\nDatabases\\n{\\n  \\'FY 2013 Texas City Debt Outstanding\\': \\'13CITYTRLP.xls\\',\\n  \\'FY 2013 Texas County Debt Outstanding\\': \\'13CNYTRLP.xls\\',\\n  \\'FY 2013 Texas Independent School District (ISD)Debt Outstanding\\':\\'13ISDvaMOrvLP.xls\\'\\n}\\nCity and County databases\\nWe\\'re only interested in the first sheet: \"Tax-Supported Debt\"\\nFor each city/county, we want the users to see the info in columns F, G, H, K and N.\\nF - Debt Outstanding\\nG - Interest Owed on that debt\\nH - Total Debt Service (F+G)\\nK - Population\\nN - Debt Outstanding Per Capita\\nSchool district debt\\nWe\\'re interested in TWO sheets: \"13VotedDebt\" and \"13M&O Debt.\" (There are about 1,000 ISDs. Most have Voted Debt. About 180 also have M&O Debt, so for most ISDs, the M&O line will read \"0\")\\nFor each sheet, we\\'re interested in F, G and H (same as with City and County Debt) and N. which is \"Full Year ADA2012\" (student enrollment).\\nWe want to show \"Debt Outstanding Per Student\" but the state data doesn\\'t do the math for us in the way I want so we\\'ll need to plug it in ourselves. For each district, we want the following: (Voted Debt Outstanding + M&O Debt Outstanding)/ADA2012.\\nNote\\nFinally, I want us to do something similar to the \"How MY CITY Compares\" box that the Comptroller uses on her Local Debt website. I\\'m fine copying their approach or doing something different. Just want the user to have a way to see how their city/county/ISD compares to others.\\nLinks\\n\\nhttp://www.brb.state.tx.us/lgs/lgspubs2013.aspx\\nhttp://www.brb.state.tx.us/pub/lgs/fy2013/13CITYTRLP.xls\\nhttp://www.brb.state.tx.us/pub/lgs/fy2013/13CNYTRLP.xls\\nhttp://www.brb.state.tx.us/pub/lgs/fy2013/13ISDvaMOrvLP.xls\\n\\nhttp://www.texastransparency.org/Special_Features/Debt_at_a_Glance/counties.php\\nSetup\\nYou are going to need PostgreSQL. If your are using OSX, we recommend Postgres.app. You will need to have a database called local_debt:\\nCREATE DATABASE local_debt;\\nThen you need to run:\\npip install -r requirements.txt\\nmake resetdb\\n\\nYou will also need to setup the webapp. Just run:\\ncd webapp\\nnpm install -g grunt-cli bower\\nnpm install && bower install\\ngrunt serve\\n\\nFeel free to start hacking.\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nsalesforce-tools\\ntools for wrangling data for Salesforce\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nRoad From Rita\\nA project coinciding with the 10-year anniversary of Hurricane Rita, the state's forgotten — but no less devastating — storm.\\n\\n\\n\",\n",
       "  'language': 'CSS'},\n",
       " {'readme': 'No readme file.', 'language': 'Makefile'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\narmstrong.hatband\\nProvides an enhanced Django admin interface and utilities for use in\\nArmstrong.\\nNotably it brings VisualSearch.js for fast content lookup, CKEditor\\nas a fully featured rich-text editor and other features like drag 'n drop\\nJavaScript for easy programming of orderable content (like lists).\\n\\nUsage\\nTODO\\n\\nInstallation & Configuration\\nSupports Django 1.3, 1.4, 1.5, 1.6 on Python 2.6 and 2.7.\\n\\npip install armstrong.hatband\\n\\nAdd armstrong.hatband to your INSTALLED_APPS\\n\\nIn urls.py replace the normal admin with Hatband,\\nfrom armstrong import hatband as admin.\\nThe rest of the URL configuration stays identical to what is expected for\\nthe traditional Django admin.\\n\\n\\n\\nContributing\\nDevelopment occurs on Github. Participation is welcome!\\n\\nFound a bug? File it on Github Issues. Include as much detail as you\\ncan and make sure to list the specific component since we use a centralized,\\nproject-wide issue tracker.\\nTesting? pip install tox and run tox\\nHave code to submit? Fork the repo, consolidate your changes on a topic\\nbranch and create a pull request. The armstrong.dev package provides\\ntools for testing, coverage and South migration as well as making it very\\neasy to run a full Django environment with this component's settings.\\nQuestions, need help, discussion? Use our Google Group mailing list.\\n\\n\\nState of Project\\nArmstrong is an open-source news platform that is freely available to any\\norganization. It is the result of a collaboration between the Texas Tribune\\nand Bay Citizen and a grant from the John S. and James L. Knight\\nFoundation. Armstrong is available as a complete bundle and as individual,\\nstand-alone components.\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nPymViewer\\nA single-page site for testing Pym.js embeds. Built using the Texas Tribune News Apps Graphic Kit.\\nSetup\\nFirst, install the dependencies.\\nnpm install\\nUsage\\nDuring development, you can serve the page locally.\\nnpm run serve\\nTo build the production version, run the build command. It can then be found in the dist folder.\\nnpm run build\\nAll of the content placed in dist will be gzipped, and will only work if push it somewhere that can serve gzipped files. If you would rather this not happen, you can go into the gulpfile.babel.js file and comment out all the gzip tools. (This will be better soon!) Then run npm run build again.\\ngulp.task('styles', () => {\\n  return gulp.src('app/**/*.scss')\\n    .pipe($.sass({\\n      precision: 10,\\n      onError: console.error.bind(console, 'Sass error: ')\\n    }))\\n    .pipe($.autoprefixer(['last 2 versions', 'IE 9', 'IE 8']))\\n    .pipe(gulp.dest('.tmp'))\\n    .pipe(stream({match: '**/*.css'}))\\n    .pipe($.if('*.css', $.csso()))\\n    // .pipe($.gzip({append: false}))\\n    .pipe(gulp.dest('dist'))\\n    .pipe($.size({title: 'styles'}));\\n});\\nConnect and deploy to S3\\nThe default method of publication is via Amazon S3 and awscli.\\nTo use the commands to deploy your project to Amazon S3, you'll need to add a [profile newsapps] to ~/.aws/config. It should look something like this:\\n[profile newsapps]\\naws_access_key_id=YOUR_UNIQUE_ID\\naws_secret_access_key=YOUR_SECRET_ACCESS_KEY\\n\\nThe project will deploy to the bucket and slug provided in package.json. Change those from our locations to push to your own bucket!\\nRun this command to deploy.\\nnpm run deploy\\n\\nYou don't have to use this — the files npm run build compiles in dist can be moved anywhere that serves files.\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nPerry Palooza\\nA multi-part feature on the legacy Gov. Rick Perry\\nNode modules that you'll need to install to process speadsheet:\\n+googleapis\\n+request\\n+untildify\\n+xlsx\\n+marked\\n\\n\\n\",\n",
       "  'language': 'HTML'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\npixelcite\\n\\nWhat is this?\\nWhat\\'s in here?\\nBootstrap the project\\nHide project secrets\\nSave media assets\\nRun the project\\nCOPY editing\\nCompile static assets\\nTest the rendered app\\nDeploy to S3 and EC2\\nInstall web services\\nRun a remote fab command\\nReport analytics\\n\\nWhat is this?\\nQuotations for Twitter.\\nThis project is based on the NPR app-template.\\nWhat\\'s in here?\\nThe project contains the following folders and important files:\\n\\nconfs -- Server configuration files for nginx and uwsgi. Edit the templates then fab <ENV> servers.render_confs, don\\'t edit anything in confs/rendered directly.\\ndata -- Data files, such as those used to generate HTML.\\nfabfile -- Fabric commands for automating setup, deployment, data processing, etc.\\netc -- Miscellaneous scripts and metadata for project bootstrapping.\\njst -- Javascript (Underscore.js) templates.\\nless -- LESS files, will be compiled to CSS and concatenated for deployment.\\ntemplates -- HTML (Jinja2) templates, to be compiled locally.\\nwww -- Static and compiled assets to be deployed. (a.k.a. \"the output\")\\nwww/assets -- A symlink to an S3 bucket containing binary assets (images, audio).\\napp.py -- A Flask app for rendering the project locally.\\napp_config.py -- Global project configuration for scripts, deployment, etc.\\ncopytext.py -- Code supporting the Editing workflow\\ncrontab -- Cron jobs to be installed as part of the project.\\npublic_app.py -- A Flask app for running server-side code.\\nrender_utils.py -- Code supporting template rendering.\\nrequirements.txt -- Python requirements.\\nstatic.py -- Static Flask views used in both app.py and public_app.py.\\n\\nBootstrap the project\\nNode.js is required for the static asset pipeline. If you don\\'t already have it, get it like this:\\nbrew install node\\ncurl https://npmjs.org/install.sh | sh\\n\\nThen bootstrap the project:\\ncd pixelcite\\nmkvirtualenv --no-site-packages pixelcite\\npip install -r requirements.txt\\nnpm install\\nfab update\\n\\nProblems installing requirements? You may need to run the pip command as ARCHFLAGS=-Wno-error=unused-command-line-argument-hard-error-in-future pip install -r requirements.txt to work around an issue with OSX.\\nHide project secrets\\nProject secrets should never be stored in app_config.py or anywhere else in the repository. They will be leaked to the client if you do. Instead, always store passwords, keys, etc. in environment variables and document that they are needed here in the README.\\nSave media assets\\nLarge media assets (images, videos, audio) are synced with an Amazon S3 bucket specified in app_config.ASSETS_S3_BUCKET in a folder with the name of the project. (This bucket should not be the same as any of your app_config.PRODUCTION_S3_BUCKETS or app_config.STAGING_S3_BUCKETS.) This allows everyone who works on the project to access these assets without storing them in the repo, giving us faster clone times and the ability to open source our work.\\nSyncing these assets requires running a couple different commands at the right times. When you create new assets or make changes to current assets that need to get uploaded to the server, run fab assets.sync. This will do a few things:\\n\\nIf there is an asset on S3 that does not exist on your local filesystem it will be downloaded.\\nIf there is an asset on that exists on your local filesystem but not on S3, you will be prompted to either upload (type \"u\") OR delete (type \"d\") your local copy.\\nYou can also upload all local files (type \"la\") or delete all local files (type \"da\"). Type \"c\" to cancel if you aren\\'t sure what to do.\\nIf both you and the server have an asset and they are the same, it will be skipped.\\nIf both you and the server have an asset and they are different, you will be prompted to take either the remote version (type \"r\") or the local version (type \"l\").\\nYou can also take all remote versions (type \"ra\") or all local versions (type \"la\"). Type \"c\" to cancel if you aren\\'t sure what to do.\\n\\nUnfortunantely, there is no automatic way to know when a file has been intentionally deleted from the server or your local directory. When you want to simultaneously remove a file from the server and your local environment (i.e. it is not needed in the project any longer), run fab assets.rm:\"www/assets/file_name_here.jpg\"\\nRun the project\\nA flask app is used to run the project locally. It will automatically recompile templates and assets on demand.\\nworkon $PROJECT_SLUG\\npython public_app.py\\n\\nVisit localhost:8000 in your browser.\\nCOPY editing\\nThis app uses a Google Spreadsheet for a simple key/value store that provides an editing workflow.\\nView the sample copy spreadsheet.\\nThis document is specified in app_config with the variable COPY_GOOGLE_DOC_KEY. To use your own spreadsheet, change this value to reflect your document\\'s key (found in the Google Docs URL after &key=).\\nA few things to note:\\n\\nIf there is a column called key, there is expected to be a column called value and rows will be accessed in templates as key/value pairs\\nRows may also be accessed in templates by row index using iterators (see below)\\nYou may have any number of worksheets\\nThis document must be \"published to the web\" using Google Docs\\' interface\\n\\nThe app template is outfitted with a few fab utility functions that make pulling changes and updating your local data easy.\\nTo update the latest document, simply run:\\nfab copytext.update \\n\\nNote: copytext.update runs automatically whenever fab render is called.\\nAt the template level, Jinja maintains a COPY object that you can use to access your values in the templates. Using our example sheet, to use the byline key in templates/index.html:\\n{{ COPY.attribution.byline }}\\n\\nMore generally, you can access anything defined in your Google Doc like so:\\n{{ COPY.sheet_name.key_name }}\\n\\nYou may also access rows using iterators. In this case, the column headers of the spreadsheet become keys and the row cells values. For example:\\n{% for row in COPY.sheet_name %}\\n{{ row.column_one_header }}\\n{{ row.column_two_header }}\\n{% endfor %}\\n\\nWhen naming keys in the COPY document, pleaseattempt to group them by common prefixes and order them by appearance on the page. For instance:\\ntitle\\nbyline\\nabout_header\\nabout_body\\nabout_url\\ndownload_label\\ndownload_url\\n\\nCompile static assets\\nCompile LESS to CSS, compile javascript templates to Javascript and minify all assets:\\nworkon pixelcite\\nfab render\\n\\n(This is done automatically whenever you deploy to S3.)\\nTest the rendered app\\nIf you want to test the app once you\\'ve rendered it out, just use the Python webserver:\\ncd www\\npython -m SimpleHTTPServer\\n\\nDeploy to S3 and EC2\\nfab staging master deploy\\n\\nInstall web services\\nWeb services are configured in the confs/ folder.\\nTo check that these files are being properly rendered, you can render them locally and see the results in the confs/rendered/ directory.\\nfab servers.render_confs\\n\\nYou can also deploy only configuration files by running (normally this is invoked by deploy):\\nfab servers.deploy_confs\\n\\nRun a  remote fab command\\nSometimes it makes sense to run a fabric command on the server, for instance, when you need to render using a production database. You can do this with the fabcast fabric command. For example:\\nfab staging master servers.fabcast:deploy\\n\\nIf any of the commands you run themselves require executing on the server, the server will SSH into itself to run them.\\nAnalytics\\nThe Google Analytics events tracked in this application are:\\n\\n\\n\\nCategory\\nAction\\nLabel\\nValue\\nCustom 1\\nCustom 2\\n\\n\\n\\n\\npixelcite\\ntweet\\n\\n\\n\\n\\n\\n\\npixelcite\\nsave-image\\n\\n\\n\\n\\n\\n\\npixelcite\\nlogin\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nState of Mind\\n##Community Concerns Come to the Capitol\\n\\n\\n',\n",
       "  'language': 'HTML'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\ntxlege84\\nThe lege app to end all lege apps.\\nRequirements\\n\\nDjango 1.7\\nvirtualenv/virtualenvwrapper\\nNode/npm\\n\\nGulp\\n\\n\\nBower\\nSass + Ruby\\n\\nGetting started\\nPlease note – this guide assumes you are using OS X. If you aren't, you hopefully know the equivalent commands to make these things happen. If you don't, find someone to help you!\\nFirst, clone the project.\\ngit clone https://github.com/texastribune/txlege84.git\\nThen, create the virtual environment for your project.\\nmkvirtualenv txlege84-dev\\nDon't forget to activate it!\\nworkon txlege84-dev\\nNext, install the Python development requirements.\\npip install -r requirements/local.txt\\nOnce that finishes, run the initial migration, then try to run the project!\\npython txlege84/manage.py migrate\\npython txlege84/manage.py runserver\\nYou should be able to check the site out at http://localhost:8000, but it's gonna look funky. Time to set up styling!\\nFirst, we install all our node and bower dependencies.\\nnpm install && bower install\\nThis project uses gulp to run scss compiling and other various tasks.\\nNow here's where the magic happens. Gulp uses BrowserSync to handle live reloading of the page during development. So when you're working on the front end, you'll need two terminal windows or tabs.\\nIn the first, you'll run the Django runserver command.\\npython txlege84/manage.py runserver\\nIn the second, you'll run the gulp serve command.\\ngulp serve\\nThis routes the Django's development server through BrowserSync's development server, so whenever anything changes with the templates or styles the page reloads automatically.\\nWith both of those running, visit http://localhost:3000 to view the site.\\nNow get to work!\\nBootstrapping the data for development\\nYou'll need a Sunlight Foundation API key to run these steps. Once you have it, you'll need to add it to your environment. There are a number of ways to do that, but the easiest way is:\\nexport SUNLIGHT_API_KEY=<api-key-characters>\\nNext, you need to prep your database.\\npython txlege84/manage.py migrate\\nThen, run the make command to load the data.\\nmake prep_for_development\\n\\n\\n\",\n",
       "  'language': 'HTML'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nnewsapps-deploy\\nA collection of awscli-powered deploy scripts that can be easily reused.\\nRequirements\\n\\nnode.js >= 0.10 or io.js\\n\\nnpm\\n\\n\\nPython 2.6.5+\\n\\nawscli\\n\\n\\n\\nInstallation\\nnpm install --save-dev newsapps-deploy\\nUsage\\nnewsapps-deploy depends on a file to pass in configuration. By default, it looks for config.json — but you are welcome to point it elsewhere via --config.\\nUsage: newsapps-deploy [COMMAND] [--config=PATH] [--dry-run] [--gzip] [--\\nproduction]\\n\\nCommands:\\n  deploy  Deploy the project to S3\\n  push    Push assets to S3\\n  pull    Pull assets from S3\\n\\nOptions:\\n  -c, --config      The path to the config file         [default: \"config.json\"]\\n  -g, --gzip        Pushes code to your specified bucket with predefined HTTP\\n                    cache headers                                      [boolean]\\n  -p, --production  Push code to the production bucket (defaults to development)\\n                                                                       [boolean]\\n  -d, --dry-run     Print the commands instead of running them (good for\\n                    checking things out)                               [boolean]\\n  -h, --help        Show help                                          [boolean]\\nconfig.json should contain an object with a deployment key with the following fields:\\n{\\n  \"deployment\": {\\n    \"dev_s3_bucket\": \"notarealbucket.texastribune.org\",\\n    \"prod_s3_bucket\": \"graphics.texastribune.org\",\\n    \"path\": \"graphics\",\\n    \"slug\": \"corgis-vs-red-pandas\",\\n    \"aws_profile\": \"newsapps\",\\n    \"dist_folder\": \"dist\",\\n    \"assets_folder\": \"app/assets\"\\n  }\\n}\\nField definitions\\ndev_s3_bucket\\nPush test deploys to this bucket\\nprod_s3_bucket\\nPush production deploys to this bucket\\npath\\nThe path to the folder where code should be pushed.\\nslug\\nThe slug of the folder (located in path) where code should be pushed.\\naws_profile\\nThe AWS credential to use for deployment. If empty, nothing will be passed to awscli via --profile.\\ndist_folder\\nThe folder that should be pushed up to S3. This should be your content ready for deploy.\\nassets_folder\\nThe folder where raw assets are located — typically app/assets.\\nDefault cache headers\\nThe assumpion is that the .css and .js are being revisioned. If --gzip is passed, the following files will be assumed to be gzipped:\\n.html\\n.css\\n.js\\n.json\\n\\n.css and .js are set to expire in 1 year. Images — .jpg, .png, .svg, and .gif — are set to expire in 1 day. .json files will expire in 1 hour. .html have no set expiration.\\nWait – this depends on a Python library?\\nThere is a library for AWS written in node.js called aws-sdk. However, multiple parts of our workflow already generously depend on awscli — to the point it can be assumed that it\\'s already installed and configured.\\nAlso — aws-sdk has no command line interface built in. Any tasks we need to accomplish would have to be written ourselves. Why reinvent the wheel? ¯_(ツ)_/¯\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nNews Apps Quiz Kit\\nThe News Apps Quiz Kit is a boilerplate for embeddable quizes. It was built for the Texas Tribune\\'s News Apps team, but could easily be altered to cater to other organization\\'s/individual\\'s needs. It is powered by the News Apps Graphic Kit.\\nSample Spreadsheet Template\\n\\n\\n\\nquestion\\ntype\\nanswer\\na\\nb\\nc\\nd\\nincorrect_response_md\\ncorrect_response_md\\n\\n\\n\\n\\nThe Tribune has an amazing news apps team.\\nTF\\nTRUE\\n\\n\\n\\n\\nGo home, you\\'re drunk.\\nThank you!\\n\\n\\nWhich news apps member has the coolest cat poster?\\nMULT\\nA\\nBecca\\nJolie\\nAnnie\\nRyan\\n:{ #sadmustache\\nGood job.\\n\\n\\nWho plays in a fake band with \"the ladies\"?\\nMULT\\nd\\nBecca\\nJolie\\nAnnie\\nRyan\\n:{ #sadmustache\\nGood job.\\n\\n\\nWho is most likely to be eaten by a dinosaur?\\nMULT\\nB\\nBecca\\nJolie\\nAnnie\\nRyan\\n:{ #sadmustache\\nGood job.\\n\\n\\nWho tells secrets to their cat?\\nMULT\\nc\\nBecca\\nJolie\\nAnnie\\nRyan\\n:{ #sadmustache\\nGood job.\\n\\n\\n\\nEach row represents a question. Under type, indicate whether the question is true/false (TF) or multiple choice (MULT). For TF questions, just write \"true\" or \"false\" in the answer columns, and leave the multiple choice answer columns blank. For MULT questions, fill in each of the multiple choice answers, and then indicate which is correct in the answer column. (The template will accept capital or lowercase letters A/a, B/b, C/c or D/d.)\\nFlow Charts\\nFlow charts are a little bit different than a regular quiz. You\\'ll need to give each question an ID, and indicate which ID (i.e. question) to show next for each option. To create a flow chart, add the _flowchart.scss partial to your main.scss, flowchart.js to your index.html and {% include \"flowchart.html\" %}, which contains {% macro flowchart() %}, and extend the code in your index.html. Here\\'s a flow-chart spreadsheet template:\\n\\n\\n\\nid\\ntext_md\\na_option_md\\nb_option_md\\na_next\\nb_next\\nimg\\n\\n\\n\\n\\nstart\\nAre you a cat or dog person?\\nCat\\nDog\\ncat\\ndog\\n\\n\\n\\ncat\\nDo you tell secrets to your cat?\\nYes! She\\'s the best secret-keeper.\\nNo, I have real friends.\\nannie\\nbecca\\n\\n\\n\\ndog\\nWould you rather have a chickenosaurus or a corgi?\\nChickenosaurus\\nCorgi\\njolie\\nryan\\n\\n\\n\\nannie\\nYou\\'re Annie Daniel!\\n\\n\\n\\n\\nannie.jpg\\n\\n\\nbecca\\nYou\\'re Becca Aaronson!\\n\\n\\n\\n\\nbecca.jpg\\n\\n\\njolie\\nYou\\'re Jolie McCullough!\\n\\n\\n\\n\\njolie.jpg\\n\\n\\nryan\\nYou\\'re Ryan Murphy!\\n\\n\\n\\n\\nryan.jpg\\n\\n\\n\\nFeatures\\n\\nLive reloading and viewing powered by BrowserSync\\nCompiling of Sass/SCSS with Ruby Sass\\nCSS prefixing with autoprefixer\\nCSS sourcemaps with gulp-sourcemaps\\nCSS compression with csso\\nJavaScript linting with jshint\\nJavaScript compression with uglifyjs\\nTemplate compiling with nunjucks\\nImage compression with gulp-imagemin\\nAsset revisioning with gulp-rev and gulp-rev-replace\\npym.js included by default for easy embedding in hostile CMS environments\\n\\nQuickstart\\n\\nDownload the project folder or clone the repo.\\nnpm install\\nAuthorize your computer if this is your first time to ever use the kit: npm run spreadsheet/authorize\\nUse the spreadsheet templates above to create your quiz or flowchart\\nAdd your Google sheet\\'s ID to the config.json, and override any sheets that need to be processed differently. (keyvalue or objectlist)\\nPull the spreadsheet data into your project by running npm run spreadsheet/fetch. Check your data.json file to make sure it\\'s there.\\nAdd the quiz or flowchart macro to your index.html file, and the related script file. For example:\\n\\n{% include \"quiz.html\" %}\\n{{ quiz() }}\\n\\n<!-- other code -->\\n\\n<!-- build:js scripts/main.min.js -->\\n<script src=\"/node_modules/jquery/dist/jquery.min.js\"></script>\\n<script src=\"/node_modules/pym.js/dist/pym.js\"></script>\\n<script src=\"scripts/quiz.js\"></script>\\n<!-- endbuild -->\\n\\n\\n\\nPreview the project locally by running gulp serve\\n\\nConnect to S3\\nTo use the commands to deploy your project to Amazon S3, you\\'ll need to add a [profile newsapps] to ~/.aws/config. It should look something like this:\\n[profile newsapps]\\naws_access_key_id=YOUR_UNIQUE_ID\\naws_secret_access_key=YOUR_SECRET_ACCESS_KEY\\n\\nThen you can run these commands to build and deploy:\\ngulp\\nnpm run deploy\\n\\nThe package will deploy to graphics.texastribune.org/. You\\'ll need to add a slug to package.json to designate a specific S3 location.\\nAvailable Commands\\nnpm run spreadsheet/authorize\\nAllows your computer to interact with the scraper. Only needs to be done once! Any future uses of the graphic kit can skip this.\\nnpm run spreadsheet/fetch\\nPulls down the project\\'s spreadsheet and processes it into the data.json file.\\nnpm run spreadsheet/edit\\nOpens the project\\'s spreadsheet in your browser.\\nnpm run deploy\\nDeploys the project.\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\ntx_lobbying\\n\\nVery early alpha\\nAbout the data\\nThe two main sources of data are:\\n\\n\\nThe lists of registered lobbyists, by year:\\nhttp://www.ethics.state.tx.us/dfs/loblists.htm\\n\\n\\nAnd the coversheets for the lobbyist activies reports (LA):\\nhttp://www.ethics.state.tx.us/dfs/search_LOBBY.html\\n\\n\\nNames come from both sources of data, but only the coversheets have detailed\\ninformation about names.\\nThe information for lobbying interests come from the registration forms. This\\ninformation is all entered by hand and will be very hard to use without\\nscrubbing. There should be a way to de-duplicate lobbying interests, and a way\\nto regenerate a lobbyist's interests based on the raw registration data stored.\\nThe forms can, and are, amended often. When you do updates, it's a good idea\\nto go back a few years.\\nGetting up and running\\nSet your environment variables. One way is to have a .env file with:\\nDJANGO_SETTINGS_MODULE=example_project.settings\\nDATABASE_URL=postgres:///tx_lobbying\\nHAYSTACK_URL=http://1270.0.01:9200/\\n\\n# optional if you want to geocode\\n# https://geoservices.tamu.edu/Services/Geocode/WebService/\\nTAMU_API_KEY=5f4dcc3b5aa765d61d8327deb882cf99\\n\\nand then using autoenv or source .env to activate it.\\nInstall deps:\\npip install -r requirements.txt\\nnpm install\\n\\nGet your database up and running with (Postgres instructions):\\ncreatedb tx_lobbying  # if you have a simple Postgres setup\\nphd createdb  # if you have a complicated DATABASE_URL, pip install postdoc\\n\\n# inital migration\\nmake resetdb\\n\\nThen load data with:\\nmake scrape\\n\\nRunning tests\\nChange template1 to be postgis enabled:\\npsql template1\\n\\nCREATE EXTENSION postgis;\\nCREATE EXTENSION postgis_topology;\\n\\nmake test\\n\\nDevelopment\\npip install -r requirement-dev.txt\\n\\nThe project is configured to use sqlite by default, but if don't want to stab\\nyourself, set your DATABASE_URL to real database like Postgres.\\nStart the development server:\\npython example_project/manage.py runserver\\n\\nHow the data is modeled\\nDiagram:\\n\\nDetails:\\nModels that correspond directly to a piece of raw data are labelled with\\nraw.\\n\\nLobbyist: the main model, represents a lobbyist\\nExpenseDetailReport: Not that useful yet. raw\\nCoversheet: Expense coversheets. raw\\nLobbyistStats: A summary of expenses for a year\\nRegistrationReport: A lobbyist's registration for a year. raw\\nLobbyistAnnum: Represent's a year of a lobbyist's existence in the database\\nCompensation: How a lobbyist was compensated by their client. raw\\nInterest: A lobbyist's clients, aka an interested party\\nInterestStats: A summary of an interest's stats for a year\\nAddress: Any time we need to store an address\\n\\nThe Future\\nFeatures not handled in this version but perhaps the next:\\n\\nHandle corporate mergers/splits/renames\\nLink entities to campaign finance (state and federal)\\n\\nAdditional Data Sources\\nIn addition to the data from the TEC, these services are also used:\\n\\nName de-duplication handled by nomenklatura.\\nGeocoding handled by TAMU Geoservices\\n\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nDonation Builder\\nYou can also read the Donation Builder docs here.\\nAbout the Project\\nDonation Builder is a framework to help organizations showcase their work and seek new members and support for funding their projects. The primary audience for this project is nonprofit organizations with small teams who want a better and quick-to-set-up way to inform potential new members and supporters about their mission and work, as well as easily lead them through the donation process.\\nWhy We Built It this Way\\nWe chose Middleman for the project because we wanted a mostly static site. The Google Drive gem for Middleman provides the ability to easily update the content on the site without touching the code once it\\'s set up, for those elements that are dynamic.\\nFeatures\\n\\nFully responsive framework ready to be customized with an organization\\'s visual brand, including images, colors, and fonts\\nAbility to load information from Google Spreadsheets, so that non-technical members of your team can update membership levels and benefits, contact information, organization information, social share messages, content to highlight, and more\\nEasily integrated with your Google Analytics account\\nAbility to hook up to a variety of payment processors, including Dwolla, Givalike, and DonorPerfect, to accept the donations\\nAbility to deploy through Amazon Web Services or Heroku\\n\\nSupport\\n\\nChrome, Firefox, Safari, IE9+\\nUsers with JavaScript disabled\\nTested with Browserstack across a variety of devices\\n\\nGetting Started\\nTo get started, clone down the project repo.\\nIf you don\\'t already have Ruby installed, you need to install it. You need Ruby version 2.1.4, which is specified in the .ruby-version file. One way to manage your Ruby versions is to use rbenv.\\nYou need the Ruby gem bundler. If you need to install the bundler, run:\\ngem install bundler\\n\\nInstall the necessary gems from the Gemfile by running:\\nbundle install\\n\\nDevelopment\\nIf you\\'ve never worked with Middleman before, you can check out the docs here.\\nMiddleman is configured to live reload the project in the browser as changes are made to files. To start up the Middleman server, run:\\nbundle exec middleman\\n\\nTo build the site, run:\\nbundle exec middleman build\\n\\nWhen Middleman builds, it creates a static file for each file located in the source folder. The build process is configured in config.rb.\\nIntegrating with Google Drive\\nDonation Builder uses the middleman-google_drive gem to integrate with Google Drive. Configure your Google Docs authentication as detailed in the Setup section.\\nThe project is set up so that the site can be updated and customized through Google Spreadsheets. Use the same setup for your Google Spreadsheets to correctly pull in the info. You can view the spreadsheet setup here.\\nAnalytics\\nYou can hook up the project to your Google Analytics account to track how people are interacting with the site. In the Google Spreadsheet, you should have an Analytics tab. Look for the column labeled \\'google_analytics_id\\', and enter your Google Analytics ID here. A second column labeled \\'google_analytics_domain\\'  is where you should enter your domain.\\nSee this screenshot.\\nVisual Customization\\nBrand Colors\\nDonation Builder uses Sass to write its styles. In _settings.scss, there are some color variables. The project uses three theme colors, a main theme color, a secondary theme color, and a tertiary theme color. These colors are then applied to the buttons, navigation links, labels, and other UI elements throughout the project to quickly and easily use your organization\\'s colors.\\nTypography\\nYou can customize the site with the fonts your organization already uses. This can also be done in the _settings.scss file. Look for the variables $main-font and $secondary-font. The default main font is \\'Roboto\\' with a sans-serif fallback; the default secondary font is \\'Open Sans Condensed\\', which has another sans-serif font as a fallback.\\nImages\\nTo switch out most images, you can replace an image in the /source/images folder that corresponds to the image you\\'d like to change. These images are named in an intuitive manner so that it\\'s clear where each image appears in the project. The images and where they appear are described below in case the names are not clear. If you still aren\\'t sure, try switching out the image and see where it appears in the project.\\nLogo\\nYour organization\\'s logo appears in the masthead of each page on the site. The logo should be approximately 300px wide and 60px tall. In the source/images/ folder is an image named logo.png. Replace this image with your logo.\\nFavicon\\nYou may want to customize the favicon that appears on the browser tab. To do this, replace the image called favicon.ico with your favicon.\\nBanner Backgrounds\\nYou can customize the backgrounds used for the banners at the top of each page with your organization\\'s photos. To set a background for the landing page banner, replace the photo named landing-header-bg.jpg. To set a background for the premium membership page banner, replace the photo named premium-membership-banner.jpg.\\nOrganization Info\\nIn the AboutOrg tab of the Google Spreadsheet, you can set up the summary for your organization, a short tagline, a call-to-action, your organization\\'s name, and the url for your organization.\\nContact Information\\nContact information letting people know who to contact with questions can also be configured in the Google Spreadsheet, under the Contacts tab. You can set department, name, title, phone, and email for each contact.\\nMembership Levels\\nYou can set up your organization\\'s membership level names, donation amounts and benefits in the Google Spreadsheet. You do this on the MemberLevels tab. See this screenshot.\\nname\\nThis field is your membership level name. For example, \"Activist\" or \"Enthusiast\".\\ndonation_amount\\nThis is the donation amount that people should give to be at the membership level. Add the amounts without dollar signs. For amounts over 999, include the comma (for example, 1,000).\\nmost_popular\\nSet this to TRUE for your most popular membership level, and leave this field blank for all other levels. This activates a \"Most Popular\" flag when this level is displayed on the landing page and on the membership grid page.\\npremium_member\\nSet this to TRUE for any premium membership levels that your organization offers. This activates a \"Premium Member\" flag when this level is displayed on the landing page and on the membership grid page. It also tells the project to feature these premium membership levels on the premium membership page.\\nbenefits\\nYou can list the benefits for each membership level here. Between each benefit, include <br> to put in a line break. You may also want to include \"Plus all above benefits\" as one of the benefits in this field.\\nCustomizing Content\\nOn both the landing page and the premium membership page, there are four spots provided for you to showcase either images or videos to inform people what their donations will go toward supporting. You set them in the Showcase tab. See this screenshot. To showcase images, enter \\'images\\' in the \\'content_type\\' column, and to showcase videos, enter \\'videos\\' in the \\'content_type\\' column.\\nIf you selected images, upload the images of your choice and name them \"image-one.pg\" (This is the top featured image), \"image-two.png\", \"image-three.png\", and \"image-four.png\".\\nIf you selected videos, upload the videos of your choice and name them \"video-one.pg\" (This is the top featured video), \"video-two.png\", \"video-three.png\", and \"video-four.png\". You\\'ll also need to set the Youtube link for your videos in the Showcase tab under \"video_one\", \"video_two\", \"video_three\", and \"video_four\".\\nLabels for the images or videos can be set in the spreadsheet with the columns \"label_one\", \"label_two\", \"label_three\", and \"label_four\".\\nPage Information\\nYou can set your page titles, headers, and urls in the Pages tab of the Google Spreadsheet.\\nNavigation Links\\nYou can add more navigation links to the navbar as needed. These links are included in both the desktop nav and the mobile nav. For example, you might want to add a \"Quick Donation\" link or a link to more information about your organization. Additional nav links can be set up in the NavLinks tab. For each additional nav link, add the URL target and the link text to display. It\\'s recommended that you add only up to three additional nav links so you don\\'t overwhelm the page.\\nFooter Links\\nYou can set up the links that should appear in the footer in the FooterLinks tab. For each link you\\'d like to include in the footer, add the URL and the link text to display. Note that the project is configured to hide the footer by default if the device width is 985px or greater. You can change this in the _footer.scss file if you prefer a different behavior here.\\nSocial Sharing\\nYou can customize the default messages for when people choose to email, tweet or post to Facebook about donating to your organization. You configure this in Google Spreadsheets on the Social tab. See this screenshot. The Facebook fields set the Open Graph metadata for your site, to ensure Facebook scrapes and displays the information you want when people post.\\nTo set your Facebook share image, you also need to include the image you\\'d like to use there in the /source/images folder named facebook-share-image.png.\\nIntegrating with a Payment Processor\\nThe project is designed to be able to be integrated with the payment processor that works best for you. It\\'s set up to integrate most fluidly through linking to a form where people can then enter their information and submit their contribution. Some examples of payment processors with which it works well include Givalike, Dwolla and DonorPerfect.\\nYou can set up the links where people should be sent to enter their information and submit their contribution in the DonationInfo tab. See this screenshot. You can set different URLs here for generic contributions and for renewals.\\nPossible Customizations\\nComing soon!\\nDeploying\\nAmazon Web Services\\nTo deploy to AWS, the project uses the middleman-s3_sync gem to push and compile the site to Amazon S3 after building.\\nIn the config.rb file, look for \"activate :s3_sync do |config|\". Set config.bucket to your AWS bucket. Set config.region to your AWS bucket region. Set your AWS_ACCESS_KEY and your AWS_ACCESS_SECRET as environment variables.\\nBy default, in config.rb, config.after_build is set to false. When you\\'re ready to deploy, set this to true. Check that your AWS_ACCESS_KEY and AWS_ACCESS_SECRET environment variables are set, and then run:\\nbundle exec middleman build\\n\\nIn your terminal, you should see s3_sync applying any updates to the files in the project. You can also check the project S3 bucket to ensure that all files have been synced there. Change config.after_build back to its default of false after deploying.\\nHeroku\\nIf you\\'ve never used Heroku before, you\\'ll need to create a Heroku account.\\nFor Heroku, you\\'ll need Ruby 2.1.4.\\nThe first time you deploy, create your project for Heroku:\\nheroku create\\n\\nBuild the project locally and commit the updates:\\nbundle exec middleman build\\n\\nHeroku is set up in config.ru to use the build directory.\\nPush the project to Heroku:\\ngit push heroku master\\n\\nTo see your deployed project:\\nheroku open\\n\\nRsync, FTP, SFTP and Git\\nYou can use a middleman-deploy gem to deploy the project over Rsync, FTP, SFTP or Git.\\nExamples\\nThe Texas Tribune uses this project for its donations site. You can see it live here and visit the repo here. This should give you a good idea of what\\'s possible with the project and what additional customizations you can add to the project so that it fits your particular needs.\\nLook at the example project on Heroku here.\\nContributors\\n\\nKathryn Beaty - kbeaty@texastribune.org\\n\\nThank you to Masonry, the digital creative agency we worked with to reach the design for this project.\\nPlease feel free to reach out with any questions or comments. Thanks!\\n\\n\\n',\n",
       "  'language': 'HTML'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nDonor Wall: Spring Membership 2015\\nA temporary fix to ensure 2015 spring membership donations are docuemented in realtime as we switch from Sugar to Salesforce.\\nBuilt w/ News Apps' graphic kit. To install, clone repo and run:\\nnpm install && bower install\\nnpm install jquery\\n\\nTo test locally, run:\\ngulp serve\\n\\nTo deploy to Tribune's S3 account, you'll need to add a [profile newsapps] to ~/.aws/config. It should look something like this:\\n[profile newsapps]\\naws_access_key_id=YOUR_UNIQUE_ID\\naws_secret_access_key=YOUR_SECRET_ACCESS_KEY\\n\\nThen you can run these commands to build and deploy:\\ngulp\\nnpm run deploy\\n\\nThe package will deploy to graphics.texastribune.org/donor-wall. To change the location, update the package.json file.\\nFor more commands, see graphic kit.\\n\\n\\n\",\n",
       "  'language': 'CSS'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nWaterbug\\n\\nWhat is this?\\nAssumptions\\nWhat\\'s in here?\\nBootstrap the project\\nHide project secrets\\nSave media assets\\nAdd a page to the site\\nRun the project\\nCOPY configuration\\nCOPY editing\\nArbitrary Google Docs\\nRun Python tests\\nRun Javascript tests\\nCompile static assets\\nTest the rendered app\\nDeploy to S3\\nDeploy to EC2\\nInstall cron jobs\\nInstall web services\\nRun a remote fab command\\nReport analytics\\n\\nWhat is this?\\nWaterbug puts a watermarked logo and appropriate credit line on top of a photo for tweeting purposes.\\nSample Image\\n\\nInterface\\n\\nThis code is open source under the MIT license. See LICENSE for complete details.\\nAssumptions\\nThe following things are assumed to be true in this documentation.\\n\\nYou are running OSX.\\nYou are using Python 2.7. (Probably the version that came OSX.)\\nYou have virtualenv and virtualenvwrapper installed and working.\\nYou have NPR\\'s AWS credentials stored as environment variables locally.\\n\\nFor more details on the technology stack used with the app-template, see our development environment blog post.\\nWhat\\'s in here?\\nThe project contains the following folders and important files:\\n\\nconfs -- Server configuration files for nginx and uwsgi. Edit the templates then fab <ENV> servers.render_confs, don\\'t edit anything in confs/rendered directly.\\ndata -- Data files, such as those used to generate HTML.\\nfabfile -- Fabric commands for automating setup, deployment, data processing, etc.\\netc -- Miscellaneous scripts and metadata for project bootstrapping.\\njst -- Javascript (Underscore.js) templates.\\nless -- LESS files, will be compiled to CSS and concatenated for deployment.\\ntemplates -- HTML (Jinja2) templates, to be compiled locally.\\ntests -- Python unit tests.\\nwww -- Static and compiled assets to be deployed. (a.k.a. \"the output\")\\nwww/assets -- A symlink to an S3 bucket containing binary assets (images, audio).\\nwww/live-data -- \"Live\" data deployed to S3 via cron jobs or other mechanisms. (Not deployed with the rest of the project.)\\nwww/test -- Javascript tests and supporting files.\\napp.py -- A Flask app for rendering the project locally.\\napp_config.py -- Global project configuration for scripts, deployment, etc.\\ncopytext.py -- Code supporting the Editing workflow\\ncrontab -- Cron jobs to be installed as part of the project.\\npublic_app.py -- A Flask app for running server-side code.\\nrender_utils.py -- Code supporting template rendering.\\nrequirements.txt -- Python requirements.\\nstatic.py -- Static Flask views used in both app.py and public_app.py.\\n\\nBootstrap the project\\nNode.js is required for the static asset pipeline. If you don\\'t already have it, get it like this:\\nbrew install node\\ncurl https://npmjs.org/install.sh | sh\\n\\nThen bootstrap the project:\\ncd waterbug\\nmkvirtualenv waterbug\\npip install -r requirements.txt\\nnpm install\\nfab update\\n\\nProblems installing requirements? You may need to run the pip command as ARCHFLAGS=-Wno-error=unused-command-line-argument-hard-error-in-future pip install -r requirements.txt to work around an issue with OSX.\\nHide project secrets\\nProject secrets should never be stored in app_config.py or anywhere else in the repository. They will be leaked to the client if you do. Instead, always store passwords, keys, etc. in environment variables and document that they are needed here in the README.\\nAny environment variable that starts with waterbug will be automatically loaded when app_config.get_secrets() is called.\\nSave media assets\\nLarge media assets (images, videos, audio) are synced with an Amazon S3 bucket specified in app_config.ASSETS_S3_BUCKET in a folder with the name of the project. (This bucket should not be the same as any of your app_config.PRODUCTION_S3_BUCKETS or app_config.STAGING_S3_BUCKETS.) This allows everyone who works on the project to access these assets without storing them in the repo, giving us faster clone times and the ability to open source our work.\\nSyncing these assets requires running a couple different commands at the right times. When you create new assets or make changes to current assets that need to get uploaded to the server, run fab assets.sync. This will do a few things:\\n\\nIf there is an asset on S3 that does not exist on your local filesystem it will be downloaded.\\nIf there is an asset on that exists on your local filesystem but not on S3, you will be prompted to either upload (type \"u\") OR delete (type \"d\") your local copy.\\nYou can also upload all local files (type \"la\") or delete all local files (type \"da\"). Type \"c\" to cancel if you aren\\'t sure what to do.\\nIf both you and the server have an asset and they are the same, it will be skipped.\\nIf both you and the server have an asset and they are different, you will be prompted to take either the remote version (type \"r\") or the local version (type \"l\").\\nYou can also take all remote versions (type \"ra\") or all local versions (type \"la\"). Type \"c\" to cancel if you aren\\'t sure what to do.\\n\\nUnfortunantely, there is no automatic way to know when a file has been intentionally deleted from the server or your local directory. When you want to simultaneously remove a file from the server and your local environment (i.e. it is not needed in the project any longer), run fab assets.rm:\"www/assets/file_name_here.jpg\"\\nAdding a page to the site\\nA site can have any number of rendered pages, each with a corresponding template and view. To create a new one:\\n\\nAdd a template to the templates directory. Ensure it extends _base.html.\\nAdd a corresponding view function to app.py. Decorate it with a route to the page name, i.e. @app.route(\\'/filename.html\\')\\nBy convention only views that end with .html and do not start with _  will automatically be rendered when you call fab render.\\n\\nRun the project\\nA flask app is used to run the project locally. It will automatically recompile templates and assets on demand.\\nworkon $PROJECT_SLUG\\nfab app\\n\\nVisit localhost:8000 in your browser.\\nCOPY configuration\\nThis app uses a Google Spreadsheet for a simple key/value store that provides an editing workflow.\\nTo access the Google doc, you\\'ll need to create a Google API project via the Google developer console.\\nEnable the Drive API for your project and create a \"web application\" client ID. Use http://localhost:8000/authenticate/ and http://127.0.0.1:8000/authenticate for the redirect URIs and http://localhost:8000 and http://127.0.0.1:8000 for the Javascript origins.\\nYou\\'ll need to set some environment variables:\\nexport GOOGLE_OAUTH_CLIENT_ID=\"something-something.apps.googleusercontent.com\"\\nexport GOOGLE_OAUTH_CONSUMER_SECRET=\"bIgLonGStringOfCharacT3rs\"\\nexport AUTHOMATIC_SALT=\"jAmOnYourKeyBoaRd\"\\n\\nNote that AUTHOMATIC_SALT can be set to any random string. It\\'s just cryptographic salt for the authentication library we use.\\nOnce set up, run fab app and visit http://localhost:8000 in your browser. If authentication is not configured, you\\'ll be asked to allow the application for read-only access to Google drive, the account profile, and offline access on behalf of one of your Google accounts. This should be a one-time operation across all app-template projects.\\nIt is possible to grant access to other accounts on a per-project basis by changing GOOGLE_OAUTH_CREDENTIALS_PATH in app_config.py.\\nCOPY editing\\nView the sample copy spreadsheet.\\nThis document is specified in app_config with the variable COPY_GOOGLE_DOC_KEY. To use your own spreadsheet, change this value to reflect your document\\'s key. (The long string of random looking characters in your Google Docs URL. For example: 1DiE0j6vcCm55Dyj_sV5OJYoNXRRhn_Pjsndba7dVljo)\\nA few things to note:\\n\\nIf there is a column called key, there is expected to be a column called value and rows will be accessed in templates as key/value pairs\\nRows may also be accessed in templates by row index using iterators (see below)\\nYou may have any number of worksheets\\nThis document must be \"published to the web\" using Google Docs\\' interface\\n\\nThe app template is outfitted with a few fab utility functions that make pulling changes and updating your local data easy.\\nTo update the latest document, simply run:\\nfab text.update\\n\\nNote: text.update runs automatically whenever fab render is called.\\nAt the template level, Jinja maintains a COPY object that you can use to access your values in the templates. Using our example sheet, to use the byline key in templates/index.html:\\n{{ COPY.attribution.byline }}\\n\\nMore generally, you can access anything defined in your Google Doc like so:\\n{{ COPY.sheet_name.key_name }}\\n\\nYou may also access rows using iterators. In this case, the column headers of the spreadsheet become keys and the row cells values. For example:\\n{% for row in COPY.sheet_name %}\\n{{ row.column_one_header }}\\n{{ row.column_two_header }}\\n{% endfor %}\\n\\nWhen naming keys in the COPY document, please attempt to group them by common prefixes and order them by appearance on the page. For instance:\\ntitle\\nbyline\\nabout_header\\nabout_body\\nabout_url\\ndownload_label\\ndownload_url\\n\\nArbitrary Google Docs\\nSometimes, our projects need to read data from a Google Doc that\\'s not involved with the COPY rig. In this case, we\\'ve got a helper function for you to download an arbitrary Google spreadsheet.\\nThis solution will download the uncached version of the document, unlike those methods which use the \"publish to the Web\" functionality baked into Google Docs. Published versions can take up to 15 minutes up update!\\nMake sure you\\'re authenticated, then call oauth.get_document(key, file_path).\\nHere\\'s an example of what you might do:\\nfrom copytext import Copy\\nfrom oauth import get_document\\n\\ndef read_my_google_doc():\\n    file_path = \\'data/extra_data.xlsx\\'\\n    get_document(\\'0AlXMOHKxzQVRdHZuX1UycXplRlBfLVB0UVNldHJYZmc\\', file_path)\\n    data = Copy(file_path)\\n\\n    for row in data[\\'example_list\\']:\\n        print \\'%s: %s\\' % (row[\\'term\\'], row[\\'definition\\'])\\n\\nread_my_google_doc()\\n\\nRun Python tests\\nPython unit tests are stored in the tests directory. Run them with fab tests.\\nRun Javascript tests\\nWith the project running, visit localhost:8000/test/SpecRunner.html.\\nCompile static assets\\nCompile LESS to CSS, compile javascript templates to Javascript and minify all assets:\\nworkon waterbug\\nfab render\\n\\n(This is done automatically whenever you deploy to S3.)\\nTest the rendered app\\nIf you want to test the app once you\\'ve rendered it out, just use the Python webserver:\\ncd www\\npython -m SimpleHTTPServer\\n\\nDeploy to S3\\nfab staging master deploy\\n\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nAEIS\\nTools for analyzing data from the Academic Excellence Indicator System and the Texas Academic Performance Report.\\nFor the 2012-2013 school year and later, see the Texas Academic Performance Reports.\\n\\nThe TAPRs were previously known as the Academic Excellence Indicator System (AEIS) Reports. Those reports were published from 1990-91 to 2011-12. They may be found at the AEIS Archive.\\n\\nHow to Use\\nTo scrape and download all data from AEIS:\\n$ python -m aeis.scrape data\\n\\nTo analyze the columns of the downloaded data:\\n$ python analyze.py data --reload\\n$ ls analysis.shelf\\n$ ls metadata.shelf\\n\\nYou should now be able to decompose any analyzed field into its metadata:\\n$ python analyze.py --decompose DH00A00T013R\\n\\nTo index all data in ElasticSearch:\\n$ export ES_HOST=localhost:9200\\n$ python index.py data --recreate\\n\\nNext Steps\\nWe still need to scrape and analyze the 2013 academic indicators,\\nbut those have moved to a new system: the Texas Academic Performance Report.\\nThe 2012-2013 data can be downloaded from this page:\\nhttp://ritter.tea.state.tx.us/perfreport/tapr/2013/download/DownloadData.html\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nUndrinkable\\nHundreds of thousands of Texans living along the state’s southern border have no access to safe, clean, reliable drinking water — and that’s if they have running water at all.\\n\\n\\n',\n",
       "  'language': 'HTML'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nTexas Elections Scrapers\\n\\n\\nHi. This is just me fooling around trying to come up a better way to scrape\\nelection results. The tricky logic has been refined in other Texas Tribune\\nprojects, but they were deeply tied to other logic.\\nThe idea is to split the process up into multiple logical steps that other\\npeople might find useful:\\n\\nIngest results: Typically either with curl or cat or anything that pipes\\noutput to stdout.\\nSerialize the output html as JSON: Does not attempt to extract information.\\nJust separates data from the html. This is the hard part that scrapers have\\ntrouble with.\\nInterpret the serialized output: Turns the raw serialized data into\\nsomething you might expect to see from a nice API.\\n\\nIn a Extract, transform, load (ETL) process, this just covers the extractions,\\nwith support for minor transforming.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nNews Apps Django Template\\nA custom template for News Apps Django projects. In a constant state of development.\\nRequirements\\n\\nDjango 1.7\\nvirtualenvwrapper\\nWillingness to change this README to something that makes sense with your project post-generation\\n\\nGetting started\\nPlease note – this guide assumes you are using OS X. If you aren\\'t, you hopefully know the equivalent commands to make these things happen. If you don\\'t, find someone to help you!\\nFirst, create the folder for you project.\\nmkdir <folder-of-your-project>\\nThen, create the virtual environment for your project.\\nmkvirtualenv <name-of-your-project>-dev\\nNext, install Django.\\npip install django\\nYou may need to re-initialize your virtual environment for autocomplete of Django commands. (workon <name-of-your-project>-dev)\\nNow we\\'re ready to start cookin\\'. (The name of your project doesn\\'t have to be the same as the folder you created.)\\ndjango-admin startproject --template=https://github.com/texastribune/newsapps-django-template/archive/master.zip --extension=gitignore,html,py <name-of-your-project> <folder-of-your-project>\\nJump into your newly created project folder, get git initialized, and make your first commit!\\ncd <folder-of-your-project>\\ngit init\\ngit add .\\ngit commit -m \"Initial commit\"\\nNow, install your local development requirements.\\npip install -r requirements/local.txt\\nYou should be able to run your first migrate now! Give it a try.\\npython <name-of-your-project>/manage.py migrate\\nAnd it should be able to handle runserver now, too.\\npython <name-of-your-project>/manage.py runserver\\nIf everything checks out, you\\'re good to go. Don\\'t forget to replace this README!\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nriogrande.texastribune.org\\nGet ready. The water is coming.\\n\\n\\n',\n",
       "  'language': 'CSS'},\n",
       " {'readme': 'No readme file.', 'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nripley\\n\\n\"The stomach burster.\" All of the best places to eat near the office.\\nResources\\n\\nGood for copying x,y coords and plain text searching of location: http://mygeoposition.com/\\nGoogle Maps : will provide walking/driving distance estimates and parsing URL will also give lat/long coords\\n\\nInstructions\\n\\n\\nAdd stuff to suchnoms.yml (in alphabetical order so we don\\'t accidentally\\nadd the same place twice please).\\n\\n\\nUpdate the geojson file:\\n make build\\n\\n\\n\\nFAQ\\nWhat\\'s your favorite place to eat?\\n\\nI really enjoy a 7-11 pizza. It\\'s only $5, so it\\'s really a good shame/$\\nvalue.\\n\\nWhy is the script written in Python? Python 3 in fact.\\n\\nShutup, it\\'s the future! It\\'ll work in Python 2 too.\\n\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\njquery.aura.js\\nAllows you to trigger code when the mouse gets near an element.\\nWhy\\nWe use it to start loading expensive resources when we think a user is about to\\nask for them. This way, we trade always taking a bit hit up front with always\\ntaking a small hit for this library to sometimes taking a big hit later without\\nadding delays. The problem with using mouse events directly with the element is\\nthat by the time a user\\'s mouse is over a button, it\\'s too late to start\\npreloading that button\\'s interaction.\\nHow\\njquery.aura.js works by injecting invisible helper elements on top of your\\npage, and detecting when the mouse moves over it. If you\\'re watching for mouse\\nevents, these helper elements shouldn\\'t interfere because they remove\\nthemselves from the DOM the moment they get activated. If this script does not\\nwork for you, there are alternatives, see the [Prior Art] section.\\nUsage\\nTrigger a callback when the mouse gets within a 40 pixel box around <div id=\"cake\"/>:\\n$(\\'#cake\\').aura(40, function () { alert(\\'Remember your diet!\\'); });\\n\\nYou can also specify the box as if you were defining css padding (pixels only):\\n$(\\'#cookies\\').aura(\\'40px 30px 20px 10px\\', function () { alert(\"I\\'ll just have one\"); });\\n\\nYou can also specify the callback promise style:\\n$(\\'#pie\\').aura(15).then(function () { alert(\\'3.14159265359\\'); });\\n\\nDemo\\nFor Developers\\nGetting started\\n# install requirements\\nnpm install\\n\\n\\n# do a build\\ngrunt\\nPrior art\\n\\nhttps://github.com/e-sites/perimeter.js\\n\\nVery similar. Useful if you need repeated detection, it listens to the\\n\\'mousemove\\' event instead of \\'mouseover\\'.\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTeradek Sputnik\\nThis runs the server component for Teradek's Bond video transmitters.\\nUsage\\nHere are some examples of how you could use this image. For more, see the\\nmakefile.\\nStarting a Sputnik server\\ndocker run -p 5111:5111 -p 1957:1957 -p 554:554 --detach --name sputnik \\\\\\n  texastribune/sputnik\\n\\nView the logs on a running server\\nYou can do this two ways. You can just attach to the server:\\ndocker attach --sig-proxy=false sputnik\\n\\nOr you can mount the logs in a utility container:\\ndocker run --rm -i -t --name debug --link sputnik:sputnik --volumes-from sputnik ubuntu\\n$ tail -f /var/log/sputnik*.log\\n\\nUse your own Sputnik configuration\\n\\n\\nGet the default Sputnik configuration. Skip this if you already have some\\nconfig files.\\n # assuming you want it copied to ~/volumes/sputnik and it already exists\\n docker run --detach --name sputnik-tmp texastribune/sputnik\\n docker cp sputnik-tmp:data/conf ~/volumes/sputnik\\n docker rm -f sputnik-tmp\\n\\n\\n\\nRun Sputnik using this configuration:\\n docker run -p 5111:5111 -p 1957:1957 -p 554:554 --detach --name sputnik \\\\\\n   -v ~/volumes/sputnik:/data texastribune/sputnik\\n\\n\\n\\nChanges saved in the Sputnik admin will persist to your configs. You won't\\nneed to edit them yourself under normal circumstances. NOTE: Depending\\non how you run Sputnik, you should either edit ports in the config and use\\n--net=host, or avoid editing ports and configure ports using Docker's port\\npublishing.\\n\\n\\nSome suggested changes are: change admin and guest passwords, enable the\\nstream monitor, and change the upload folder to /data/upload.\\n\\n\\nReferences\\n\\nhttp://cdn.teradek.com/Public/Sputnik/Docs/Teradek_Sputnik_Setup_Guide_v2_0512.pdf\\n\\n\\n\\n\",\n",
       "  'language': 'Makefile'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\ntx-prez-candidates\\nA list of all of the potential presidential candidates with Texas connections.\\nNode modules that you'll need to install to process speadsheet: +googleapis +request +untildify +xlsx +marked\\n\\n\\n\",\n",
       "  'language': 'CSS'},\n",
       " {'readme': 'No readme file.', 'language': 'CSS'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTexas Tribune News Apps Generator\\nYeoman generator that scaffolds out a front-end web app OR embeddable graphic using Texas Tribune stylings. Generously inspired by the original Yeoman web app generator.\\nPLEASE NOTE: This is still very much in development. Your mileage my vary until we hit 1.0.\\n\\nFeatures\\n\\nAutomatic CSS prefixing with autoprefixer\\nBuilt in preview server with LiveReload\\nAutomatic compiling of Sass/SCSS with grunt-contrib-sass (requires Ruby + Sass)\\nAutomatic wiring of installed Bower components with grunt-wiredep\\nBuilt in image optimization via grunt-contrib-imagemin\\n\\nAnd so much more! Please take a look at each generator's package.json and Gruntfile.js to learn more about what's possible.\\nInstall\\n$ npm install -g generator-tt-newsapps\\nThe News Apps Generator depends on Node.js and npm.\\nUsage\\nSo what do you wanna create? Are you developing a web app or a web graphic? The command you run will differ depending on your needs.\\nThe web app generator is the default. Create the folder you want your project to live in then run the following command inside of it.\\n$ mkdir <project-name>\\n$ cd <project-name>\\n$ yo tt-newsapps\\nIf you are creating a web graphic, use this command instead.\\n$ mkdir <project-name>\\n$ cd <project-name>\\n$ yo tt-newsapps:graphic\\nHow about a map?\\n$ mkdir <project-name>\\n$ cd <project-name>\\n$ yo tt-newsapps:map\\nAfter installing, run grunt serve to preview what you generator has set up. Now get to work! When it comes time to ship to production, use grunt build to prepare dist folder.\\nDeployment\\nYou'll need aws-cli for deployment. Install it, then run:\\nmake deploy\\nLicense\\nLicensed under the MIT License.\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nQuick n dirty CLI for checking ec2 inventory\\nSpecifically design for my own personal workflow.\\nHere Be Dragons\\nIf you\\'re doing something serious, you should look into these better\\nalternatives:\\n\\nhttps://github.com/DrGonzo65/ec2-cli-tools - very similar package that lets\\nyou glob\\nhttps://github.com/mattrobenolt/ec2 - lets you query instances using django-\\norm-like syntax\\nhttps://github.com/scopely-devops/skew - lets you glob ARNs\\nawscli - official aws cli\\n\\nIf you\\'re still here, here\\'s the deal: This is a CLI for querying aws for what\\ninstances are running and display them in a table with the same columns I have\\nenabled in the web interface. You can sort by specifying the headers:\\nvoyeur name\\nvoyeur launch_time\\nvoyeur environment\\n\\nYou can filter similarily like:\\nvoyeur environment=production\\n\\nOr go crazy (show production foo sites ordered by launch time):\\nvoyeur environment=production site=foo launch_time\\n\\nI picked the columns and names so that line lengths are under 120 columns;\\nwhich is how wide my terminal is.\\nBut Wait, There\\'s More\\nIf you make the first argument elb or rds, you can list your elastic load\\nbalancers and databases too. You can also do add ec2 if you like being\\nconsistent.\\nExample\\nList EC2\\nname                   environment    site     ip              private_ip      launch_time    id\\n---------------------  -------------  -------  --------------  --------------  -------------  ----------\\nhodor-weg              prod           groot    54.85.94.132    10.0.0.2        2014-06-17     i-27fghb69\\ngroot-elasticsearch    prod           hodor                    10.0.0.1        2014-08-08     i-abcdcaz7\\nList ELB\\n$ voyeur elb\\nname     dns_name                                     pool  created_time\\n-------  -------------------------------------------  ------  ------------------------\\ngroot    groot-1529220.us-east-1.elb.amazonaws.com         3  2012-02-03T01:41:02.930Z\\nhodor    hodor-4545272.us-east-1.elb.amazonaws.com         2  2014-03-08T06:15:53.610Z\\nList RDS\\n$ voyeur rds\\nid          uri\\n----------  -------------------------------------------------------------------------------\\ngroot-db    postgres://iamgroot@groot-db.cya8ag0rj.us-east-1.rds.amazonaws.com:5432/groot\\nhodor-db    postgres://iamgroot@hodor-db.cya8ag0rj.us-east-1.rds.amazonaws.com:5432/groot\\nInstallation\\npip install https://github.com/texastribune/ec2-voyeur/archive/master.tar.gz\\n\\nUsage\\nConfigure boto following the official instructions.\\nA basic configuration would just involve setting the AWS_ACCESS_KEY_ID,\\nAWS_SECRET_ACCESS_KEY environment variables.\\nAWS IAM Policy\\nThis library requires the Describe* permissions. You can use use the \"Read Only\\nAccess\" policy template, or you can make your own with at least these\\npermissions:\\n{\\n  \"Statement\": [\\n    {\\n      \"Action\": [\\n        \"ec2:Describe*\",\\n        \"elasticloadbalancing:Describe*\",\\n        \"rds:Describe*\"\\n      ],\\n      \"Effect\": \"Allow\",\\n      \"Resource\": \"*\"\\n    }\\n  ]\\n}\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\n2014-quotes\\nA roundup of memorable quotes from 2014 in a cute lil slideshow.\\nNode modules that you'll need to install to process speadsheet: +googleapis +request +untildify +xlsx +marked\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nfall-membership-drive\\nGetting Started\\nTo get started, clone down the project repo.\\nYou'll need the Ruby gem bundler. To install the bundler, run:\\ngem install bundler\\n\\nInstall the necessary gems from the Gemfile by running:\\nbundle install\\n\\nDevelopment\\nMiddleman runs locally on port 4567. It will live reload as changes are made to files. To start up the Middleman server, run:\\nbundle exec middleman\\n\\nTo build the site, run:\\nbundle exec middleman build\\n\\nWhen Middleman builds, it creates a static file for each file located in the source folder. The build process is configured in config.rb.\\nMake sure that you've already installed the editorconfig plugin for your editor at http://editorconfig.org/#download to conform to the Tribune's coding styles.\\nDeploying\\nTo deploy, the project uses the middleman-s3_sync gem to push and compile the site to s3 after building.\\nBy default, in config.rb, config.after_build is set to false. Set this to true. Check that your AWS_ACCESS_KEY and AWS_ACCESS_SECRET environment variables are set, and then run:\\nbundle exec middleman build\\n\\nIn your terminal, you should see s3_sync applying any updates to files for the project. You can also check the project s3 bucket to ensure that all files have been synced there. Change config.after_build back to its default of false after deploying.\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nThis is just a Docker wrapper around this project:\\nhttps://github.com/bitly/google_auth_proxy\\nFollow the directions there for creating a client ID and secret.\\nYou may want to run it something like this:\\ndocker run -d --publish=0.0.0.0:8080:8080 texastribune/google-auth-proxy -client-id=\"<client id>\" \\\\\\n  -client-secret=\"<client secret>\" -google-apps-domain=\"mydomain.org\" \\\\\\n  -http-address=\"0.0.0.0:8080\" \\\\\\n  -redirect-url=\"https://app.mydomain.org/oauth2/callback\" \\\\\\n  -upstream=\"http://my-protected-app.mydomain.org/\"\\n\\n\\n\\n',\n",
       "  'language': 'Makefile'},\n",
       " {'readme': 'No readme file.', 'language': 'No language specified.'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\ns3-parallel-put  Parallel uploads to Amazon AWS S3\\ns3-parallel-put speeds the uploading of many small keys to Amazon AWS S3 by\\nexecuting multiple PUTs in parallel.\\nDependencies\\n\\nPython 2.X\\nBoto\\n\\nUsage\\nThe program reads your credentials from the environment variables\\nAWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.\\ns3-parallel-put --bucket=BUCKET --prefix=PREFIX SOURCE\\nKeys are computed by combining PREFIX with the path of the file, starting\\nfrom SOURCE.  Values are file contents.\\nThere are a few other options:\\n--dry-run causes the program to print what it would do, but not to upload\\nany files.  It is strongly recommended that you test the program with this\\noption before transferring any real data.\\n--limit=N causes the program to upload no more than N files.  Combined\\nwith --dry-run, this is also useful for testing.\\n--put=MODE sets the heuristic used for deciding whether to upload a file\\nor not.  Valid modes are:\\n\\n\\nadd set the key\\'s content if the key is not already present.\\n\\n\\nstupid always set the key\\'s content.\\n\\n\\nupdate set the key\\'s content if the key is not already present and its\\ncontent has changed (as determined by its ETag).\\n\\n\\nThe default heuristic is update.  If you know that the keys are not\\nalready present then stupid is fastest (it avoids an extra HEAD request\\nfor each key).  If you know that some keys are already present and that they\\nhave the correct values, then add is faster than update (it avoids\\ncalculating the MD5 sum of the content on the client side).\\n--content-type=CONTENT-TYPE sets the Content-Type header.\\n(accepted parameter guess), (eg. --content-type=guess)\\n--gzip compresses all values and sets the Content-Encoding header to\\ngzip.\\n--processes=N sets the number of parallel upload processes.\\n--verbose causes more output to be printed, including progress of individual files.\\n--quiet causes less output.\\n--secure and --insecure control whether a secure connection is used.\\n--grant applies a\\nCanned ACL\\nto all files uploaded.\\n--header=HEADER:VALUE adds an arbitrary header to the S3 file. This\\noption can be specified multiple times.\\nArchitecture\\n\\nA walker process generates (filename, key_name) pairs and inserts them in\\nput_queue.\\nMultiple putter processes consume these pairs in parallel, uploading the\\nfiles to S3 and sending file-by-file statistics to stat_queue.\\nA statter process consumes these file-by-file statistics and generates\\nsummary statistics.\\n\\nBugs\\n\\nLimited error checking.\\n\\nTo Do\\n\\n\\nUpdate documentation.\\n\\n\\nAutomatically parallelize uploads of large files by splitting into chunks.\\n\\n\\nRelated projects\\n\\nJetS3t\\ns3cmd\\nsync_media_s3\\n\\nLicence\\nThe MIT License (MIT)\\nCopyright (c) 2011-2014 Tom Payne\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\nvim: set spell spelllang=en textwidth=76:\\n\\n\\n',\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nThis is an attempt to put all the requirements necessary to do headless browser\\ntesting in one docker image and a script to run some of those tests (most\\nlikely through a CI).\\nThe script will run YSLow and sitespeed.io.\\nThe script will output a \\'yslow.xml\\' file for the base URL, a TAP file for each\\nURL, and sitespeed HTML output for the base URL.  All of this is left in the\\n\"/results\" volume.\\n\\n\\n',\n",
       "  'language': 'Shell'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nWjordpress\\n\\n\\nWjordpress is a reusable app for Django that allows you to use WordPress as\\nyour editing interface and use Django for your presentation. You an also hook\\nmultiple WordPress sites to a single Django project.\\nYou choose your level of integration. You can do a one time import, periodic\\nsync, manual sync, or near real-time updates using a web hook.\\nYou can also start using WordPress content immediately without writing any\\nurls, views, or templates with the built in templatetag.\\nSee the ReadTheDocs site for\\nmore.\\n\\nScenarios\\nEasy peasy lemon squeezy:\\n\\nSetup a link to a WordPress site\\nWrite views and templates to display WordPress content\\n\\nThis is how the reference project syncs with my blog\\nBring your own models:\\n\\nSetup a link to a WordPress site\\nCreate a post_save signal on the Wjordpress models to sync to your own\\ncontent models\\nWrite a view and template to display your content models\\n\\n\\nHow to pronounce \"Wjordpress\": http://youtu.be/tmyGrk99uzM\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nox-scale\\nFor guessing the weight of oxen\\nSetting up the project\\nInstall requirements:\\npip install -r requirements.txt\\nnpm install\\n\\nSetup your Python path:\\nadd2virtualenv .\\n\\nSetup your environment:\\nDJANGO_SETTINGS_MODULE=ox_scale.settings\\nDEBUG=1\\n\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nWARNING: If not careful you could configure an open relay with this.  Limit what can speak to port 25 of this container.\\nNotes:\\n\\n/var/log is exported as a VOLUME so you can view the postfix logs\\nuses MANDRILL_USERNAME and MANDRILL_KEY as environment variables\\nsee Makefile\\n\\n\\n\\n',\n",
       "  'language': 'Shell'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nmiddleman-newsapp\\nThis template was designed to help creating strong newsapps.\\nInstallation\\nMiddleman now supports project templates. To use Amicus as a template, clone the Git repository into ~/.middleman, like so:\\ngit clone http://github.com/malev/middleman-newsapp.git ~/.middleman/newsapp\\nthen use the new template argument for the middleman init command:\\nmiddleman init my_new_project --template=newsapp\\nEasy peasy!\\nTODO\\n\\nAssets should be included on relative path\\nCSV to HTML helper\\n\\n\\n\\n',\n",
       "  'language': 'CSS'},\n",
       " {'readme': 'No readme file.', 'language': 'JavaScript'},\n",
       " {'readme': 'No readme file.', 'language': 'CSS'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nShouldering the Burden\\nShouldering the Burden (renamed to Bypassed by the Miracle) is a weeklong series of stories about people who have been left out of the economic growth of the Texas Miracle. This part of the story, No Miracle Here, is a series of vignettes about these people.\\nSetup\\nThe builder depends on both Grunt and http://bower.io/, which both depend on Node.js. If you do not have these installed, you'll need to get them.\\nNode.js comes first – we recommend using Homebrew if you are a OS X creature (which we all are, for now). This should play well with Windows, but documenting that setup process is not a priority.\\nbrew update && install node\\nOnce Node.js is installed, you'll use npm – the Node.js package manager – to install Grunt and Bower globally.\\nnpm install -g grunt-cli bower\\nOnce those are in, you're ready to start installing the builder's dependencies.\\nFrom the template's root directory:\\nnpm install && bower install\\nAnd away you go!\\nMaps\\nTo create the maps, use https://github.com/duner/d3_texas_pins and the NYTimes's SVG Crowbar. I ended up using ImageMagik to convert the SVGs to PNGs so that they'd work on more browsers.\\n\\n\\n\",\n",
       "  'language': 'CSS'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nAbout cloudviz\\nThis package exposes Amazon CloudWatch as a data source for Google Chart Tools.  With it, you can quickly generate graphs like this:\\n\\nCloudviz is proudly sponsored by Bizo.  For details on the cloudviz\\'s origin, see this blog post.\\nIf you\\'re looking for easiest way to start graphing CloudWatch data, check out Cloudgrapher.  Cloudgrapher is a free CloudWatch dashboard, and is effectively a hosted, batteries-included extension of cloudviz.\\nGetting started\\n\\nFamiliarize yourself with:\\n\\nAmazon CloudWatch (docs)\\nGoogle Visualization API (docs)\\n\\n\\nDownload and install:\\n\\nboto - a Python interface for Amazon Web Services\\ngviz_api - a Python library for creating Google Visualization API data sources\\npytz - world timezone definitions\\n\\n\\nSet **AWS_ACCESS_KEY_ID **and **AWS_SECRET_ACCESS_KEY **in settings.py\\nMake cloudviz.py web-accessible using your favorite HTTP server\\n\\nUsing cloudviz\\ncloudviz expects the following query parameters as a JSON-encoded string passed to a qs parameter.  Default values for each parameter may be set in settings.py:\\n\\nnamespace (str) - CloudWatch namespace (e.g., \"AWS/ELB\")\\nmetric (str) - CloudWatch metric (e.g., \"Latency\")\\nunit (str) - CloudWatch unit (e.g., \"Seconds\")\\nstatistics (list of str) - CloudWatch statistics (e.g., [\"Average\",\"Maximum\"])\\ndimensions (dict of str) - CloudWatch dimensions (e.g., {\"LoadBalancerName\": \"example-lb\"})\\nend_time (date) - end time for queried interval (e.g., new Date)\\nstart_time (date) - start time for queried interval (e.g., start_time.setDate(end_time.getDate-3))\\nrange (int) - desired time range, in hours, for queried interval (e.g., 24).  Note: range may be substituted for start_time, end_time, or both:\\n\\nif range and end_time are specified, start_time is calculated as ( end_time - range )\\nif range and start_time are specified, end_time is calculated as ( start_time + range )\\nif only range is specified, end_time is set to the current time and start_time is calculated as ( current time - range )\\n\\n\\nperiod (int) - (optional) CloudWatch period (e.g., 120).  Notes: must be a multiple of 60; if period is not specified, it will be automatically calculated as the smallest valid value (resulting in the most data points being returned) for the queried interval.\\nregion (str) - (optional) AWS region (e.g., \"us-west-1\"; default is \"us-east-1\")\\ncalc_rate (bool) - (optional) when set to True and statistics includes \"Sum\", cloudviz converts Sum values to per-second Rate values by dividing Sum by seconds in period (e.g., for RequestCount, 150 Requests per period / 60 seconds per period = 2.5 Requests per second)\\ncloudwatch_queries (list of dict) - encapsulates each CloudWatch query, allowing for multiple queries to be graphed in a single chart.  Minimally, cloudwatch_queries must contain one dict with prefix defined.  Optionally, any of the above parameters may also be defined inside one or more cloudwatch_queries\\n\\nprefix (str) - text identifier for data returned by a single CloudWatch query. This is prepended to the chart label of each data series (e.g., \"My LB \")\\n\\n\\n\\nExample: Graphing CPU utilization of two instances\\nHere\\'s a JavaScript snippet for building a URL to pass to cloudviz.  See examples/host-cpu.html for the rest of the code.  Note that **start_time **and **end_time **are set in settings.py.\\nvar qa = {\\n            \"namespace\": \"AWS/EC2\",       // CloudWatch namespace (string\\n            \"metric\": \"CPUUtilization\",   // CloudWatch metric (string)\\n            \"unit\": \"Percent\",            // CloudWatch unit (string)\\n            \"statistics\": [\"Average\",\"Maximum\"],      // CloudWatch statistics (list of strings)\\n            \"period\": 600,                // CloudWatch period (int)\\n            \"cloudwatch_queries\":         // (list of dictionaries)\\n            [\\n                {\\n                    \"prefix\": \"Instance 1 CPU \",   // label prefix for associated data sets (string)\\n                    \"dimensions\": { \"InstanceId\": \"i-bd14d3d5\"} // CloudWatch dimensions (dictionary)\\n                },\\n                {\\n                    \"prefix\": \"Instance 2 CPU \"\\n                    \"dimensions\": { \"InstanceId\": \"i-c514d3ad\"}\\n                }\\n            ]\\n         };\\n\\nvar qs = JSON.stringify(qa);\\nvar url = \\'http://\\' + window.location.host + \\'/cloudviz?qs=\\' + qs;  // assumes cloudviz.py is called at /data\\n\\nThe resulting URL should look something like this:\\nhttp://localhost:8080/cloudviz?qs={%22namespace%22:%22AWS/EC2%22,%22metric%22:%22CPUUtilization%22,%22unit%22:%22Percent%22,%22statistics%22:[%22Average%22,%22Maximum%22],%22period%22:600,%22cloudwatch_queries%22:[{%22prefix%22:%22Instance%201%20CPU%20%22,%22dimensions%22:{%22InstanceId%22:%22i-bd14d3d5%22}},{%22prefix%22:%22Instance%202%20CPU%20%22,%22dimensions%22:{%22InstanceId%22:%22i-c514d3ad%22}}]}&tqx=reqId%3A0\\nAnd the graph, when passed through Google\\'s Visualization API:\\n\\nMore examples\\nAdditional examples can be found in examples/, and are written to act as plug-and-play templates.\\nLicensing\\nCopyright 2010 Bizo, Inc. (Mike Babineau <michael.babineau@gmail.com>)\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\nTODO\\nThe Texas Tribune\\'s additions\\nStarting a server on port 5000:\\npython web.py\\n\\nOpen localhost:5000/reports/index.html\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nquotable\\n\\nWhat is this?\\nAssumptions\\nWhat\\'s in here?\\nInstall requirements\\nProject secrets\\nAdding a template/view\\nRun the project locally\\nHandling static assets\\nEditing workflow\\nRun Javascript tests\\nRun Python tests\\nCompile static assets\\nTest the rendered app\\nDeploy\\n\\nWhat is this?\\nQuotable is an app that lets you make sharable images out of quotations.\\n\\n\\nThis code is open source under the MIT license. See LICENSE for complete details.\\nAssumptions\\nThe following things are assumed to be true in this documentation.\\n\\nYou are running OSX.\\nYou are using Python 2.7. (Probably the version that came OSX.)\\nYou have virtualenv and virtualenvwrapper installed and working.\\n\\nFor more details on the technology stack used with the app-template, see our development environment blog post.\\nWhat\\'s in here?\\nThe project contains the following folders and important files:\\n\\ndata -- Data files, such as those used to generate HTML.\\netc -- Miscellaneous scripts and metadata for project bootstrapping.\\njst -- Javascript (Underscore.js) templates.\\nless -- LESS files, will be compiled to CSS and concatenated for deployment.\\ntemplates -- HTML (Jinja2) templates, to be compiled locally.\\ntests -- Python unit tests.\\nwww -- Static and compiled assets to be deployed. (a.k.a. \"the output\")\\nwww/live-data -- \"Live\" data deployed to S3 via cron jobs or other mechanisms. (Not deployed with the rest of the project.)\\nwww/test -- Javascript tests and supporting files.\\napp.py -- A Flask app for rendering the project locally.\\napp_config.py -- Global project configuration for scripts, deployment, etc.\\ncopytext.py -- Code supporting the Editing workflow\\nfabfile.py -- Fabric commands automating setup and deployment.\\nrender_utils.py -- Code supporting template rendering.\\nrequirements.txt -- Python requirements.\\n\\nInstall requirements\\nNode.js is required for the static asset pipeline. If you don\\'t already have it, get it like this:\\nbrew install node\\ncurl https://npmjs.org/install.sh | sh\\n\\nThen install the project requirements:\\ncd quotable\\nnpm install less universal-jst -g --prefix node_modules\\nmkvirtualenv --no-site-packages quotable\\npip install -r requirements.txt\\nfab update_copy\\n\\nProject secrets\\nProject secrets should never be stored in app_config.py or anywhere else in the repository. They will be leaked to the client if you do. Instead, always store passwords, keys, etc. in environment variables and document that they are needed here in the README.\\nAdding a template/view\\nA site can have any number of rendered templates (i.e. pages). Each will need a corresponding view. To create a new one:\\n\\nAdd a template to the templates directory. Ensure it extends _base.html.\\nAdd a corresponding view function to app.py. Decorate it with a route to the page name, i.e. @app.route(\\'/filename.html\\')\\nBy convention only views that end with .html and do not start with _  will automatically be rendered when you call fab render.\\n\\nRun the project locally\\nA flask app is used to run the project locally. It will automatically recompile templates and assets on demand.\\nworkon quotable\\npython app.py\\n\\nVisit localhost:8000 in your browser.\\nHandling static assets\\nMake an s3 bucket for your static assets. Update ASSETS_S3_BUCKET in app_config.py with the new location. This should be separate from the s3 bucket where you are deploying your app.\\nStatic assets should be stored in www/assets. To push new assets to the server, run fab assets_up. To pull existing assets down, run fab assets_down. To delete an asset, run fab assets_rm:\\'www/assets/FILE_NAME_OR_UNIX_GLOB\\'.\\nFor example, if you are starting from scratch, you would copy assets into the www/assets folder and then run fab assets_up. If you are working on this project with someone who has already created assets, and you would like to get them, run fab assets_down. And if you would like to delete all JPEG files in the folder www/assets, run fab assets_rm:\\'www/assets/*.jpg\\'.\\nEditing workflow\\nThe app is rigged up to Google Docs for a simple key/value store that provides an editing workflow.\\nView the sample copy spreadsheet here. A few things to note:\\n\\nIf there is a column called key, there is expected to be a column called value and rows will be accessed in templates as key/value pairs\\nRows may also be accessed in templates by row index using iterators (see below)\\nYou may have any number of worksheets\\nThis document must be \"published to the web\" using Google Docs\\' interface\\n\\nThis document is specified in app_config with the variable COPY_GOOGLE_DOC_KEY. To use your own spreadsheet, change this value to reflect your document\\'s key (found in the Google Docs URL after &key=).\\nThe app template is outfitted with a few fab utility functions that make pulling changes and updating your local data easy.\\nTo update the latest document, simply run:\\nfab update_copy\\n\\nNote: update_copy runs automatically whenever fab render is called.\\nAt the template level, Jinja maintains a COPY object that you can use to access your values in the templates. Using our example sheet, to use the byline key in templates/index.html:\\n{{ COPY.attribution.byline }}\\n\\nMore generally, you can access anything defined in your Google Doc like so:\\n{{ COPY.sheet_name.key_name }}\\n\\nYou may also access rows using iterators. In this case, the column headers of the spreadsheet become keys and the row cells values. For example:\\n{% for row in COPY.sheet_name %}\\n{{ row.column_one_header }}\\n{{ row.column_two_header }}\\n{% endfor %}\\n\\nRun Javascript tests\\nWith the project running, visit localhost:8000/test/SpecRunner.html.\\nRun Python tests\\nPython unit tests are stored in the tests directory. Run them with fab tests.\\nCompile static assets\\nCompile LESS to CSS, compile javascript templates to Javascript and minify all assets:\\nworkon quotable\\nfab render\\n\\n(This is done automatically whenever you deploy to S3.)\\nTest the rendered app\\nIf you want to test the app once you\\'ve rendered it out, just use the Python webserver:\\ncd www\\npython -m SimpleHTTPServer\\n\\nDeploy\\nfab deploy\\n\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': 'No readme file.', 'language': 'Shell'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nNews Apps App Template\\nA starting point for all of News Apps web mid-to-large size projects.\\nEventually, this will become Yeoman generator. For now, you'll need to clone this down and rename things manually. Not ideal! But will be fixed soon-ish.\\nSetup\\nThis tool depends on both Grunt and http://bower.io/, which both depend on Node.js. If you do not have these installed, you'll need to get them.\\nNode.js comes first – we recommend using Homebrew if you are a OS X creature (which we all are, for now). This should play well with Windows, but documenting that setup process is not a priority.\\nbrew update && install node\\nOnce Node.js is installed, you'll use npm – the Node.js package manager – to install Grunt and Bower globally.\\nnpm install -g grunt-cli bower\\nOnce those are in, you're ready to start installing the builder's dependencies.\\nFrom the template's root directory:\\nnpm install && bower install\\nAnd away you go!\\nHow to use\\nTODO\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nPym.js\\nAbout\\nUsing iframes in a responsive page can be frustrating. It’s easy enough to make an iframe’s width span 100% of its container, but sizing its height is tricky — especially if the content of the iframe changes height depending on page width (for example, because of text wrapping or media queries) or events within the iframe.\\nPym.js embeds and resizes an iframe responsively (width and height) within its parent container. It also bypasses the usual cross-domain issues.\\nUse case: The NPR Visuals team uses Pym.js to embed small custom bits of code (charts, maps, etc.) inside our CMS without CSS or JavaScript conflicts. See an example of this in action.\\n› Read the documentation\\n› Browse the API\\nDevelopment tasks\\nGrunt configuration is included for running common development tasks.\\nJavascript can be linted with jshint:\\ngrunt jshint\\n\\nAPI documention can be generated with jsdoc:\\ngrunt jsdoc\\n\\nLicense & Credits\\nReleased under the MIT open source license. See LICENSE for details.\\nPym.js was built by the NPR Visuals team, based on work by the NPR Tech Team and Ioseb Dzmanashvili. Thanks to Erik Hinton for suggesting the name.\\nAdditional contributors:\\n\\nPierre-Yves Jamon\\njugglinmike\\nDavid Rogers\\n\\n\\n\\n',\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nmkvirtualenv tribwire\\npip install -r requirements.txt\\nrun server with python example/manage.py runserver\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nBuild:\\ndocker build --tag=texastribune/logstash .\\n\\nor\\nmake build\\n\\nRun:\\ndocker run --detach=true --publish=514:514 --publish=5000:5000 texastribune/logstash\\n\\nor\\nmake run\\n\\n(I like to use the long version of the parameters because it's easier for me remember what they mean.)\\n\\n\\n\",\n",
       "  'language': 'Shell'},\n",
       " {'readme': 'No readme file.', 'language': 'Shell'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nlogstash-forwarder\\n♫ I\\'m a lumberjack and I\\'m ok! I sleep when idle, then I ship logs all day! I parse your logs, I eat the JVM agent for lunch! ♫\\n(This project was recently renamed from \\'lumberjack\\' to \\'logstash-forwarder\\' to\\nmake its intended use clear. The \\'lumberjack\\' name now remains as the network protocol, and \\'logstash-forwarder\\' is the name of the program. It\\'s still the same lovely log forwarding program you love.)\\nQuestions and support\\nIf you have questions and cannot find answers, please join the #logstash irc\\nchannel on freenode irc or ask on the logstash-users@googlegroups.com mailing\\nlist.\\nWhat is this?\\nA tool to collect logs locally in preparation for processing elsewhere!\\nResource Usage Concerns\\nPerceived Problems: Some users view logstash releases as \"large\" or have a generalized fear of Java.\\nActual Problems: Logstash, for right now, runs with a footprint that is not\\nfriendly to underprovisioned systems such as EC2 micro instances; on other\\nsystems it is fine. This project will exist until that is resolved.\\nTransport Problems\\nFew log transport mechanisms provide security, low latency, and reliability.\\nThe lumberjack protocol used by this project exists to provide a network\\nprotocol for transmission that is secure, low latency, low resource usage, and\\nreliable.\\nConfiguring\\nlogstash-forwarder is configured with a json file you specify with the -config flag:\\nlogstash-forwarder -config yourstuff.json\\nHere\\'s a sample, with comments in-line to describe the settings. Please please\\nplease keep in mind that comments are technically invalid in JSON, so you can\\'t\\ninclude them in your config.:\\n{\\n  # The network section covers network configuration :)\\n  \"network\": {\\n    # A list of downstream servers listening for our messages.\\n    # logstash-forwarder will pick one at random and only switch if\\n    # the selected one appears to be dead or unresponsive\\n    \"servers\": [ \"localhost:5043\" ],\\n\\n    # The path to your client ssl certificate (optional)\\n    \"ssl certificate\": \"./logstash-forwarder.crt\",\\n    # The path to your client ssl key (optional)\\n    \"ssl key\": \"./logstash-forwarder.key\",\\n\\n    # The path to your trusted ssl CA file. This is used\\n    # to authenticate your downstream server.\\n    \"ssl ca\": \"./logstash-forwarder.crt\",\\n\\n    # Network timeout in seconds. This is most important for\\n    # logstash-forwarder determining whether to stop waiting for an\\n    # acknowledgement from the downstream server. If an timeout is reached,\\n    # logstash-forwarder will assume the connection or server is bad and\\n    # will connect to a server chosen at random from the servers list.\\n    \"timeout\": 15\\n  },\\n\\n  # The list of files configurations\\n  \"files\": [\\n    # An array of hashes. Each hash tells what paths to watch and\\n    # what fields to annotate on events from those paths.\\n    {\\n      \"paths\": [ \\n        # single paths are fine\\n        \"/var/log/messages\",\\n        # globs are fine too, they will be periodically evaluated\\n        # to see if any new files match the wildcard.\\n        \"/var/log/*.log\"\\n      ],\\n\\n      # A dictionary of fields to annotate on each event.\\n      \"fields\": { \"type\": \"syslog\" }\\n    }, {\\n      # A path of \"-\" means stdin.\\n      \"paths\": [ \"-\" ],\\n      \"fields\": { \"type\": \"stdin\" }\\n    }, {\\n      \"paths\": [\\n        \"/var/log/apache/httpd-*.log\"\\n      ],\\n      \"fields\": { \"type\": \"apache\" }\\n    }\\n  ]\\n}\\n\\nGoals\\n\\nMinimize resource usage where possible (CPU, memory, network).\\nSecure transmission of logs.\\nConfigurable event data.\\nEasy to deploy with minimal moving parts.\\nSimple inputs only:\\n\\nFollows files and respects rename/truncation conditions.\\nAccepts STDIN, useful for things like varnishlog | logstash-forwarder....\\n\\n\\n\\nBuilding it\\n\\n\\nInstall go\\n\\n\\nCompile logstash-forwarder\\n git clone git://github.com/elasticsearch/logstash-forwarder.git\\n cd logstash-forwarder\\n go build\\n\\n\\n\\nPackaging it (optional)\\nYou can make native packages of logstash-forwarder.\\nTo build the packages, you will need ruby and fpm installed.\\ngem install fpm\\n\\nNow build an rpm:\\n    make rpm\\n\\nOr:\\n    make deb\\n\\nInstalling it (via packages only)\\nIf you don\\'t use rpm or deb make targets as above, you can skip this section.\\nPackages install to /opt/logstash-forwarder.\\nThere are no run-time dependencies.\\nRunning it\\nGenerally:\\nlogstash-forwarder -config logstash-forwarder.conf\\n\\nSee logstash-forwarder -help for all the flags\\nThe config file is documented further up in this file.\\nKey points\\n\\nYou\\'ll need an SSL CA to verify the server (host) with.\\nYou can specify custom fields for each set of paths in the config file. Any\\nnumber of these may be specified. I use them to set fields like type and\\nother custom attributes relevant to each log.\\n\\nGenerating an ssl certificate\\nLogstash supports all certificates, including self-signed certificates. To generate a certificate, you can run the following command:\\n$ openssl req -x509 -batch -nodes -newkey rsa:2048 -keyout logstash-forwarder.key -out logstash-forwarder.crt\\n\\nThis will generate a key at logstash-forwarder.key and the certificate at logstash-forwarder.crt. Both the server that is running logstash-forwarder as well as the logstash instances receiving logs will require these files on disk to verify the authenticity of messages.\\nRecommended file locations:\\n\\ncertificates: /etc/pki/tls/certs\\nkeys: /etc/pki/tls/private\\n\\nUse with logstash\\nIn logstash, you\\'ll want to use the lumberjack input, something like:\\ninput {\\n  lumberjack {\\n    # The port to listen on\\n    port => 12345\\n\\n    # The paths to your ssl cert and key\\n    ssl_certificate => \"path/to/ssl.crt\"\\n    ssl_key => \"path/to/ssl.key\"\\n\\n    # Set this to whatever you want.\\n    type => \"somelogs\"\\n  }\\n}\\n\\nImplementation details\\nBelow is valid as of 2012/09/19\\nMinimize resource usage\\n\\nSets small resource limits (memory, open files) on start up based on the\\nnumber of files being watched.\\nCPU: sleeps when there is nothing to do.\\nNetwork/CPU: sleeps if there is a network failure.\\nNetwork: uses zlib for compression.\\n\\nSecure transmission\\n\\nUses OpenSSL to verify the server certificates (so you know who you\\nare sending to).\\nUses OpenSSL to transport logs.\\n\\nConfigurable event data\\n\\nThe protocol supports sending a string:string map.\\n\\nEasy deployment\\n\\nThe make deb or make rpm commands will package everything into a\\nsingle DEB or RPM.\\n\\nFuture protocol discussion\\nI would love to not have a custom protocol, but nothing I\\'ve found implements\\nwhat I need, which is: encrypted, trusted, compressed, latency-resilient, and\\nreliable transport of events.\\n\\nRedis development refuses to accept encryption support, would likely reject\\ncompression as well.\\nZeroMQ lacks authentication, encryption, and compression.\\nThrift also lacks authentication, encryption, and compression, and also is an\\nRPC framework, not a streaming system.\\nWebsockets don\\'t do authentication or compression, but support encrypted\\nchannels with SSL. Websockets also require XORing the entire payload of all\\nmessages - wasted energy.\\nSPDY is still changing too frequently and is also RPC. Streaming requires\\ncustom framing.\\nHTTP is RPC and very high overhead for small events (uncompressable headers,\\netc). Streaming requires custom framing.\\n\\nLicense\\nSee LICENSE file.\\n\\n\\n',\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': 'No readme file.', 'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\ntutum-docker-mysql\\nBase docker image to run a MySQL database server\\nMySQL version\\nmaster branch maintains MySQL from Ubuntu trusty official source. If you want to get different version of MySQL, please checkout 5.5 branch and 5.6 branch.\\nIf you want to use MariaDB, please check our tutum/mariadb image: https://github.com/tutumcloud/tutum-docker-mariadb\\nUsage\\nTo create the image tutum/mysql, execute the following command on the tutum-mysql folder:\\ndocker build -t tutum/mysql .\\n\\nTo run the image and bind to port 3306:\\ndocker run -d -p 3306:3306 tutum/mysql\\n\\nThe first time that you run your container, a new user admin with all privileges\\nwill be created in MySQL with a random password. To get the password, check the logs\\nof the container by running:\\ndocker logs <CONTAINER_ID>\\n\\nYou will see an output like the following:\\n========================================================================\\nYou can now connect to this MySQL Server using:\\n\\n    mysql -uadmin -p47nnf4FweaKu -h<host> -P<port>\\n\\nPlease remember to change the above password as soon as possible!\\nMySQL user \\'root\\' has no password but only allows local connections\\n========================================================================\\n\\nIn this case, 47nnf4FweaKu is the password allocated to the admin user.\\nRemember that the root user has no password but it\\'s only accesible from within the container.\\nYou can now test your deployment:\\nmysql -uadmin -p\\n\\nDone!\\nSetting a specific password for the admin account\\nIf you want to use a preset password instead of a random generated one, you can\\nset the environment variable MYSQL_PASS to your specific password when running the container:\\ndocker run -d -p 3306:3306 -e MYSQL_PASS=\"mypass\" tutum/mysql\\n\\nYou can now test your deployment:\\nmysql -uadmin -p\"mypass\"\\n\\nMounting the database file volume\\nIn order to persist the database data, you can mount a local folder from the host\\non the container to store the database files. To do so:\\ndocker run -d -v /path/in/host:/var/lib/mysql tutum/mysql /bin/bash -c \"/usr/bin/mysql_install_db\"\\n\\nThis will mount the local folder /path/in/host inside the docker in /var/lib/mysql (where MySQL will store the database files by default). mysql_install_db creates the initial database structure.\\nRemember that this will mean that your host must have /path/in/host available when you run your docker image!\\nAfter this you can start your mysql image but this time using /path/in/host as the database folder:\\ndocker run -d -p 3306:3306 -v /path/in/host:/var/lib/mysql tutum/mysql\\n\\nMounting the database file volume from other containers\\nAnother way to persist the database data is to store database files in another container.\\nTo do so, first create a container that holds database files:\\ndocker run -d -v /var/lib/mysql --name db_vol -p 22:22 tutum/ubuntu-trusty \\n\\nThis will create a new ssh-enabled container and use its folder /var/lib/mysql to store MySQL database files.\\nYou can specify any name of the container by using --name option, which will be used in next step.\\nAfter this you can start your MySQL image using volumes in the container created above (put the name of container in --volumes-from)\\ndocker run -d --volumes-from db_vol -p 3306:3306 tutum/mysql \\n\\nMigrating an existing MySQL Server\\nIn order to migrate your current MySQL server, perform the following commands from your current server:\\nTo dump your databases structure:\\nmysqldump -u<user> -p --opt -d -B <database name(s)> > /tmp/dbserver_schema.sql\\n\\nTo dump your database data:\\nmysqldump -u<user> -p --quick --single-transaction -t -n -B <database name(s)> > /tmp/dbserver_data.sql\\n\\nTo import a SQL backup which is stored for example in the folder /tmp in the host, run the following:\\nsudo docker run -d -v /tmp:/tmp tutum/mysql /bin/bash -c \"/import_sql.sh <rootpassword> /tmp/<dump.sql>\")\\n\\nWhere <rootpassword> is the root password set earlier and <dump.sql> is the name of the SQL file to be imported.\\nEnvironment variables\\nMYSQL_PASS: Set a specific password for the admin account.\\nCompatibliity Issues\\n\\nVolume created by MySQL 5.6 cannot be used in MySQL 5.5 Images or MariaDB images\\n\\n\\n\\n',\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nEmissions Scrapper\\nThis software will scrap \"Air Emission Event Report Database\" link\\nInstallation\\nTo use it you will need ruby 2.1.0 and mongodb.\\ngit clone [link]\\ncd emissions_scrapper\\ncp config/mongoid.yml.example config/mongoid.yml\\nmkdir tmp\\nbundle install\\nAnd you are good to go!\\nUsage\\nTo retrieve emission events just run:\\nbin/scrapper download [from] [to]\\n\\nfrom is the initial tracking number and YES! you guessed, to is the final one!\\nTo scrap the info and stored into the database just:\\nbin/scrapper scrap\\n\\nAnd finally if you need a CSV file:\\nbin/scrapper export [short|full]\\n\\nDevelopment\\nJust add cool stuff.\\n\\n\\n',\n",
       "  'language': 'Ruby'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nxscrolly.js\\nWhen X scrolls in, do Y\\nThis was created because we have a general desire to do stuff depending on the\\nscroll position, and only found specific libraries for doing specific things.\\nThis library does nothing on its own. You have to give it a set of targets, and\\na event callbacks for what to do when those targets scroll past.\\nIf you have a library that already works, use it. If you have a library that\\ndoesn't work or you find yourself constantly fighting... you might want to\\nconsider adapting it to use this underneath so you can write your own code.\\nOptions\\n// setup options: these can only be set once\\ncontainer: window,     // selector for scroll container, should also be an offset parent\\ntargets: 'section',    // selector for the targets\\n\\n// configuration options\\nupdateOffsets: 0,      // force script to re-calculate offsets:\\n                       //   0 (default)   calculate only the first time\\n                       //   1             re-calculate after `unveil`\\n                       //   2             re-calculate after `change`\\n                       //   3             re-calculate every scroll\\noffset: 0,             // pixels from the top of the page to set the origin\\nthrottle: 200          // milliseconds to de-bounce the scroll event\\nEvents\\nTo actually do something, you have to set up at least one event callback.\\nEvent callbacks have the general function prototype:\\nfunction($el)\\n\\nWhere $el is the object(s) immediately above the the origin line (typically\\nthe top of the screen), and this is the instance of XScrollY. Note that $el\\nis not the same as this.$active.\\nEvents are fired in this order:\\n\\nunveil\\nchange\\nscroll\\nstart\\n\\nunveil\\nScroll has changed focus from one element to another for the first time.\\nExample (demo):\\n// mark target as read\\nfunction($el) {\\n  $el.addClass('read');\\n}\\nchange\\nScroll has changed focus from one element to another. Example (demo):\\n// mark target as active\\nfunction($el) {\\n  this.$active.removeClass('active');\\n  $el.addClass('active');\\n}\\nscroll\\nCaptured a scroll event. This isn't that useful, but if you want a de-bounced\\nscroll event listener, you can use this. Example (demo):\\n// decorate all visible images\\nfunction($el) {\\n  this.visible($('img')).css('border', '1px solid #' + Math.floor(Math.random() * 16777215).toString(16));\\n}\\nstart\\nLike an unveil for unveil. This is run the very first time. Example\\n(demo):\\n// outline all elements on the first screen\\nfunction($el) {\\n  this.visible($('*')).css('outline', '1px solid chartreuse');\\n}\\nProperties\\n\\n$active: The currently active element(s)\\n\\nMethods\\nThese are methods you can use on this inside the event callbacks:\\n\\nvisible()  Get the targets visible on screen\\nabove()    Get the targets above the top of the screen\\naboves()   Get the targets above the bottom of the screen\\nbelow()    Get the targets below the top of the screen\\nbelows()   Get the targets below the bottom of the screen\\n\\nArguments:\\n\\noffset    Fudge the origin this many pixels down, stacks with the global\\noffset option.\\nbleed     Fudge the boundaries outward this many pixels.\\n$targets  Don't use the internal targets, define a new set.\\n\\nUsage:\\nvisible();\\nvisible(offset);\\nvisible(offset, bleed);\\nvisible(offset, bleed, $targets);\\nvisible($targets);\\njquery-xscrolly.js\\nThis can also be used as a jQuery plugin:\\n$('section').xscrolly(options);\\n\\nAnd the instance can be found on the xscrolly jQuery data:\\nvar xsy = $('section').data('xscrolly');\\n\\nLive Demos\\n\\nexamples/basic.html\\nexamples/fixed.html\\nexamples/lazyimg.html\\nexamples/offset.html\\nexamples/rows.html\\nexamples/spy.html\\n\\nDeveloping\\nxscrolly uses grunt to build. For some reason, the qunit tests may not work\\nfrom the command line. If that happens, you can bypass it with: grunt build.\\nSimilar libraries\\nThere's a similar jquery plugin called Waypoints. The major\\ndifference is that xscrolly will always mark a target as active. For example,\\non page load, the top target in xscrolly will be active, but won't be active\\nin Waypoints until you scroll to it.\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': 'No readme file.', 'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nDownload this gist. Extract it. Then run these commands from the command line\\nwhere you extracted it:\\n# make a temporary virtualenv, requires virtualenvwrapper\\nmktmpenv\\n# go back to the previous directory\\ncd -\\n# setup\\npip install https://github.com/texastribune/django-gistpage/tarball/master\\ncurl http://www.texastribune.org/test/gistpage/ -o index.html\\npython -m DjangoGistServer 8000\\n\\nNow open http://localhost:8000 in your browser.\\n\\n\\n',\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nDjango-GistPage\\nDisclaimer: This is pre-alpha code. Not all the claims made in in this\\ndocument are implemented.\\nThis package provides two components:\\n\\nDjangoGistServer: A way to develop a simple static site that uses the\\nfull Django templating engine, including extending some Django project\\'s base\\ntemplates.\\nGistPage: A way to take a page developed using DjangoGistServer that\\'s\\nbeen published as a GitHub gist and serve it at a url in some Django project.\\n\\nYour static page should be in a directory that looks like this:\\nindex.html\\napp.css\\napp.js\\n\\nIf you use a template base that supports DjangoGistServer, you don\\'t have to\\nworry about including css and js. If your base doesn\\'t support DjangoGistServer,\\nadd these two lines somewhere inside index.html:\\n<link href=\"app.css\" rel=\"stylesheet\" type=\"text/css\">\\n<script type=\"text/javascript\" src=\"app.js\"></script>\\n\\nMagic Warning: If you have multiple css and js files, they will be\\nconcatenated into app.css and app.js. You may have guessed at this point that it\\ndoesn\\'t matter what your files are named. TODO add docs on how to deal with\\nincluding third-party JS/CSS.\\nStatic pages are published as gists. Which are great because you get history and\\ncan package multiple files together. A sample gist is available at:\\n\\nhttps://gist.github.com/crccheck/93a8a020f6789d373a6c\\n\\nDjangoGistServer\\nSample Usage:\\nUsing virtualenvwrapper and vi as the editor:\\n# do development inside a virtualenv because this uses a lot of libraries\\nmkvirtualenv gistme\\npip install django-gistpage\\nmkdir my_awesome_page\\ncd my_awesome_age\\nvi index.html\\nvi app.js\\nvi app.css\\npython -m DjangoGistServer 8000 --template-dir=/a/b/c/templates/\\n# your directory is now being served on port 8000\\n\\n\\nTurning your static page into a gist:\\nThere are several command line tools you can install do do this:\\n\\nTODO\\n\\nYou can also create a gist from GitHub, clone it locally, and push it back up.\\n\\nGistPage\\nTo serve that static site in your production Django site, you would create a new\\nGistPage object in the admin, just like a flatpage. Point it to the gist\\nversion of the static page, and that\\'s it.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nTesting harness for story.  To run, create a virtualenv, then:\\npip install -r requirements.txt\\ngunicorn scaffold:app\\n\\nNow load your app in: localhost:8000.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        Readme.md\\n      \\n\\n\\nCustom Icon Font for The Texas Tribune\\nSee demo site for a preview of the font.\\nIcoMoon Instructions\\n\\nhttp://icomoon.io/app/\\nLoad our project: use project/texastribune-ico.json\\ndo your thing\\ndownload the font and unzip it and replace ./dist\\nsave project back as texastribune-ico.json\\ncommit changes\\n\\nUsage Instructions\\n\\nIncorporate style.css\\nUse css class, not data-icon. The characters representing each icon are\\nnot final.\\n\\nChangelog\\n\\nVersion 0.6:\\n\\nupdate for new icomoon app (http://icomoon.io/#post/350)\\nadded livestream icon, tt-livestream\\nupdated -reverse/-sign class names to be more consistent\\ndeleted duplicate icons (twitter and twitter-bird)\\n\\n\\nVersion 0.5:\\n\\nadded tt logo bug\\n\\n\\nVersion 0.4:\\n\\nadded flexslider nav\\n\\n\\nVersion 0.3:\\n\\nadd share-alt\\nfont-awesome-ize the icons\\n\\n\\nVersion 0.2:\\n\\nadded a copy of Best and Worst game (game uses another font icon)\\nadded some other icons that we currently use on the site\\n\\n\\nVersion 0.1: Festival\\n\\n\\n\\n',\n",
       "  'language': 'CSS'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ngspreadsheet\\nA wrapper around a wrapper to get Google spreadsheets to look like\\ncsv.DictReader.\\nIf you\\'re used to working with CSVs or a human, you\\'ll find that working with\\nGoogle\\'s Python API for spreadsheets is so frustrating. With gspreadsheet,\\nyou can adapt your existing csv code to work with Google Spreadsheets with just\\ntwo line changes. As an added bonus, if you alter the dict, those changes get\\nsaved back to the original spreadsheet.\\n\\nInstallation\\npip install gspreadsheet\\n\\n\\nUsage\\nIf your old CSV code looked like this:\\nfrom csv import DictReader\\nreader = DictReader(open(\\'myspreadsheet.csv\\'))\\nfor row in reader:\\n    process(row)\\n\\nIt would look like this with gspreadsheet:\\nfrom gspreadsheet import GSpreadsheet\\nreader = GSpreadsheet(\"https://docs.google.com/myspreadsheet\")\\nfor row in reader:\\n    process(row)\\n\\nSo looking at more examples...\\nGet a spreadsheet if you know the key and worksheet:\\nsheet = GSpreadsheet(key=\\'tuTazWC8sZ_r0cddKj8qfFg\\', worksheet=\"od6\")\\n\\nGet a spreadsheet if you just know the url:\\nsheet = GSpreadsheet(url=\"https://docs.google.com/spreadsheet/\"\\n                         \"ccc?key=0AqSs84LBQ21-dFZfblMwUlBPOVpFSmpLd3FGVmFtRVE\")\\n\\nSince just knowing the url is the most common use case, specifying it as a kwarg\\nis optional. Just pass whatever url is in your browser as the first argument.:\\nsheet = GSpreadsheet(\"https://docs.google.com/spreadsheet/\"\\n                     \"ccc?key=0AqSs84LBQ21-dFZfblMwUlBPOVpFSmpLd3FGVmFtRVE\")\\n\\nGet the JSON representation of the spreadsheet:\\nsheet.to_JSON()\\n\\n\\nAuthenticating\\nGet a spreadsheet as a certain user:\\nsheet = GSpreadsheet(email=\"foo@example.com\", password=\"12345\",\\n                     key=\\'tuTazWC8sZ_r0cddKj8qfFg\\', worksheet=\"od6\")\\n\\nYou can also specify the email and password using environment variables:\\nGOOGLE_ACCOUNT_EMAIL and GOOGLE_ACCOUNT_PASSWORD.\\nAnd as an authenticated user, you can modify the spreadsheet.:\\nfor row in sheet:\\n    print row\\n    if row[\\'deleteme\\']:\\n        row.delete()  # delete the row from the worksheet\\n        continue\\n    row[\\'hash\\'] = md5(row[\\'name\\']).hexdigest()  # compute the hash and save it back\\n\\ndata = row.copy()   # get the last row as a plain dict\\nsheet.add_row(data)  # copy the last row and append it back to the sheet\\n\\n\\nAdvanced Usage: Saving data back to the spreadsheet\\nIf you modify the dict that represents a row, those changes will get pushed back\\nto the spreadsheet:\\n>>> row[\\'value\\']\\n\\'foo\\'\\n>>> row[\\'value\\'] = \\'bar\\'  # Change this value\\n>>> row[\\'value\\']\\n\\'bar\\'\\n\\n\\nAdvanced Usage: Deferring Saves\\nIf you do multiple changes to a row, the script can get very slow because it has\\nto make a syncronous request back to the server with every change. To avoid\\nthis, you can turn on deferred saves by setting deferred_save=True when\\ninstantiating a GSpreadsheet. Just remember to .save():\\nsheet = GSpreadsheet(email=\"foo@example.com\", password=\"12345\",\\n                     key=\\'tuTazWC8sZ_r0cddKj8qfFg\\', worksheet=\"od6\",\\n                     deferred_save=True)\\n\\nrow = sheet.next()\\nfor key in row.keys():\\n    row[\\'key\\'] = \\'\\'\\nrow.save()\\n\\n\\nScary Warnings\\nI really want to say this is alpha software, but we\\'ve been using bits and\\npieces of this for over a year now. Everything is subject to change, even the\\nnames. This also relies on google\\'s relatively ancient gdata package, which does\\nnot have support for Python 3.\\n\\nChangelog\\n\\nv0.4.0 - Added .to_JSON method. Added tox coverage.\\n\\n\\nSimilar Python packages\\n\\ngspread\\n\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\ndjango-kawarimi\\n\\n\\n\\n',\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': 'No readme file.', 'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ntt_dataviews\\nDjango class-based views (CBVs) for Texas Tribune's data applications.\\nNote, this is not a complete solution for building data applications like the\\nTexas Tribune.  Instead, it's opened as an insight into how we structure part\\nof our applications.  It's meant to be a learning tool, not used directly.\\nThat said, if you're interested, feel free to dive in.\\n\\nConventions\\n\\nStatic Assets\\nTODO\\n\\nLanding Pages\\nTODO\\n\\nDetail Pages\\nTODO\\n\\nDeveloping\\nYou need Grunt installed to develop the static assets associated with this\\n(see Grunt's Getting Started guide for more information on setting up Grunt).\\nOnce you have Grunt installed, you need to install the development packages via\\nnpm like this:\\nnpm install .\\n\\nOnce everything is installed, you can build the Sass with this command:\\ngrunt sass\\n\\nThere is also a convenience method for continually building the Sass files via\\nthe watch command like this:\\ngrunt watch\\n\\n\\n\\n\",\n",
       "  'language': 'CSS'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ntt_dailyemailblast\\nSimple application for handling our daily emails to partners.\\n\\nNote\\nThis could be a lot more complex and do all manner of\\nawesome stuff like auto-generating the body based on stories that\\nare in the CMS.  That\\'s so much bigger than we need right now.\\nThis is definitely in the \"works for what we need\" camp, feel free\\nto use it as an example of sending emails out using Celery, but\\nprobably not something to actually use.\\n\\n\\nInstallation & Configuration\\nYou can install this using pip like this:\\npip install tt_dailyemailblast\\n\\nOnce installed, you need to add it to your INSTALLED_APPS.  You can do that\\nhowever you like or you can copy-and-paste this in after your\\nINSTALLED_APPS are defined:\\nINSTALLED_APPS += [\\'tt_dailyemailblast\\', ]\\n\\nThe next setting to configure is the from email address:\\nTT_DAILYEMAILBLAST_FROMEMAIL = \\'no-reply@texastribune.org\\'\\n\\ntt_dailyemailblast can accept any backend to suit your integration needs.\\nTo send an email, you\\'ll need a template for the body.\\ntt_dailyemailblast will look for templates in your project in this order:\\n\\ntt_dailyemailblast/<blast_type>/<recipient_list>/<recipient>.html\\ntt_dailyemailblast/<blast_type>/<recipient_list>.html\\ntt_dailyemailblast/<blast_type>.html\\n\\nIf you need special context generated in that template, you can specify that\\nwith the context backend setting:\\nTT_DAILYEMAILBLAST_CONTEXT = \\'tt.dailyemailblast.context_backend\\'\\n\\nFinally, to actually send emails, tt_dailyemailblast will use the sync\\nbackend by default. You can explicitly specify that with the setting:\\nfrom tt_dailyemailblast.settings.sync import *\\n\\nTo use async Celery workers to send, add:\\nfrom tt_dailyemailblast.settings.async import *\\n\\n\\nUsage\\n\\nTemplates\\nYour templates should render to html. The context will get these variables:\\n\\n{{ blast }} - The DailyEmailBlast object.\\n{{ recipient }} - The Recipient object.\\n{{ recipient_list }} - The RecipientList object.\\n\\n\\nContext Backend\\nYour context backend should take the arguments:\\n\\nblast\\nrecipient\\nrecipient_list\\n\\nAnd should be patterened after this basic example:\\nfrom tt_dailyemailblast.context_backends import basic\\n\\ndef context_backend(blast, recipient, recipient_list):\\n    context = basic(blast, recipient, recipient_list)\\n    # modify context\\n    return context\\n\\n\\nTesting\\nTesting is so easy! A one-armed driving bear could do it! Just follow these\\nsteps:\\n\\nCreate a recipient list with only your email address\\nCreate a blast\\nMake sure the blast has a template that will render\\nFill in the Sent on and Sent completed on fields even though the\\nblast has not been sent yet\\nSave the blast\\nInstall pops\\nSend the blast from the \\'Send Now!\\' button\\n\\n\\nExample\\nAll of the usage is outlined, along with tests inside the example\\ndirectory.  See that directory for more information on how to run the tests and\\nexample project.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\ntwhp-interactive\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        Readme.md\\n      \\n\\n\\nSimpleModal - A modal dialog framework for jQuery\\n\\nDocumentation\\nDemos\\nDownloads\\n\\nQuestions?\\nFor questions, please post them on stackoverflow and tag the question with \"simplemodal\".\\nFor issues or feature requests, please enter them on the issues page.\\nTwitter\\n\\n@ericmmartin\\n@simplemodal\\n\\nDonate\\nDonations are greatly appreciated.\\nBuilding SimpleModal\\n\\nMake sure that you have Java and Ant installed.\\n\\nIn the main directory of the distribution (the one that this file is in), type the following to make all versions of SimpleModal:\\nant\\n\\nThe standard, uncompressed, SimpleModal code.\\nMakes: ./dist/jquery.simplemodal.VERSION.js\\nant full\\n\\nFinally, you can remove all the built files using the command:\\nant clean\\n\\n\\n\\n',\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nHorseradish\\nIt burns, but not for long.\\nHorseradish is an image library for managing photographic assets and related metadata:\\n\\nvarious image sizes\\nalt text and captions\\ncredit and source information\\nnotes and tags\\n\\nSettings\\nThe following environment variables are required:\\n\\nDEBUG\\nDATABASE_URL\\nSECRET_KEY\\nAWS_KEY\\nAWS_SECRET\\nAWS_BUCKET\\nAWS_LOCATION (optional)\\nELASTICSEARCH_URL\\nELASTICSEARCH_INDEX\\nGOOGLEAUTH_DOMAIN\\nGOOGLEAUTH_REALM\\nSTATIC_ROOT\\nRAVEN_DSN\\n\\n\\n\\n',\n",
       "  'language': 'CSS'},\n",
       " {'readme': 'No readme file.', 'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nTexas Tribune App Templates\\nThis repository contains the Texas Tribune Django Application templates.\\n\\nUsage\\nClone this repository, then use django-admin.py startapp like this:\\ndjango-admin.py startapp \\\\\\n  --template=/path/to/tt_app_templates/{{ template }} \\\\\\n  -e py,rst,in,coffee,json \\\\\\n  new_awesome_app\\n\\nThe --template option tells Django where to find the base template for\\ncreating the new application.  The -e options tell Django to render the\\n.py, .rst, .in, .coffee, and .json files via Django's\\ntemplate language.\\n\\nProvided Templates\\nCurrently only one template is provided.  Additional templates will be\\ndocumented here as they are added.\\n\\ngeneric\\nThis is the general purpose application structure that all generic Django\\napplications created at the Tribune use.\\n\\n\\nContributing\\nThis project is released as open-source code in hopes that it might help people\\nunderstand how to build out their own Django application templates, but it is\\nmeant for use inside the Texas Tribune.  Please feel free to open a pull request\\nif you have any suggestions.\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nkids-count-viz\\n\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nconstitutional-amendment-election-2013\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nDjrill: Mandrill Transactional Email for Django\\nDjrill integrates the Mandrill transactional\\nemail service into Django.\\nIn general, Djrill \"just works\" with Django\\'s built-in django.core.mail\\npackage. It includes:\\n\\nSupport for HTML, attachments, extra headers, and other features of\\nDjango\\'s built-in email\\nMandrill-specific extensions like tags, metadata, tracking, and MailChimp templates\\nOptional support for Mandrill inbound email and other webhook notifications,\\nvia Django signals\\nAn optional Django admin interface\\n\\nDjrill is released under the BSD license. It is tested against Django 1.3, 1.4, and 1.5\\n(including Python 3 support with Django 1.5).\\n\\nResources\\n\\nFull documentation: https://djrill.readthedocs.org/en/master/\\nPackage on PyPI: https://pypi.python.org/pypi/djrill\\nProject on Github: https://github.com/brack3t/Djrill\\n\\n\\nDjrill 1-2-3\\n\\nInstall Djrill from PyPI:\\n$ pip install djrill\\n\\nEdit your project\\'s settings.py:\\nINSTALLED_APPS = (\\n    ...\\n    \"djrill\"\\n)\\n\\nMANDRILL_API_KEY = \"<your Mandrill key>\"\\nEMAIL_BACKEND = \"djrill.mail.backends.djrill.DjrillBackend\"\\n\\nNow the regular Django email functions\\nwill send through Mandrill:\\nfrom django.core.mail import send_mail\\n\\nsend_mail(\"It works!\", \"This will get sent through Mandrill\",\\n    \"Djrill Sender <djrill@example.com>\", [\"to@example.com\"])\\nYou could send an HTML message, complete with custom Mandrill tags and metadata:\\nfrom django.core.mail import EmailMultiAlternatives\\n\\nmsg = EmailMultiAlternatives(\\n    subject=\"Djrill Message\",\\n    body=\"This is the text email body\",\\n    from_email=\"Djrill Sender <djrill@example.com>\",\\n    to=[\"Recipient One <someone@example.com>\", \"another.person@example.com\"],\\n    headers={\\'Reply-To\\': \"Service <support@example.com>\"} # optional extra headers\\n)\\nmsg.attach_alternative(\"<p>This is the HTML email body</p>\", \"text/html\")\\n\\n# Optional Mandrill-specific extensions:\\nmsg.tags = [\"one tag\", \"two tag\", \"red tag\", \"blue tag\"]\\nmsg.metadata = {\\'user_id\\': \"8675309\"}\\n\\n# Send it:\\nmsg.send()\\n(Be sure to use a from_email that\\'s in one of your Mandrill approved sending\\ndomains, or the message won\\'t get sent.)\\n\\n\\nSee the full documentation\\nfor more features and options.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nDjango Hstore Schema\\nProblems solved\\nProvides a generic model for storing pk-key-values and a way to build convenient schemas on top of them for querying.\\nSolves these problems and more:\\n\\n\\nAdd schema mappings as you need the data\\nSCHEMA(raw_source, raw_version, raw_field)\\n=> VIEW(mapped_source, mapped_version, mapped_field)\\n\\n\\nData can be mapped to create secondary data:\\nMAP(AEIS, 2010, students_by_program_special_education_2009, raw_value)\\n=> DATA(AEIS, 2009, students_by_program_special_education, raw_value)\\n\\n\\nThe same data point is provided from multiple (possibly conflicting) sources\\nMAP(FAST, any_version, average_teacher_salary_district, raw_value)\\n=> DATA(FAST, raw_version, average_teacher_salary, raw_value)\\nMAP(FAST, any_version, average_teacher_salary_campus, raw_value)\\n=> DATA(FAST, raw_version, average_teacher_salary, raw_value)\\nREDUCE(FAST, any_version, average_tacher_salary, *raw_values)\\n=> DATA(FAST, raw_version, average_tacher_salary, coalesce(*raw_values))\\n\\n\\nData is formulated differently for different versions\\nMAP(AEIS, 2011, students_asian, raw_value)\\n=> DATA(AEIS, 2011, students_asian_pacific_islander, raw_value)\\nMAP(AEIS, 2011, students_pacific_islander, raw_value)\\n=> DATA(AEIS, 2011, students_asian_pacific_islander, raw_value)\\nREDUCE(AEIS, 2011, students_asian_pacific_islander, *raw_values)\\n=> DATA(AEIS, raw_version, students_asian_pacific_islander, sum(*raw_values))\\n\\n\\nWorkflow\\n\\nLoad primary data\\nMapReduce secondary data\\nCreate schema from data\\nProfit\\n\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\ninfinite-proteus\\nAn experiment in using any textarea editor.\\nThis is a JavaScript framework for giving users control of what editor they\\nprefer using.\\nUsage\\nInclude the script:\\n<script src=\"infinite-proteus.js\"></script>\\n\\nAdd editors:\\ninfiniteProteus.addEditor(editor);\\n\\nWhere an editor is an Object with these options:\\nname        : String\\n              The name of the editor, e.g.: TinyMCE, wysihtml5, CodeMirror\\nbutton      : String (optional)\\n              The name of the editor to use for the UI. You might want to\\n              use \\'MD\\' for the MarkDown editor,\\n              Dillinger. *HTML is allowed.*\\nisInstalled : bool Function (optional if `css` exists)\\n              A function that returns a boolean that determines if the\\n              editor is installed or not.\\ncss         : Array (optional)\\n              An array of additional CSS that needs to be loaded.\\njs          : Array (optional)\\n              An array of additional JavaScript that needs to be loaded.\\ninit        : void Function (optional)\\n              A function that should be run immediately.\\nenable      : void Function\\n                @param element  The DOM Node for the source textarea element\\n              The code that\\'s needed to turn a textarea into the editor. For\\n              TinyMCE, this would look like:\\n                  tinyMCE.execCommand(\\'mceAddControl\\', false, element.id);\\ndisable     : void Function\\n                @param element  The DOM Node for the source textarea element\\n              The code that\\'s needed to revert back into a textarea. For\\n              TinyMCE, this would look like:\\n                  tinyMCE.execCommand(\\'mceRemoveControl\\', false, element.id);\\n\\nInitialize the script:\\n<script>\\n  infiniteProteus.init(options);\\n</script>\\n\\nOptions:\\ntextareas : String (default: \\'textarea\\')\\n            jquery selector that identifies which textareas to use, e.g.:\\n            \\'textarea[data-use-proteus=1]\\'\\nremember  : Boolean (default: true)\\n            Remember the user\\'s editor preferences. Requires HTML5\\n            localstorage. Users\\' preferences override the default editor.\\n            *semi-documented:* If you specify a function, this function is\\n            used to determine how preferences are saved.\\n\\nMethods:\\n\\naddEditor : Add an editor, see above.\\ninit      : Start the script.\\nforget    : Forget user preferences.\\ndestroy   : Stop the script, remove all editors.\\n\\nMeaningful data attributes on the <textarea> element:\\ndata-editor : name of the editor that should be used on the textarea\\n\\njQuery data created on each textarea:\\nfiniteEruptions : reference to the active editor\\n\\nTips\\nIf you need to hold onto a variable between an editor\\'s enable and its\\ndisable, you\\'re best bet is to store it on the element itself as jQuery data.\\nFor an example, see the codemirror editor.\\nAuto-selecting an editor\\nIf the data-editor attribute and the editor\\'s name happen to match, that\\neditor\\'s .enable() method is run immediately, unless user preferences are\\nenabled and the user prefers another editor.\\nRemembering preferences\\nThis script will remember the last editor used (including deciding not to an\\neditor) based on the textarea\\'s id and the editor\\'s name.\\nTo clear preferences, hook up the infiniteProteus.forget() method.\\nWidget\\nA widget will be created and placed near the textarea with buttons to switch\\nbetween editors.\\nSample Editors\\n\\nMarkdown to HTML\\n\\nhttps://github.com/coreyti/showdown port of the original\\nvar converter = new Showdown.converter();\\nconverter.makeHTML(markdownText);\\n\\n\\nHTML To Markdown\\n\\nhttps://github.com/domchristie/to-markdown last updated 7 months ago\\nvar md = toMarkdown(htmlText);\\n\\nhttps://github.com/kates/html2markdown last updated 10 months ago\\nvar md = HTML2Markdown(htmlText);\\n\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ntt_streams\\nProvides a means of creating streams of content\\n\\nUsage\\nYou can use the tt_streams to collect different models from different apps into\\na unified stream of models.  Streams are made up of a series of StreamItem\\nsubclasses that you write that are specific to each app that they are used in.\\nTo skip straight ahead to simple (bordering on naive) implementation examples,\\ncheck out the project/example/models.py file.\\nEach Django app that wants to expose its models to streams should do two things:\\n\\nCreate a specific *Item model.  For example, StoryItem should subclass\\nthe StreamItem model and provide a connection via a ForeignKey to the\\nmodel that is opting in,``Story`` in this case.\\nCreate a way to get associate those models with a Stream.  This is most\\ncommonly done via an inline interface to the StreamItem sub-class.\\n\\nBeyond these requirements, everything else you do with your subclass of\\nStreamItem is up to you.  It\\'s common to use your StreamItem class as a\\ncache table so you don\\'t have to send another query to the database.\\nFor example, the StoryItem automatically stores the title of its related\\nStory model when it is saved, and a receiver is hooked up to the post_save\\nsignal for Story to ensure that all of its stream_items are resaved each\\ntime it is saved.\\nThere are a couple of things to note:\\n\\nYou can name your subclass of StreamItem whatever you would like\\nYou can name your related field whatever you like\\nIt automatically has a pub_date that you can use for ordering, but they\\nare not ordered by default\\n\\n\\nInstallation & Configuration\\nYou can install the latest release of tt_streams using pip:\\npip install tt_streams\\n\\nMake sure to add tt_streams to your INSTALLED_APPS.  You\\ncan add this however you like.  This works as a copy-and-paste solution:\\nINSTALLED_APPS += [\"tt_streams\", ]\\n\\nOnce installed, you have to run either syncdb, or migrate if you are\\nusing South.\\n\\nContributing\\n\\nCreate something awesome -- make the code better, add some functionality,\\nwhatever (this is the hardest part).\\nFork it\\nCreate a topic branch to house your changes\\nGet all of your commits in the new topic branch\\nSubmit a pull request\\n\\n\\nLicense\\nCopyright 2013 Texas Tribune\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': 'No readme file.', 'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ntt_hospitals\\nThis Django application was generated using the Texas Tribune's Generic\\nDjango app template.\\n\\nInstallation & Configuration\\nYou can install this using pip like this:\\npip install tt_hospitals\\n\\nOnce installed, you need to add it to your INSTALLED_APPS.  You can do that\\nhowever you like or you can copy-and-paste this in after your\\nINSTALLED_APPS are defined.\\nINSTALLED_APPS += ['tt_hospitals', ]\\n\\nNow you're ready to start using tt_hospitals.\\n\\nUsage\\nTODO\\n\\nExample\\nAll of the usage is outlined, along with tests inside the example\\ndirectory.  See that directory for more information on how to run the tests and\\nexample project.\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\npgload\\n\\n\\n',\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nHaystack\\n\\n\\nAuthor:\\nDaniel Lindsley\\nDate:\\n2013/04/28\\n\\n\\nHaystack provides modular search for Django. It features a unified, familiar\\nAPI that allows you to plug in different search backends (such as Solr,\\nElasticsearch, Whoosh, Xapian, etc.) without having to modify your code.\\nHaystack is BSD licensed, plays nicely with third-party app without needing to\\nmodify the source and supports advanced features like faceting, More Like This,\\nhighlighting, spatial search and spelling suggestions.\\nYou can find more information at http://haystacksearch.org/.\\n\\nGetting Help\\nThere is a mailing list (http://groups.google.com/group/django-haystack/)\\navailable for general discussion and an IRC channel (#haystack on\\nirc.freenode.net).\\n\\nDocumentation\\n\\nDevelopment version: http://docs.haystacksearch.org/\\nv1.2.X: http://django-haystack.readthedocs.org/en/v1.2.6/\\nv1.1.X: http://django-haystack.readthedocs.org/en/v1.1/\\n\\n\\nRequirements\\nHaystack has a relatively easily-met set of requirements.\\n\\nPython 2.5+\\nDjango 1.3+\\n\\nAdditionally, each backend has its own requirements. You should refer to\\nhttp://docs.haystacksearch.org/dev/installing_search_engines.html for more\\ndetails.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\npyelasticsearch\\nA Python client for elasticsearch.\\n\\nUsage\\npyelasticsearch handles the low-level interactions with elasticsearch,\\nallowing you to use native Python datatypes to index or perform queries.\\nExample:\\nconn = ElasticSearch(\\'http://localhost:9200/\\')\\n\\n# Index some documents.\\nconn.index({\"name\":\"Joe Tester\", \"age\": 25, \"title\": \"QA Master\"}, \"contacts\", \"person\", 1)\\nconn.index({\"name\":\"Jessica Coder\", \"age\": 32, \"title\": \"Programmer\"}, \"contacts\", \"person\", 2)\\nconn.index({\"name\":\"Freddy Tester\", \"age\": 29, \"title\": \"Office Assistant\"}, \"contacts\", \"person\", 3)\\n\\n# Refresh the index to pick up the latest documents.\\nconn.refresh([\"contacts\"])\\n\\n# Get just Jessica\\'s document.\\njessica = conn.get(\"contacts\", \"person\", 2)\\n\\n# Perform a simple search.\\nresults = conn.search(\"name:joe OR name:freddy\")\\n\\n# Perform a search using the elasticsearch Query DSL (http://www.elasticsearch.org/guide/reference/query-dsl)\\nquery = {\\n    \"query_string\": {\\n        \"query\": \"name:tester\"\\n    },\\n    \"filtered\": {\\n        \"filter\": {\\n            \"range\": {\\n                \"age\": {\\n                    \"from\": 27,\\n                    \"to\": 37,\\n                },\\n            },\\n        },\\n    },\\n}\\nresults = conn.search(query)\\n\\n# Clean up.\\nconn.delete_index(\"contacts\")\\n\\nFor more examples, please check out the doctests & tests.py.\\n\\nLicense\\nLicensed under the New BSD license.\\n\\nCredits\\nUsed pysolr as a jumping off point - thanks guys.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.txt\\n      \\n\\n\\ndjango-newcache\\n===============\\n\\nNewcache is an improved memcached cache backend for Django. It provides four\\nmajor advantages over Django\\'s built-in cache backend:\\n\\n * It supports pylibmc.\\n * It allows for a function to be run on each key before it\\'s sent to memcached.\\n * It supports setting cache keys with infinite timeouts.\\n * It mitigates the thundering herd problem.\\n\\nIt also has some pretty nice defaults. By default, the function that\\'s run on\\neach key is one that hashes, versions, and flavors the key.  More on that \\nlater.\\n\\n\\nHow to Install\\n--------------\\n\\nThe simplest way is to just set it as your cache backend in your settings.py, \\nlike so::\\n\\n    CACHE_BACKEND = \\'newcache://127.0.0.1:11211/?binary=true\\'\\n\\nNote that we\\'ve passed an additional argument, binary, to the backend.  This\\nis because pylibmc supports using binary mode to talk to memcached. This is a\\ncompletely optional parameter, and can be omitted safely to use the old text \\nmode. It is ignored when using python-memcached.\\n\\n\\nDefault Behavior\\n----------------\\n\\nEarlier we said that by default it hashes, versions, and flavors each key. What\\ndoes this mean?  Let\\'s go through each item in detail.\\n\\nKeys in memcached come with many restrictions, both on their length and on \\ntheir contents.  Practically speaking, this means that you can\\'t put spaces\\nin your keys, and they can\\'t be very long.  One simple solution to this is to\\ncreate an sha1 hash of whatever key you want, and use the hash as your key\\ninstead.  That is what we do in newcache.  It not only allows for long keys, \\nbut it also lets us put spaces or other characters in our key as well.\\n\\nSometimes it\\'s necessary to clear the entire cache. We can do this using \\nmemcached\\'s flushing mechanisms, but sometimes a cache is shared by many things\\ninstead of just one web app.  It\\'s a shame to have everything lose its\\nfresh cache just because one web app needed to clear its cache. For this, we\\nintroduce a simple technique called versioning. A version number is added to\\neach cache key, and when this version is incremented, all the old cache keys\\nwill become invalid because they have an incorrect version.\\n\\nThis is exposed as a new setting, CACHE_VERSION, and it defaults to 1.\\n\\nFinally, we found that as we split our site out into development, staging, and\\nproduction, we didn\\'t want them to share the same cache.  But we also didn\\'t\\nwant to spin up a new memcached instance for each one.  So we came up with the\\nidea of flavoring the cache.  The concept is simple--add a FLAVOR setting and\\nmake it something like \\'dev\\', \\'prod\\', or \\'test\\'.  With newcache, this flavor\\nstring will be added to each key, ensuring that there are no collisions.\\n\\nConcretely, this is what happens::\\n\\n    # CACHE_VERSION = 2\\n    # FLAVOR = \\'staging\\'\\n    cache.get(\\'games\\')\\n    # ... would actually call ...\\n    cache.get(\\'staging-2-9cfa7aefcc61936b70aaec6729329eda\\')\\n\\n\\nChanging the Default\\n--------------------\\n\\nAll of the above is simply the default, you may provide your own callable\\nfunction to be run on each key, by supplying the CACHE_KEY_MODULE setting. It\\nmust provide a get_key function which takes any instance of basestring and \\noutput a str.\\n\\n\\nThundering Herd Mitigation\\n--------------------------\\n\\nThe thundering herd problem manifests itself when a cache key expires, and many\\nthings rush to get or generate the data stored for that key all at once.  This \\nis doing a lot of unnecessary work and can cause service outages if the\\ndatabase cannot handle the load.  To solve this problem, we really only want \\none thread or process to fetch this data.\\n\\nOur method of solving this problem is to shove the old (expired) value back \\ninto the cache for a short time while the first process/thread goes and updates\\nthe key.  This is done in a completely transparent way--no changes should need\\nto be made in the application code.\\n\\nWith this cache backend, we have provided an extra \\'herd\\' keyword argument to \\nthe set, add, and set_many methods--which is set to True by default. What this \\ndoes is transform your cache value into a tuple before saving it to the cache. \\nEach value is structured like this:\\n\\n    (A herd marker, your original value, the expiration timestamp)\\n\\nThen when it actually sets the cache, it sets the real timeout to a little bit\\nlonger than the expiration timestamp. Actually, this \"little bit\" is \\nconfigurable using the CACHE_HERD_TIMEOUT setting, but it defaults to 60 \\nseconds.\\n\\nNow every time we read a value from the cache, we automatically unpack it and \\ncheck whether it\\'s expired.  If it has expired, we put it back in the cache for \\nCACHE_HERD_TIMEOUT seconds, but (and this is the key) we act as if it were a \\ncache miss (so we return None, or whatever the default was for the call.)\\n\\n*Note*: If you want to set a value to be used as a counter (with incr and\\n        decr) then you\\'ll want to bypass the herd mechanism.\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\n\\nGargoyle\\n\\nGargoyle is a platform built on top of Django which allows you to switch functionality of your application on and off based on conditions.\\n\\nResources\\n\\nDocumentation\\nBug Tracker\\nCode\\n\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': 'No readme file.', 'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nNexus\\nNexus is a pluggable admin application in Django. It's designed to give you a simple design and architecture for building admin applications.\\n(This project is still under active development)\\n\\nScreenshot\\n\\n\\nInstall\\nInstall it with pip (or easy_install):\\npip install nexus\\n\\n\\nConfig\\nYou'll need to enable it much like you would django.contrib.admin.\\nFirst, add it to your INSTALLED_APPS setting:\\nINSTALLED_APPS = (\\n    ...\\n    'nexus',\\n)\\n\\nNow you'll want to include it within your urls.py:\\nimport nexus\\n\\n# sets up the default nexus site by detecting all nexus_modules.py files\\nnexus.autodiscover()\\n\\n# urls.py\\nurlpatterns = patterns('',\\n    ('^nexus/', include(nexus.site.urls)),\\n)\\n\\nBy default Nexus requires django.contrib.auth and django.contrib.sessions. If you are using a custom auth system you can skip these requirements by using the setting NEXUS_SKIP_INSTALLED_APPS_REQUIREMENTS = True in your django settings.\\n\\nModules\\nNexus by default includes a module that will automatically pick up django.contrib.admin.\\nOther applications which provide Nexus modules:\\n\\nGargoyle\\nMemcache\\nRedis\\ndjango-debug-logging\\n\\n(docs on writing modules coming soon)\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nThink of it as flatpages for small bits of reusable content you might want to insert into your templates and manage from the admin interface.\\nThis is really nothing more than a model and a template tag.\\nBy adding chunks to your installed apps list in your Django project and performing a ./manage.py syncdb, you\\'ll be able to add as many \"keyed\" bits of content chunks to your site.\\nThe idea here is that you can create a chunk of content, name it with a unique key (for example: home_page_left_bottom) and then you can call this content from a normal template.\\nWhy would anyone want this?\\nWell it essentially allows someone to define \"chunks\" (I had wanted to call it blocks, but that would be very confusing for obvious reasons) of content in your template that can be directly edited from the awesome Django admin interface.  Throwing a rich text editor control on top of it make it even easier.\\nUsage:\\n{% load chunks %}\\n<html>\\n  <head>\\n    <title>Test</title>\\n  </head>\\n  <body>\\n    <h1> Blah blah blah</h1>\\n    <div id=\"sidebar\">\\n        ...\\n    </div>\\n    <div id=\"left\">\\n        {% chunk \"home_page_left\" %}\\n    </div>\\n    <div id=\"right\">\\n        {% chunk \"home_page_right\" %}\\n    </div>\\n  </body>\\n</html>\\n\\nThis is really helpful in those cases where you want to use django.contrib.flatpages but you need multiple content areas.  I hope this is helpful to people and I\\'ll be making minor edits as I see them necessary.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\narmstrong.apps.couchdb\\nA set of generic views for embedding CouchDB data into a view.\\n\\nWarning\\nUntil this is removed, the code mentioned here is not functional.\\nThis README is for README-driven-development\\n\\n\\nIntroduction\\nCouchDB can server HTML (or any other data) directly to the client.  This is\\ngreat for a green field project, but not for an existing one.  This application\\nprovides an easy way to wrap data generated by CouchDB in a Django view.\\nHere\\'s an example of how to use this inside your urls.py:\\nfrom django.conf.urls.defaults import patterns, url\\nurlpatterns = patterns(\"\",\\n    url(r\"^path/(?P<name>.*)$\", \"armstrong.apps.couchdb.views.show\", {\\n        \"couch_url\": \"local_db\",\\n        \"design_doc\": \"awesome\",\\n    }),\\n)\\n\\nThis renders the response in the armstrong/apps/couchdb/show.html template\\nusing the response from the localhost CouchDB using the local_db database and\\nthe _design/awesome design document.\\nFor example, sending the following request to the browser:\\nhttp://example.com/path/my-show\\n\\nTranslates to this CouchDB request:\\nhttp://127.0.0.1:5984/local_db/_design/awesome/_shows/my-show\\n\\n\\nViews\\nThe views provide a simple mechanism for grabbing data from CouchDB and putting\\nit into a template.  There are four arguments:\\n\\nname\\nThis is the name of the view, list, or show that is to be passed to\\nCouchDB.  This can not be empty.\\ncouch_url\\nThis defines the location your Couch database.  It takes the last portion\\nof the path and uses that as the database name, treating the other portion\\nas the URL of the database to connect to.  This is required, but can be\\nspecified the COUCH_DEFAULT_URL setting value (see below).\\ndesign_doc\\nThe name of the design document to be used.  The _design/ prefix is\\nadded to this if there is no / inside the name.  Like couch_url,\\nthis is required but can be provided via a setting.  It looks to\\nCOUCH_DEFAULT_DESIGN_DOC for its default value.\\ntemplate_name\\nThis is the name of the template to load.  This defaults to\\narmstrong/apps/couchdb/<view name>.html if not explicitly provided.\\nEach template is given a RequestContext for the given request.  It\\ncontains a response (the raw dict-like object returned from `httplib2`_\\nand body (the actual response from the server) values.  If the body\\nis JSON, it is decoded using django.utils.simplejson.  Otherwise the\\nvalue is a string.\\n\\nThere are three views available:\\n\\nlist\\nshow\\nview\\n\\nEach view translates directly into a request to a request against the CouchDB\\nserver.  It follows the same formula for all views.\\n<couch_url>/_design/<design_doc>/_<view>/<name>\\n\\nSo a request to show with the couch_url of \"this\", design_doc\\nvalue of \"awesome\" and a name of \"data-from-couch\" would generate\\nthe following URL:\\nhttp://127.0.0.1:5984/this/_design/awesome/_show/data-from-couch\\n\\n\\nAvailable Configuration\\nYou can avoid having to repeatedly define couch_url and design_doc, you\\ncan add those values to your settings module.\\n\\nCOUCH_DEFAULT_URL\\nThis specifies a specific URL for your Couch database.  This is overridden\\nby an explicit couch_url passed into the view.\\nCOUCH_DEFAULT_DESIGN_DOC\\nThis refers to the design document to use.  This assumes that it should be\\nprefixed with _design/.\\n\\n\\nContributing\\nContributions are welcomed and encouraged.  Please follow these instructions\\nfor making a contribution:\\n\\nFork this repository\\nCreate a topic branch.  Be descriptive with its name.\\nMake some great addition, fix a bug, or clean it up\\nSubmit a Pull Request\\n\\nYou can also report bugs via Issue Tracker.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\narmstrong.base\\nBase classes to provide generic functionality.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': 'No readme file.', 'language': 'CoffeeScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ndjango-staticfiles-enquirejs\\nenquire.js meets Django staticfiles\\n\\nAbout enquire.js\\nenquire.js is a lightweight, pure javascript library (with no dependencies)\\nfor programmatically responding to media queries.\\n\\nUsage\\nThis application is meant to be used with django.contrib.staticfiles or\\ndjango-staticfiles.  Make sure that staticfiles is set up and configured,\\nthen install this application using pip:\\npip install django-staticfiles-enquirejs\\n\\nFinally, make sure that enquirejs is listed in your INSTALLED_APPS.  You\\ncan use this oneliner to add it as well:\\nINSTALLED_APPS += [\\'enquirejs\\', ]\\n\\nAdding enquire.js to your Django templates:\\n\\nUsing django.contrib.staticfiles or django-staticfiles with a\\ntemplatetag:\\n{% load static from staticfiles %}\\n<script src=\"{% static \\'enquirejs/enquire.min.js\\' %}\"></script>\\n\\n\\nor the context variable STATIC_URL:\\n<script src=\"{{ STATIC_URL }}enquirejs/enquire.min.js\"></script>\\n\\n\\n\\n\\nBuild\\nUsing make:\\n\\nSet VERSION to the target version to build\\nRun make\\n\\nmakefile example:\\necho \"1.5.6\" > VERSION\\nmake\\n\\nUsing build.py:\\n\\nRun python support/build.py <version> to build that version of enquirejs.\\nThis will also update the VERSION file.\\n\\nbuild.py example:\\npython support/build.py 1.5.6\\n\\n\\nLicense\\nThis software is licensed under the Apache Software License. See the\\nLICENSE file in the top distribution directory for the full license text.\\nenquire.js is licensed MIT (http://www.opensource.org/licenses/mit-license.php)\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        readme.textile\\n      \\n\\n\\nOverview\\nBootstrap-wysihtml5 is a javascript plugin that makes it easy to create simple, beautiful wysiwyg editors with the help of wysihtml5 and Twitter Bootstrap.\\nExamples\\n\\nhttp://jhollingworth.github.com/bootstrap-wysihtml5\\n\\nQuick Start\\nIf you are using rails use the bootstrap-wysihtml5-rails gem.\\ngem install bootstrap-wysihtml5-rails\\n\\nOtherwise, download the latest version of bootstrap-wysihtml5.\\nFiles to reference\\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/bootstrap-wysihtml5.css\"></link>\\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/bootstrap.min.css\"></link>\\n<script src=\"js/wysihtml5-0.3.0.js\"></script>\\n<script src=\"js/jquery-1.7.2.min.js\"></script>\\n<script src=\"js/bootstrap.min.js\"></script>\\n<script src=\"js/bootstrap-wysihtml5.js\"></script>\\n\\nUsage\\n<textarea id=\"some-textarea\" placeholder=\"Enter text ...\"></textarea>\\n<script type=\"text/javascript\">\\n\\t$(\\'#some-textarea\\').wysihtml5();\\n</script>\\n\\nYou can get the html generated by getting the value of the text area, e.g.\\n$(\\'#some-textarea\\').val();\\n\\nAdvanced\\nOptions\\nAn options object can be passed in to .wysihtml5() to configure the editor:\\n$(\\'#some-textarea\\').wysihtml5({someOption: 23, ...})\\n\\nButtons\\nTo override which buttons to show, set the appropriate flags:\\n$(\\'#some-textarea\\').wysihtml5({\\n\\t\"font-styles\": true, //Font styling, e.g. h1, h2, etc. Default true\\n\\t\"emphasis\": true, //Italics, bold, etc. Default true\\n\\t\"lists\": true, //(Un)ordered lists, e.g. Bullets, Numbers. Default true\\n\\t\"html\": false, //Button which allows you to edit the generated HTML. Default false\\n\\t\"link\": true, //Button to insert a link. Default true\\n\\t\"image\": true, //Button to insert an image. Default true,\\n\\t\"color\": false //Button to change color of font  \\n});\\n\\nCustom Templates for Toolbar Buttons\\nTo define custom templates for buttons, you can submit a customTemplates hash with the new definitions.  Each entry should be a function which expects ‘locale’ and optional ‘options’ to manage the translations.\\nFor example, the default template used for the editHtml mode button looks like this (with size fetched from the optional ‘options’)\\n<li>\\n  <div class=\\'btn-group\\'>\\n    <a class=\\'btn\" + size + \"\\' data-wysihtml5-action=\\'change_view\\' title=\\'\" + locale.html.edit + \"\\'><i class=\\'icon-pencil\\'></i></a>\"\\n  </div>\\n</li>\\n\\nYou can change it to not use the pencil icon (for example) by defining the custom template like this:\\nvar myCustomTemplates = {\\n  html : function(locale) {\\n    return \"<li>\" +\\n           \"<div class=\\'btn-group\\'>\" +\\n           \"<a class=\\'btn\\' data-wysihtml5-action=\\'change_view\\' title=\\'\" + locale.html.edit + \"\\'>HTML</a>\" +\\n           \"</div>\" +\\n           \"</li>\";\\n  }\\n}\\n\\n// pass in your custom templates on init\\n$(\\'#some-textarea\\').wysihtml5({\\n   customTemplates: myCustomTemplates\\n});\\n\\nThis will override only the toolbar template for html, and all others will use the default.\\nStylesheets\\nIt is possible to supply a number of stylesheets for inclusion in the editor ``:</p>\\n<pre>\\n$(\\'#some-textarea\\').wysihtml5({\\n\\t\"stylesheets\": [\"/path/to/editor.css\"]\\n});\\n</pre>\\n<h3>Events</h3>\\n<p>Wysihtml5 exposes a <a href=\"https://github.com/xing/wysihtml5/wiki/Events\">number of events</a>. You can hook into these events when initialising the editor:</p>\\n<pre>\\n$(\\'#some-textarea\\').wysihtml5({\\n\\t\"events\": {\\n\\t\\t\"load\": function() { \\n\\t\\t\\tconsole.log(\"Loaded!\");\\n\\t\\t},\\n\\t\\t\"blur\": function() { \\n\\t\\t\\tconsole.log(\"Blured\");\\n\\t\\t}\\n\\t}\\n});\\n</pre>\\n<h3>Shallow copy by default, deep on request</h3>\\n<p>Options you pass in will be added to the defaults via a shallow copy.  (see <a href=\"http://api.jquery.com/jQuery.extend/\" title=\"\">jQuery.extend</a> for details). You can use a deep copy instead (which is probably what you want if you&#8217;re adding tags or classes to parserRules) via &#8216;deepExtend&#8217;, as in the parserRules example below.</p>\\n<h3>Parser Rules</h3>\\n<p>If you find the editor is stripping out tags or attributes you need, then you&#8217;ll want to extend (or replace) parserRules.  This example extends the defaults to allow the <code>&lt;strong&gt;</code> and <code>&lt;em&gt;</code> tags, and the class &#8220;middle&#8221;:</p>\\n<pre>\\n$(\\'#some-textarea\\').wysihtml5(\\'deepExtend\\', {\\n  parserRules: {\\n    classes: {\\n      \"middle\": 1\\n    },\\n    tags {\\n      strong: {},\\n      em: {}\\n    }\\n  }\\n});\\n</pre>\\n<p>There&#8217;s quite a bit that can be done with parserRules; see <a href=\"https://github.com/xing/wysihtml5/blob/master/parser_rules/advanced.js\">wysihtml5&#8217;s advanced parser ruleset</a> for details.  bootstrap-wysihtml5&#8217;s default parserRules can be found <a href=\"https://github.com/jhollingworth/bootstrap-wysihtml5/blob/master/src/bootstrap-wysihtml5.js\">in the source</a> (just search for &#8216;parserRules&#8217; in the file).</p>\\n<h3>Defaults</h3>\\n<p>You can change bootstrap-wysihtml5&#8217;s defaults by altering:</p>\\n<pre>\\n$.fn.wysihtml5.defaultOptions\\n</pre>\\n<p>This object has the same structure as the options object you pass in to .wysihtml5().  You can revert to the original defaults by calling:</p>\\n<pre>\\n$(this).wysihtml5(\\'resetDefaults\\') \\n</pre>\\n<p>Operations on the defaults are not thread-safe; if you&#8217;re going to change the defaults, you probably want to do it only once, after you load the bootstrap-wysihtml plugin and before you start instantiating your editors.</p>\\n<h2>The underlying wysihtml5 object</h2>\\n<p>You can access the <a href=\"https://github.com/xing/wysihtml5\">wysihtml5 editor object</a> like this:</p>\\n<pre>\\nvar wysihtml5Editor = $(\\'#some-textarea\\').data(\"wysihtml5\").editor;\\nwysihtml5Editor.composer.commands.exec(\"bold\");\\n</pre>\\n<h2>I18n</h2>\\n<p>You can use Bootstrap-wysihtml5 in other languages. There are some translations available in the src/locales directory. You can include your desired one after the plugin and pass its key to the editor. Example:</p>\\n<pre>\\n&lt;script src=\"src/locales/bootstrap-wysihtml5.pt-BR.js\"&gt;&lt;/script&gt;\\n&lt;script type=\"text/javascript\"&gt;\\n  $(\\'#some-textarea\\').wysihtml5({locale: \"pt-BR\"});\\n&lt;/script&gt;\\n</pre>\\n<p>It is possible to use custom translations as well. Just add a new key to $.fn.wysihtml5.locale before calling the editor constructor.</p>\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.textile\\n      \\n\\n\\nwysihtml5 0.3.0\\nwysihtml5 is an open source rich text editor based on HTML5 technology and the progressive-enhancement approach.\\nIt uses a sophisticated security concept and aims to generate fully valid HTML5 markup by preventing unmaintainable tag soups and inline styles.\\nThe code is completely library agnostic: No jQuery, Prototype or similar is required.\\nThis project was initiated and is supported by the XING AG. Thanks!\\nFeatures\\n\\nAuto linking of urls as-you-type\\nGenerates valid and semantic HTML5 markup (no <font>)\\nUses class-names instead of inline styles\\nUnifies line-break handling across browsers (hitting enter will create <br> instead of <p> or <div>)\\nAuto-parses content inserted via copy & paste (from Word, Powerpoint, PDF, other web pages, …)\\nConverts invalid or unknown html tags into valid/similar tags\\nSource code view for users with HTML skills\\nUses sandboxed iframes in order to prevent identity theft through XSS\\nEditor inherits styles and attributes (placeholder, autofocus, …) from original textarea (you only have to style one element)\\nSpeech-input for Chrome\\n\\nBrowser Support\\nThe rich text editing interface is supported in IE8+, FF 3.5+, Safari 4+, Safari on iOS 5+, Opera 11+ and Chrome.\\nGraceful Degradation: Users with other browsers will see the textarea and are still able to write plain HTML by themselves.\\nDemos\\n\\nSimple Editor with italic and bold buttons\\nEditor with advanced functionality\\n\\nCompanies using wysihtml5\\n\\nBasecamp – Leading web-based project management and collaboration tool\\nXING – Business Social Network with more than 12 million members\\nQype – Largest user-generated local review site in Europe\\nand many more …\\n\\nWiki\\nCheck our Wiki Pages including a simple Getting Started Tutorial.\\nResearch\\nBefore starting wysihtml5 we spent a lot of time investigating the different browsers and their behaviors.\\nCheck this repository:\\nhttps://github.com/tiff/wysihtml5-tests\\nA compatibility table for rich text query commands can be found here:\\nhttp://tifftiff.de/contenteditable/compliance_test.html\\nA pure native rich text editor with HTML validator and live source preview is here:\\nhttp://tifftiff.de/contenteditable/editor.html\\nContributors\\n\\n@tiff\\n@ingochao\\n@uwe\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\narmstrong.apps.related_content\\nProvides mechanism for relating content to other models\\nYou can use armstrong.apps.related_content to link two separate models\\ntogether through a GenericForeignKey for the source and the\\ndestination.  You can further organize the relationship with\\nRelatedType (think: \"articles\", \"images\", \"external_links\", and so on) and\\nall relationships are ordered.\\n\\nUsage\\nYou do not have to change your models to utilize related content---it exists\\noutside of your model.  There are two fields that you can add that give you\\neasy access to your related content:\\n\\narmstrong.apps.related_content.fields.RelatedObjectsField\\narmstrong.apps.related_content.fields.ReverseRelatedObjectsField\\n\\nThe first let\\'s you access objects where your model is the source, the\\nlatter lets you access objects where your model is the destination.  Note\\nthat these return the actual models that are related, not the\\nRelatedContent model.  If you need access to the raw RelatedContent\\nmodel directly from your model, see\\narmstrong.apps.related_content.fields.RelatedContentField.\\nYou can also use the RelatedContentInline for exposing an admin interface\\nto your related content inside Django\\'s admin.\\n\\nAccessing Related Content\\nYou can access fields through the RelatedObjectsField or\\nReverseRelatedObjectsField by calling all() or\\nby_type(\"some_type\").  These return QuerySet-like objects, but since they\\nare generic relationships, they\\'re not quite QuerySets.\\nInside templates, you can access related content by type using the dot-syntax.\\nFor example, you could load the first related content of a type \"articles\"\\nwith this syntax:\\n{{ my_article.related.articles.0 }}\\n\\n\\nInstallation & Configuration\\nYou can install the latest release of armstrong.apps.related_content using\\npip:\\npip install armstrong.apps.related_content\\n\\nMake sure to add armstrong.apps.related_content to your INSTALLED_APPS.\\nYou can add this however you like.  This works as a copy-and-paste solution:\\nINSTALLED_APPS += [\"armstrong.apps.related_content\", ]\\n\\nOnce installed, you have to run either syncdb, or migrate if you are\\nusing South.\\n\\nBackwards Incompatible Changes\\n\\nVersion 2.0\\n\\nRelatedObjectsField no longer extends\\ngenericm2m.models.RelatedObjectsDescriptor.\\nAll fields have been been moved into the fields module now.\\n\\n\\n\\n\\nContributing\\n\\nCreate something awesome -- make the code better, add some functionality,\\nwhatever (this is the hardest part).\\nFork it\\nCreate a topic branch to house your changes\\nGet all of your commits in the new topic branch\\nSubmit a pull request\\n\\n\\nState of Project\\nArmstrong is an open-source news platform that is freely available to any\\norganization.  It is the result of a collaboration between the Texas Tribune\\nand Bay Citizen, and a grant from the John S. and James L. Knight\\nFoundation.\\nTo follow development, be sure to join the Google Group.\\narmstrong.apps.related_content is part of the Armstrong project.  You\\'re\\nprobably looking for that.\\n\\nLicense\\nCopyright 2011-2012 Bay Citizen and Texas Tribune\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ntx_locations\\nThis Django application was generated using the Texas Tribune's Generic\\nDjango app template.\\n\\nInstallation & Configuration\\nYou can install this using pip like this:\\npip install tx_locations\\n\\nOnce installed, you need to add it to your INSTALLED_APPS.  You can do that\\nhowever you like or you can copy-and-paste this in after your\\nINSTALLED_APPS are defined.\\nINSTALLED_APPS += ['tx_locations', ]\\n\\nNow you're ready to start using tx_locations.\\n\\nUsage\\nTODO\\n\\nExample\\nAll of the usage is outlined, along with tests inside the example\\ndirectory.  See that directory for more information on how to run the tests and\\nexample project.\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\ntt-chart-service\\nConfiguration\\nheroku config:set TT_CHART_SERVICE_TOKEN=<secret>\\n\\nLocal Testing\\npython tests.py\\n\\nLive Testing\\ncurl --data \"svg=`cat test.svg`\" $web_url/render/?token=$TT_CHART_SERVICE_TOKEN\\n\\nLicense\\nApache 2.0\\n\\n\\n',\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': \"\\n\\n\\n\\n        Readme.md\\n      \\n\\n\\nHeroku buildpack: GeoDjango\\nThis is a Heroku buildpack for GeoDjango apps.\\nIt extends the original Python buildpack by adding GEOS, Proj.4 and GDAL, per the GeoDjango installation\\ninstructions.\\nThis buildpack assumes your PostGIS server lives outside the Heroku stack, though we'd love to see it\\nforked to handle any setup for Heroku's Postgres services.\\nUsage\\nExample usage:\\n$ heroku create --stack cedar --buildpack http://github.com/cirlabs/heroku-buildpack-geodjango/\\n\\n$ git push heroku master\\n...\\n-----> Heroku receiving push\\n-----> Fetching custom buildpack... done\\n-----> Python app detected\\n-----> Fetching and installing GEOS 3.3.2\\n-----> Installing ...\\n   GEOS installed\\n-----> Fetching and installing Proj.4 4.7.0\\n-----> Installing ...\\n   Proj.4 installed\\n-----> Fetching and installing GDAL 1.8.1\\n-----> Installing ...\\n   GDAL installed\\n-----> Preparing virtualenv version 1.7\\n... etc.\\n\\nNotes\\nAll libraries are stored in /app/.github.\\nIMPORTANT: You will need to set two Django settings in order for GEOS and GDAL to work properly!\\nGEOS_LIBRARY_PATH = '/app/.geodjango/geos/lib/libgeos_c.so'\\nGDAL_LIBRARY_PATH = '/app/.geodjango/gdal/lib/libgdal.so'\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        Readme.md\\n      \\n\\n\\nHeroku buildpack: GeoDjango\\nThis is a Heroku buildpack for GeoDjango apps.\\nIt extends the original Python buildpack by adding GEOS, Proj.4 and GDAL, per the GeoDjango installation\\ninstructions.\\nThis buildpack assumes your PostGIS server lives outside the Heroku stack, though we'd love to see it\\nforked to handle any setup for Heroku's Postgres services.\\nUsage\\nExample usage:\\n$ heroku create --stack cedar --buildpack http://github.com/cirlabs/heroku-buildpack-geodjango/\\n\\n$ git push heroku master\\n...\\n-----> Heroku receiving push\\n-----> Fetching custom buildpack... done\\n-----> Python app detected\\n-----> Fetching and installing GEOS 3.3.2\\n-----> Installing ...\\n   GEOS installed\\n-----> Fetching and installing Proj.4 4.7.0\\n-----> Installing ...\\n   Proj.4 installed\\n-----> Fetching and installing GDAL 1.8.1\\n-----> Installing ...\\n   GDAL installed\\n-----> Preparing virtualenv version 1.7\\n... etc.\\n\\nNotes\\nAll libraries are stored in /app/.github.\\nIMPORTANT: You will need to set two Django settings in order for GEOS and GDAL to work properly!\\nGEOS_LIBRARY_PATH = '/app/.geodjango/geos/lib/libgeos_c.so'\\nGDAL_LIBRARY_PATH = '/app/.geodjango/gdal/lib/libgdal.so'\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nSpeaker Watch!\\nJust a simple script to watch the Texas Ethics Commission website for\\nchanges to the list of declared speakers and SMS a list of people when it\\nhappens.\\n\\nAbout this Code\\nThis is just a quick hack job.  Its being offered up as example code for the\\npurpose of learning and shouldn't be used for anything other than learning.\\nThat said...\\n\\nInstallation & Configuration\\nInstall the requirements:\\npip install -r requirements.txt\\n\\nAdd the following values to your environment variables:\\nTWILIO_ACCOUNT_SID={{ your Twilio account ID }}\\nTWILIO_AUTH_TOKEN={{ your Twilio auth token }}\\nFROM_PHONE_NUMBER={{ your Twilio phone # }}\\nNUMBERS_TO_NOTIFY={{ a comma separated list of numbers to call }}\\n\\nThe NUMBERS_TO_NOTIFY value assumes that all numbers are US and in the\\nformat of AAABBBCCCC and are comma-separated.\\nNote, you do need a Twilio account for this to all work.\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\ntx_bills_scraper\\nBill scraper for Texas State Legislature\\n\\n\\n',\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': 'No readme file.', 'language': 'No language specified.'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.md\\n      \\n\\n\\nd3chart\\nA work in progress library to build simple charts using d3.\\nDocumentation\\ntexastribune.github.com/d3chart\\nDev Requirements\\n\\nCoffeeScript: npm install coffeescript\\nDocco (to generate documentation): npm install docco\\nJist (to upload examples): gem install jist\\nForeman (to run a dev webserver want coffescript watcher): gem install foreman\\nPython (for dev webserver and pygments, a Docco requirement)\\n\\nDev Commands\\n\\nforeman start: Start dev server and coffescript watcher\\nmake: Generate documentation, docco and all examples\\nmake gist: Upload examples as gists\\n\\nmake gist will only update existing gists, and will only update gists that\\nare not yet commited to keep the gists' git history cleaner and\\nreduce network demand.\\n\\n\\n\",\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\nTT Glossary\\nA reusable Django app for glossing terms with rollover interactions.\\nInstallation\\nAdd glossary to your INSTALLED_APPS and run syncdb.\\nOptionally, you can set the GLOSSARY_CONTEXT_VARIABLE setting to determine the name of the glossary variable in the template context. This setting defaults to \"TT_GLOSSARY\".\\nUsage\\nThe app installs two models, Glossary and Term, that can be managed in admin.\\nTo use the glossaries in your template:\\n{% load glossary %}\\n{% load_glossary \"My Glossary\" %}\\n{% gloss \"My Term\" %}\\n\\nThis wraps the term in an abbr tag with term\\'s definition as its title:\\n<abbr class=\"gloss glossed\" title=\"My definition\">My Term</abbr>\\n\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nofficeledger\\nOfficeledger provides a simple method of keeping track of a pool of money for\\nbuying office things like groceries\\nBSD License\\nCopyright (c) 2011, Texas Tribune\\nAll rights reserved.\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\nRedistributions of source code must retain the above copyright notice, this\\nlist of conditions and the following disclaimer.  Redistributions in binary\\nform must reproduce the above copyright notice, this list of conditions and the\\nfollowing disclaimer in the documentation and/or other materials provided with\\nthe distribution.\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ndjango-sentry\\nSentry provides you with a generic interface to view and interact with your error logs. By\\ndefault, it will catch any exception thrown by Django and store it in a database. With this\\nit allows you to interact and view near real-time information to discover issues and more\\neasily trace them in your application.\\n\\nNote: Sentry has only been tested under a PostgreSQL environment.\\n\\n\\nScreenshot\\n\\n\\nRequirements\\n\\n\\nDjango >= 1.2 (to use a secondary database to store error logs)\\ndjango-indexer (stores metadata indexes)\\ndjango-paging\\npygooglechart (to generate optional error reports)\\n\\n\\n\\nUpgrading\\nIf you use South migrations, simply run:\\npython manage.py migrate sentry\\n\\nIf you don't use South, then start.\\n\\nInstall\\nThe easiest way to install the package is via pip:\\npip install django-sentry --upgrade\\n\\nOR, if you're not quite on the same page (work on that), with setuptools:\\neasy_install django-sentry\\n\\nOnce installed, update your settings.py and add sentry, sentry.client, indexer, and paging to INSTALLED_APPS:\\nINSTALLED_APPS = (\\n    'django.contrib.admin',\\n    'django.contrib.auth',\\n    'django.contrib.contenttypes',\\n    'django.contrib.sessions',\\n\\n    # don't forget to add the dependancies!\\n    'indexer',\\n    'paging',\\n    'sentry',\\n    'sentry.client',\\n    ...\\n)\\n\\nFinally, run python manage.py syncdb to create the database tables.\\n(If you use South, you'll need to use python manage.py migrate sentry)\\n\\nMulti-server configuration\\nTo configure Sentry for use in a multi-server environment, first you'll want to configure your Sentry server (not your application):\\nINSTALLED_APPS = [\\n  ...\\n  'indexer',\\n  'paging',\\n  'sentry',\\n  'sentry.client',\\n]\\n\\nSENTRY_KEY = '0123456789abcde'\\n\\nAnd on each of your application servers, specify the URL of the Sentry server, add sentry.client to INSTALLED_APPS, and specify the same key used in your Sentry server's settings:\\n# This should be the absolute URI of sentries store view\\nSENTRY_REMOTE_URL = 'http://your.sentry.server/sentry/store/'\\n\\nINSTALLED_APPS = [\\n  ...\\n  'sentry.client',\\n]\\n\\nSENTRY_KEY = '0123456789abcde'\\n\\nYou may also specify an alternative timeout to the default (which is 5 seconds) for all outgoing logging requests:\\nSENTRY_REMOTE_TIMEOUT = 5\\n\\nSentry also allows you to support high availability by pushing to multiple servers:\\nSENTRY_REMOTE_URL = ['http://server1/sentry/store/', 'http://server2/sentry/store/']\\n\\n\\nOther configuration options\\nSeveral options exist to configure django-sentry via your settings.py:\\n\\nSENTRY_ADMINS\\nOn smaller sites you may wish to enable throttled emails, we recommend doing this by first\\nremoving the ADMINS setting in Django, and adding in SENTRY_ADMINS:\\nADMINS = ()\\nSENTRY_ADMINS = ('root@localhost',)\\n\\nThis will send out a notification the first time an error is seen, and the first time an error is\\nseen after it has been resolved.\\n\\nSENTRY_CATCH_404_ERRORS\\nEnable catching of 404 errors in the logs. Default value is False:\\nSENTRY_CATCH_404_ERRORS = True\\n\\nYou can skip other custom exception types by adding a skip_sentry = True attribute to them.\\n\\nSENTRY_DATABASE_USING\\nUse a secondary database to store error logs. This is useful if you have several websites and want to aggregate error logs onto one database server:\\n# This should correspond to a key in your DATABASES setting\\nSENTRY_DATABASE_USING = 'default'\\n\\nYou should also enable the SentryRouter to avoid things like extraneous table creation:\\nDATABASE_ROUTERS = [\\n        'sentry.routers.SentryRouter',\\n        ...\\n]\\n\\n\\nNote\\nThis functionality REQUIRES Django 1.2. We highly recommend using HTTP over multi-db, as it can cause issues with dependancies such as django-indexer.\\n\\n\\nSENTRY_TESTING\\nEnabling this setting allows the testing of Sentry exception handler even if Django DEBUG is enabled.\\nDefault value is False\\n\\nNote\\nNormally when Django DEBUG is enabled the Sentry exception handler is immediately skipped\\n\\n\\nSENTRY_NAME\\nThis will override the server_name value for this installation. Defaults to socket.get_hostname().\\n\\nIntegration with logging\\ndjango-sentry supports the ability to directly tie into the logging module. To use it simply add SentryHandler to your logger:\\nimport logging\\nfrom sentry.client.handlers import SentryHandler\\n\\nlogging.getLogger().addHandler(SentryHandler())\\n\\n# Add StreamHandler to sentry's default so you can catch missed exceptions\\nlogging.getLogger('sentry').addHandler(logging.StreamHandler())\\n\\nYou can also use the exc_info and extra=dict(url=foo) arguments on your log methods. This will store the appropriate information and allow django-sentry to render it based on that information:\\n\\nlogging.error('There was some crazy error', exc_info=sys.exc_info(), extra={'url': request.build_absolute_uri()})\\nAny additional information you pass into the extra clause will also be stored as meta information with the event. As long as the key\\nname is not reserved (url) and not private (_foo) it will be displayed on the Sentry dashboard.\\n\\nUsage\\nSet up a viewer server (or use your existing application server) and add sentry to your INSTALLED_APPS and your included URLs:\\n# urls.py\\nurlpatterns = patterns('',\\n    (r'^admin/', include(admin.site.urls)),\\n    (r'^sentry/', include('sentry.urls')),\\n)\\n\\nNow enjoy your beautiful new error tracking at /sentry/.\\n\\nAPI\\nFor the technical, here's some further docs:\\nIf you wish to access these within your own views and models, you may do so via the standard model API:\\nfrom sentry.models import Message, GroupedMessage\\n\\n# Pull the last 10 unresolved errors.\\nGroupedMessage.objects.filter(status=0).order_by('-last_seen')[0:10]\\n\\nYou can also record errors outside of handler if you want:\\nfrom sentry.client.base import SentryClient\\n\\ntry:\\n        ...\\nexcept Exception, exc:\\n        SentryClient.create_from_exception([exc_info=None, url=None, view=None])\\n\\nIf you wish to log normal messages (useful for non-logging integration):\\nfrom sentry.client.base import SentryClient\\nimport logging\\n\\nSentryClient.create_from_text('Message Message'[, level=logging.WARNING, url=None])\\n\\nBoth the url and level parameters are optional. level should be one of the following:\\n\\nlogging.DEBUG\\nlogging.INFO\\nlogging.WARNING\\nlogging.ERROR\\nlogging.FATAL\\n\\nIf you have a custom exception class, similar to Http404, or something else you don't want to log,\\nyou can also add skip_sentry = True to your exception class or instance, and sentry will simply ignore\\nthe error.\\n\\nNotes\\n\\nsentry-client will automatically integrate with django-idmapper.\\nsentry-client supports South migrations.\\nThe fact that the admin shows large quantities of results, even if there aren't, is not a bug. This is an efficiency hack on top of Django.\\n\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.md\\n      \\n\\n\\njquery.tablebars\\nJQuery plugin for quickly visualizing tabular numerical data\\n\\n\\n',\n",
       "  'language': 'JavaScript'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nDjango Compressor\\nDjango Compressor combines and compresses linked and inline Javascript\\nor CSS in a Django templates into cacheable static files by using the\\ncompress template tag.\\nHTML in between {% compress js/css %} and {% endcompress %} is\\nparsed and searched for CSS or JS. These styles and scripts are subsequently\\nprocessed with optional, configurable compilers and filters.\\nThe default filter for CSS rewrites paths to static files to be absolute\\nand adds a cache busting timestamp. For Javascript the default filter\\ncompresses it using jsmin.\\nAs the final result the template tag outputs a <script> or <link>\\ntag pointing to the optimized file. These files are stored inside a folder\\nand given an unique name based on their content. Alternatively it can also\\nreturn the resulting content to the original template directly.\\nSince the file name is dependend on the content these files can be given\\na far future expiration date without worrying about stale browser caches.\\nThe concatenation and compressing process can also be jump started outside\\nof the request/response cycle by using the Django management command\\nmanage.py compress.\\n\\nConfigurability & Extendibility\\nDjango Compressor is highly configurable and extendible. The HTML parsing\\nis done using lxml or if it's not available Python's built-in HTMLParser by\\ndefault. As an alternative Django Compressor provides a BeautifulSoup and a\\nhtml5lib based parser, as well as an abstract base class that makes it easy to\\nwrite a custom parser.\\nDjango Compressor also comes with built-in support for CSS Tidy,\\nYUI CSS and JS compressor, the Google's Closure Compiler, a Python\\nport of Douglas Crockford's JSmin, a Python port of the YUI CSS Compressor\\ncssmin and a filter to convert (some) images into data URIs.\\nIf your setup requires a different compressor or other post-processing\\ntool it will be fairly easy to implement a custom filter. Simply extend\\nfrom one of the available base classes.\\nMore documentation about the usage and settings of Django Compressor can be\\nfound on django_compressor.readthedocs.org.\\nThe source code for Django Compressor can be found and contributed to on\\ngithub.com/jezdez/django_compressor. There you can also file tickets.\\nThe in-development version of Django Compressor can be installed with\\npip install django_compressor==dev or easy_install django_compressor==dev.\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README\\n      \\n\\n\\nDjango is a high-level Python Web framework that encourages rapid development\\nand clean, pragmatic design.\\n\\nAll documentation is in the \"docs\" directory and online at\\nhttp://docs.djangoproject.com/en/dev/. If you\\'re just getting started, here\\'s\\nhow we recommend you read the docs:\\n\\n    * First, read docs/intro/install.txt for instructions on installing Django.\\n\\n    * Next, work through the tutorials in order (docs/intro/tutorial01.txt,\\n      docs/intro/tutorial02.txt, etc.).\\n\\n    * If you want to set up an actual deployment server, read\\n      docs/howto/deployment/index.txt for instructions.\\n\\n    * You\\'ll probably want to read through the topical guides (in docs/topics)\\n      next; from there you can jump to the HOWTOs (in docs/howto) for specific\\n      problems, and check out the reference (docs/ref) for gory details.\\n\\nDocs are updated rigorously. If you find any problems in the docs, or think they\\nshould be clarified in any way, please take 30 seconds to fill out a ticket\\nhere:\\n\\nhttp://code.djangoproject.com/newticket\\n\\nTo get more help:\\n\\n    * Join the #django channel on irc.freenode.net. Lots of helpful people\\n      hang out there. Read the archives at http://botland.oebfare.com/logger/django/.\\n\\n    * Join the django-users mailing list, or read the archives, at\\n      http://groups.google.com/group/django-users.\\n\\nTo contribute to Django:\\n\\n    * Check out http://www.djangoproject.com/community/ for information\\n      about getting involved.\\n\\n\\n',\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ngmap-split-viewer\\nA simple JavaScript page for showing two split Google Maps with different\\nFusion Table data loaded on each side.\\n\\n\\n',\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nThe Budget Battle\\nOn June 15th, the Texas Tribune launched this timeline showing our\\ncoverage of the budget battle.\\nThere's a few pieces of JavaScript that are interesting that might be of use\\nto other developers working on interactive infographics.  Namely:\\n\\nSticky navigation elements\\nFading elements out\\n\\nNothing ground breaking here---it was thrown together quickly, but I thought it\\nmight be useful as examples.\\n\\n\\n\",\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': \"\\n\\n\\n\\n        README.rst\\n      \\n\\n\\ndjango-tastypie\\nCreating delicious APIs for Django apps since 2010.\\nCurrently in beta (v0.8.2) but being used actively in production on several\\nsites.\\n\\nRequirements\\n\\nPython 2.4+\\nDjango 1.1+\\nmimeparse 0.1.3+ (http://code.google.com/p/mimeparse/)\\nOlder versions will work, but their behavior on JSON/JSONP is a touch wonky.\\n\\n\\ndateutil (http://labix.org/python-dateutil)\\nlxml (http://codespeak.net/lxml/) if using the XML serializer\\npyyaml (http://pyyaml.org/) if using the YAML serializer\\nuuid (present in 2.5+, downloadable from http://pypi.python.org/pypi/uuid/) if using the ApiKey authentication\\n\\n\\nWhat's It Look Like?\\nA basic example looks like:\\n# myapp/api.py\\n# ============\\nfrom tastypie.resources import ModelResource\\nfrom myapp.models import Entry\\n\\n\\nclass EntryResource(ModelResource):\\n    class Meta:\\n        queryset = Entry.objects.all()\\n\\n\\n# urls.py\\n# =======\\nfrom django.conf.urls.defaults import *\\nfrom tastypie.api import Api\\nfrom myapp.api import EntryResource\\n\\nv1_api = Api(api_name='v1')\\nv1_api.register(EntryResource())\\n\\nurlpatterns = patterns('',\\n    # The normal jazz here then...\\n    (r'^api/', include(v1_api.urls)),\\n)\\n\\nThat gets you a fully working, read-write API for the Entry model that\\nsupports all CRUD operations in a RESTful way. JSON/XML/YAML support is already\\nthere, and it's easy to add related data/authentication/caching.\\nYou can find more in the documentation at\\nhttp://toastdriven.github.com/django-tastypie/.\\n\\nWhy tastypie?\\nThere are other, better known API frameworks out there for Django. You need to\\nassess the options available and decide for yourself. That said, here are some\\ncommon reasons for tastypie.\\n\\nYou need an API that is RESTful and uses HTTP well.\\nYou want to support deep relations.\\nYou DON'T want to have to write your own serializer to make the output right.\\nYou want an API framework that has little magic, very flexible and maps well to\\nthe problem domain.\\nYou want/need XML serialization that is treated equally to JSON (and YAML is\\nthere too).\\nYou want to support my perceived NIH syndrome, which is less about NIH and more\\nabout trying to help out friends/coworkers.\\n\\n\\nReference Material\\n\\nhttp://github.com/toastdriven/django-tastypie/tree/master/tests/basic shows\\nbasic usage of tastypie\\nhttp://en.wikipedia.org/wiki/REST\\nhttp://en.wikipedia.org/wiki/List_of_HTTP_status_codes\\nhttp://www.ietf.org/rfc/rfc2616.txt\\nhttp://jacobian.org/writing/rest-worst-practices/\\n\\n\\n\\nauthor:Daniel Lindsley\\n\\ndate:2010/06/19\\n\\n\\n\\n\\n\\n\",\n",
       "  'language': 'Python'},\n",
       " {'readme': '\\n\\n\\n\\n        README.rst\\n      \\n\\n\\nAbout\\nA simple and efficient paginator.\\n\\nJinja2\\nJinja2 is supported via Coffin:\\n{% with paginate(request, my_queryset) as results %}\\n  {{ results.paging }}\\n  {% for result in results.objects %}\\n    {{ result }}\\n  {% endfor %}\\n  {{ results.paging }}\\n{% endwith %}\\n\\n\\nDjango\\nDjango templatetags require django-templatetag-sugar:\\n{% load paging_extras %}\\n\\n{% paginate my_queryset from request as results %}\\n{{ results.paging }}\\n{% for result in results.objects %}\\n  {{ result }}\\n{% endfor %}\\n{{ results.paging }}\\n\\n\\n\\n',\n",
       "  'language': 'No language specified.'},\n",
       " {'readme': 'No readme file.', 'language': 'No language specified.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bringing it all together chaining...\n",
    "corpus = get_github_readme('https://github.com/texastribune', 8, cache=True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(original):\n",
    "    word = original.lower()\n",
    "    word = unicodedata.normalize('NFKD', word)\\\n",
    "                                .encode('ascii', 'ignore')\\\n",
    "                                .decode('utf-8', 'ignore')\n",
    "    word = re.sub(r\"[^a-z0-9'\\s]\", '', word)\n",
    "    word = word.replace('\\n',' ')\n",
    "    word = word.replace('\\t',' ')\n",
    "    return word\n",
    "\n",
    "def tokenize(original):\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    return tokenizer.tokenize(basic_clean(original))\n",
    "\n",
    "def stem(original):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in original.split()]\n",
    "    original_stemmed = ' '.join(stems)\n",
    "    return original_stemmed\n",
    "\n",
    "def lemmatize(original):\n",
    "    nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
    "    doc = nlp(original) # process the text with spacy\n",
    "    lemmas = [word.lemma_ for word in doc]\n",
    "    original_lemmatized = ' '.join(lemmas)\n",
    "    return original_lemmatized\n",
    "\n",
    "def remove_stopwords(original, extra_words=['readmemd'], exclude_words=[]):\n",
    "    tokenizer = ToktokTokenizer()\n",
    "\n",
    "    stopword_list = stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "\n",
    "    for word in extra_words:\n",
    "        stopword_list.append(word)\n",
    "    for word in exclude_words:\n",
    "        stopword_list.remove(word)\n",
    "\n",
    "    words = original.split()\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "\n",
    "    print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "    print('---')\n",
    "\n",
    "    original_nostop = ' '.join(filtered_words)\n",
    "\n",
    "    return original_nostop\n",
    "\n",
    "def prep_article(article):\n",
    "    \n",
    "    article_stemmed = stem(basic_clean(article['readme']))\n",
    "    article_lemmatized = lemmatize(article_stemmed)\n",
    "    article_without_stopwords = remove_stopwords(article_lemmatized)\n",
    "    \n",
    "    article['stemmed'] = article_stemmed\n",
    "    article['lemmatized'] = article_lemmatized\n",
    "    article['clean'] = article_without_stopwords\n",
    "    \n",
    "    return article\n",
    "\n",
    "def prepare_article_data(corpus):\n",
    "    transformed  = []\n",
    "    for article in corpus:\n",
    "        transformed.append(prep_article(article))\n",
    "    return transformed\n",
    "\n",
    "# This is to fix the string as list of words per readme file glitch\n",
    "def clean(text):\n",
    "    'A simple function to cleanup text data'\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "             .encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 33 stopwords\n",
      "---\n",
      "Removed 165 stopwords\n",
      "---\n",
      "Removed 908 stopwords\n",
      "---\n",
      "Removed 227 stopwords\n",
      "---\n",
      "Removed 64 stopwords\n",
      "---\n",
      "Removed 16 stopwords\n",
      "---\n",
      "Removed 11 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 21 stopwords\n",
      "---\n",
      "Removed 88 stopwords\n",
      "---\n",
      "Removed 245 stopwords\n",
      "---\n",
      "Removed 1164 stopwords\n",
      "---\n",
      "Removed 8 stopwords\n",
      "---\n",
      "Removed 102 stopwords\n",
      "---\n",
      "Removed 497 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 37 stopwords\n",
      "---\n",
      "Removed 475 stopwords\n",
      "---\n",
      "Removed 290 stopwords\n",
      "---\n",
      "Removed 311 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 32 stopwords\n",
      "---\n",
      "Removed 16 stopwords\n",
      "---\n",
      "Removed 21 stopwords\n",
      "---\n",
      "Removed 8 stopwords\n",
      "---\n",
      "Removed 26 stopwords\n",
      "---\n",
      "Removed 44 stopwords\n",
      "---\n",
      "Removed 159 stopwords\n",
      "---\n",
      "Removed 119 stopwords\n",
      "---\n",
      "Removed 47 stopwords\n",
      "---\n",
      "Removed 109 stopwords\n",
      "---\n",
      "Removed 132 stopwords\n",
      "---\n",
      "Removed 4 stopwords\n",
      "---\n",
      "Removed 157 stopwords\n",
      "---\n",
      "Removed 225 stopwords\n",
      "---\n",
      "Removed 115 stopwords\n",
      "---\n",
      "Removed 36 stopwords\n",
      "---\n",
      "Removed 47 stopwords\n",
      "---\n",
      "Removed 280 stopwords\n",
      "---\n",
      "Removed 107 stopwords\n",
      "---\n",
      "Removed 466 stopwords\n",
      "---\n",
      "Removed 3 stopwords\n",
      "---\n",
      "Removed 12 stopwords\n",
      "---\n",
      "Removed 166 stopwords\n",
      "---\n",
      "Removed 23 stopwords\n",
      "---\n",
      "Removed 112 stopwords\n",
      "---\n",
      "Removed 38 stopwords\n",
      "---\n",
      "Removed 76 stopwords\n",
      "---\n",
      "Removed 29 stopwords\n",
      "---\n",
      "Removed 154 stopwords\n",
      "---\n",
      "Removed 70 stopwords\n",
      "---\n",
      "Removed 77 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 218 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 21 stopwords\n",
      "---\n",
      "Removed 313 stopwords\n",
      "---\n",
      "Removed 29 stopwords\n",
      "---\n",
      "Removed 378 stopwords\n",
      "---\n",
      "Removed 526 stopwords\n",
      "---\n",
      "Removed 144 stopwords\n",
      "---\n",
      "Removed 390 stopwords\n",
      "---\n",
      "Removed 102 stopwords\n",
      "---\n",
      "Removed 131 stopwords\n",
      "---\n",
      "Removed 180 stopwords\n",
      "---\n",
      "Removed 36 stopwords\n",
      "---\n",
      "Removed 120 stopwords\n",
      "---\n",
      "Removed 200 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 52 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 86 stopwords\n",
      "---\n",
      "Removed 8 stopwords\n",
      "---\n",
      "Removed 72 stopwords\n",
      "---\n",
      "Removed 57 stopwords\n",
      "---\n",
      "Removed 161 stopwords\n",
      "---\n",
      "Removed 137 stopwords\n",
      "---\n",
      "Removed 26 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 10 stopwords\n",
      "---\n",
      "Removed 73 stopwords\n",
      "---\n",
      "Removed 19 stopwords\n",
      "---\n",
      "Removed 69 stopwords\n",
      "---\n",
      "Removed 70 stopwords\n",
      "---\n",
      "Removed 11 stopwords\n",
      "---\n",
      "Removed 156 stopwords\n",
      "---\n",
      "Removed 41 stopwords\n",
      "---\n",
      "Removed 6 stopwords\n",
      "---\n",
      "Removed 45 stopwords\n",
      "---\n",
      "Removed 84 stopwords\n",
      "---\n",
      "Removed 21 stopwords\n",
      "---\n",
      "Removed 51 stopwords\n",
      "---\n",
      "Removed 1679 stopwords\n",
      "---\n",
      "Removed 52 stopwords\n",
      "---\n",
      "Removed 110 stopwords\n",
      "---\n",
      "Removed 3 stopwords\n",
      "---\n",
      "Removed 9 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 87 stopwords\n",
      "---\n",
      "Removed 86 stopwords\n",
      "---\n",
      "Removed 9 stopwords\n",
      "---\n",
      "Removed 450 stopwords\n",
      "---\n",
      "Removed 4 stopwords\n",
      "---\n",
      "Removed 126 stopwords\n",
      "---\n",
      "Removed 131 stopwords\n",
      "---\n",
      "Removed 224 stopwords\n",
      "---\n",
      "Removed 154 stopwords\n",
      "---\n",
      "Removed 826 stopwords\n",
      "---\n",
      "Removed 35 stopwords\n",
      "---\n",
      "Removed 596 stopwords\n",
      "---\n",
      "Removed 57 stopwords\n",
      "---\n",
      "Removed 14 stopwords\n",
      "---\n",
      "Removed 54 stopwords\n",
      "---\n",
      "Removed 82 stopwords\n",
      "---\n",
      "Removed 3 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 41 stopwords\n",
      "---\n",
      "Removed 94 stopwords\n",
      "---\n",
      "Removed 73 stopwords\n",
      "---\n",
      "Removed 12 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 87 stopwords\n",
      "---\n",
      "Removed 94 stopwords\n",
      "---\n",
      "Removed 11 stopwords\n",
      "---\n",
      "Removed 81 stopwords\n",
      "---\n",
      "Removed 15 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 218 stopwords\n",
      "---\n",
      "Removed 37 stopwords\n",
      "---\n",
      "Removed 61 stopwords\n",
      "---\n",
      "Removed 8 stopwords\n",
      "---\n",
      "Removed 19 stopwords\n",
      "---\n",
      "Removed 17 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 91 stopwords\n",
      "---\n",
      "Removed 189 stopwords\n",
      "---\n",
      "Removed 324 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 68 stopwords\n",
      "---\n",
      "Removed 60 stopwords\n",
      "---\n",
      "Removed 2 stopwords\n",
      "---\n",
      "Removed 13 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 365 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 217 stopwords\n",
      "---\n",
      "Removed 34 stopwords\n",
      "---\n",
      "Removed 217 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 17 stopwords\n",
      "---\n",
      "Removed 129 stopwords\n",
      "---\n",
      "Removed 7 stopwords\n",
      "---\n",
      "Removed 35 stopwords\n",
      "---\n",
      "Removed 158 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 47 stopwords\n",
      "---\n",
      "Removed 156 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 37 stopwords\n",
      "---\n",
      "Removed 14 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 46 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 61 stopwords\n",
      "---\n",
      "Removed 26 stopwords\n",
      "---\n",
      "Removed 157 stopwords\n",
      "---\n",
      "Removed 199 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 36 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 40 stopwords\n",
      "---\n",
      "Removed 30 stopwords\n",
      "---\n",
      "Removed 363 stopwords\n",
      "---\n",
      "Removed 13 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 53 stopwords\n",
      "---\n",
      "Removed 93 stopwords\n",
      "---\n",
      "Removed 217 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 51 stopwords\n",
      "---\n",
      "Removed 221 stopwords\n",
      "---\n",
      "Removed 83 stopwords\n",
      "---\n",
      "Removed 200 stopwords\n",
      "---\n",
      "Removed 36 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 30 stopwords\n",
      "---\n",
      "Removed 30 stopwords\n",
      "---\n",
      "Removed 60 stopwords\n",
      "---\n",
      "Removed 2 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n",
      "Removed 24 stopwords\n",
      "---\n",
      "Removed 35 stopwords\n",
      "---\n",
      "Removed 79 stopwords\n",
      "---\n",
      "Removed 315 stopwords\n",
      "---\n",
      "Removed 2 stopwords\n",
      "---\n",
      "Removed 139 stopwords\n",
      "---\n",
      "Removed 78 stopwords\n",
      "---\n",
      "Removed 5 stopwords\n",
      "---\n",
      "Removed 24 stopwords\n",
      "---\n",
      "Removed 111 stopwords\n",
      "---\n",
      "Removed 12 stopwords\n",
      "---\n",
      "Removed 1 stopwords\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(211, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(prepare_article_data(corpus))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['clean', 'language']]\n",
    "# remove_stopwords(df.iloc[11].clean) - ZACH'S DIAGNOSTIC TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Python</th>\n",
       "      <td>68</td>\n",
       "      <td>0.322275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JavaScript</th>\n",
       "      <td>60</td>\n",
       "      <td>0.284360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No language specified.</th>\n",
       "      <td>20</td>\n",
       "      <td>0.094787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSS</th>\n",
       "      <td>20</td>\n",
       "      <td>0.094787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HTML</th>\n",
       "      <td>14</td>\n",
       "      <td>0.066351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shell</th>\n",
       "      <td>13</td>\n",
       "      <td>0.061611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Makefile</th>\n",
       "      <td>5</td>\n",
       "      <td>0.023697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dockerfile</th>\n",
       "      <td>5</td>\n",
       "      <td>0.023697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ruby</th>\n",
       "      <td>3</td>\n",
       "      <td>0.014218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jupyter Notebook</th>\n",
       "      <td>2</td>\n",
       "      <td>0.009479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoffeeScript</th>\n",
       "      <td>1</td>\n",
       "      <td>0.004739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         n     ratio\n",
       "Python                  68  0.322275\n",
       "JavaScript              60  0.284360\n",
       "No language specified.  20  0.094787\n",
       "CSS                     20  0.094787\n",
       "HTML                    14  0.066351\n",
       "Shell                   13  0.061611\n",
       "Makefile                 5  0.023697\n",
       "Dockerfile               5  0.023697\n",
       "Ruby                     3  0.014218\n",
       "Jupyter Notebook         2  0.009479\n",
       "CoffeeScript             1  0.004739"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = pd.concat([df.language.value_counts(),\n",
    "                    df.language.value_counts(normalize=True)], axis=1)\n",
    "languages.columns = ['n', 'ratio']\n",
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all rows that has 'No language specified.'\n",
    "df = df[df.language != 'No language specified.']\n",
    "df = df.rename(index=str, columns={\"clean\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Python</th>\n",
       "      <td>68</td>\n",
       "      <td>0.356021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JavaScript</th>\n",
       "      <td>60</td>\n",
       "      <td>0.314136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSS</th>\n",
       "      <td>20</td>\n",
       "      <td>0.104712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HTML</th>\n",
       "      <td>14</td>\n",
       "      <td>0.073298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shell</th>\n",
       "      <td>13</td>\n",
       "      <td>0.068063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Makefile</th>\n",
       "      <td>5</td>\n",
       "      <td>0.026178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dockerfile</th>\n",
       "      <td>5</td>\n",
       "      <td>0.026178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ruby</th>\n",
       "      <td>3</td>\n",
       "      <td>0.015707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jupyter Notebook</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoffeeScript</th>\n",
       "      <td>1</td>\n",
       "      <td>0.005236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   n     ratio\n",
       "Python            68  0.356021\n",
       "JavaScript        60  0.314136\n",
       "CSS               20  0.104712\n",
       "HTML              14  0.073298\n",
       "Shell             13  0.068063\n",
       "Makefile           5  0.026178\n",
       "Dockerfile         5  0.026178\n",
       "Ruby               3  0.015707\n",
       "Jupyter Notebook   2  0.010471\n",
       "CoffeeScript       1  0.005236"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = pd.concat([df.language.value_counts(),\n",
    "                    df.language.value_counts(normalize=True)], axis=1)\n",
    "languages.columns = ['n', 'ratio']\n",
    "languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_words = clean(' '.join(df[df.language == 'Python'].text))\n",
    "\n",
    "js_words = clean(' '.join(df[df.language == 'JavaScript'].text))\n",
    "css_words = clean(' '.join(df[df.language == 'CSS'].text))\n",
    "html_words = clean(' '.join(df[df.language == 'HTML'].text))\n",
    "shell_words = clean(' '.join(df[df.language == 'Shell'].text))\n",
    "docker_words = clean(' '.join(df[df.language == 'Docker'].text))\n",
    "maker_words = clean(' '.join(df[df.language == 'Makefile'].text))\n",
    "ruby_words = clean(' '.join(df[df.language == 'Ruby'].text))\n",
    "jupyter_words = clean(' '.join(df[df.language == 'Jupyter Notebook'].text))\n",
    "coffee_words = clean(' '.join(df[df.language == 'CoffeeScript'].text))\n",
    "\n",
    "all_words = clean(' '.join(df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['softwar',\n",
       " 'collect',\n",
       " 'donat',\n",
       " 'nonprofit',\n",
       " 'integr',\n",
       " 'saleforc',\n",
       " 'stripe',\n",
       " 'amazon',\n",
       " 'pay',\n",
       " 'slack',\n",
       " 'sentri',\n",
       " 'donat',\n",
       " 'python',\n",
       " 'run',\n",
       " 'flask',\n",
       " 'support',\n",
       " 'singl',\n",
       " 'recur',\n",
       " 'donat',\n",
       " 'easili',\n",
       " 'deploy',\n",
       " 'heroku',\n",
       " 'get',\n",
       " 'start',\n",
       " 'recommend',\n",
       " 'method',\n",
       " 'run',\n",
       " 'thi',\n",
       " 'repo',\n",
       " 'local',\n",
       " 'use',\n",
       " 'docker',\n",
       " 'alreadi',\n",
       " 'docker',\n",
       " 'set',\n",
       " 'want',\n",
       " 'instal',\n",
       " 'docker',\n",
       " 'mac',\n",
       " 'get',\n",
       " 'docker',\n",
       " 'environ',\n",
       " 'set',\n",
       " 'comput',\n",
       " 'also',\n",
       " 'need',\n",
       " 'env',\n",
       " 'file',\n",
       " 'set',\n",
       " 'environ',\n",
       " 'variabl',\n",
       " 'stripe',\n",
       " 'salesforc',\n",
       " 'docker',\n",
       " 'find',\n",
       " 'default',\n",
       " 'makefil',\n",
       " 'look',\n",
       " 'envdock',\n",
       " 'thi',\n",
       " 'override',\n",
       " 'dockerenvfil',\n",
       " 'environ',\n",
       " 'variabl',\n",
       " 'also',\n",
       " 'instal',\n",
       " 'precommit',\n",
       " 'use',\n",
       " 'manag',\n",
       " 'git',\n",
       " 'hook',\n",
       " 'includ',\n",
       " 'j',\n",
       " 'format',\n",
       " 'via',\n",
       " 'pretty',\n",
       " 'onc',\n",
       " 'download',\n",
       " 'run',\n",
       " 'precommit',\n",
       " 'instal',\n",
       " 'root',\n",
       " 'thi',\n",
       " 'repo',\n",
       " 'also',\n",
       " 'need',\n",
       " 'node',\n",
       " 'version',\n",
       " '8',\n",
       " 'requir',\n",
       " 'python',\n",
       " '36',\n",
       " 'see',\n",
       " 'requirementstxt',\n",
       " 'devrequirementstxt',\n",
       " 'specif',\n",
       " 'python',\n",
       " 'packag',\n",
       " 'version',\n",
       " 'environ',\n",
       " 'variabl',\n",
       " 'exampl',\n",
       " 'publisablekey',\n",
       " 'pktest12345',\n",
       " 'secretkey',\n",
       " 'sktest12335',\n",
       " 'salesforcehost',\n",
       " 'testsalesforcecom',\n",
       " 'salesforceclientid',\n",
       " 'salesforceclientsecret',\n",
       " 'salesforceusernam',\n",
       " 'salesforcepassword',\n",
       " 'salesforcetoken',\n",
       " 'celerybrokerurl',\n",
       " 'amqpguestguestrabbitmq5672',\n",
       " 'celeryresultbackend',\n",
       " 'redisredis63790',\n",
       " 'flasksecretkey',\n",
       " 'bfxebx9bt2xcdxdbxe1zxfbxabxf8x03',\n",
       " 'enablesentri',\n",
       " 'fal',\n",
       " 'sentryenviron',\n",
       " 'product',\n",
       " 'sentrydsn',\n",
       " 'httpsuserpasssentry7timeout10',\n",
       " 'enableslack',\n",
       " 'fal',\n",
       " 'slackapikey',\n",
       " 'slackchannel',\n",
       " 'donat',\n",
       " 'mailserv',\n",
       " 'mailservercom',\n",
       " 'mailusernam',\n",
       " 'mailpassword',\n",
       " 'mailport',\n",
       " '25',\n",
       " 'mailusetl',\n",
       " 'true',\n",
       " 'defaultmailsend',\n",
       " 'foobarorg',\n",
       " 'accountingmailrecipi',\n",
       " 'foobarorg',\n",
       " 'businessmemberrecipi',\n",
       " 'foobarorg',\n",
       " 'redisurl',\n",
       " 'redisredis6379',\n",
       " 'salesforceapiver',\n",
       " 'v430',\n",
       " 'reporturi',\n",
       " 'httpsfoobar',\n",
       " 'run',\n",
       " 'project',\n",
       " 'run',\n",
       " 'make',\n",
       " 'back',\n",
       " 'thi',\n",
       " 'start',\n",
       " 'rabbitmq',\n",
       " 'redi',\n",
       " 'run',\n",
       " 'make',\n",
       " 'thi',\n",
       " 'drop',\n",
       " 'flask',\n",
       " 'app',\n",
       " 'run',\n",
       " 'yarn',\n",
       " 'run',\n",
       " 'dev',\n",
       " 'abl',\n",
       " 'interact',\n",
       " 'app',\n",
       " 'localhost80',\n",
       " 'cforceroottru',\n",
       " 'celeri',\n",
       " 'appceleri',\n",
       " 'worker',\n",
       " 'loglevelinfo',\n",
       " 'celeri',\n",
       " 'beat',\n",
       " 'app',\n",
       " 'appceleri',\n",
       " 'gunicorn',\n",
       " 'appapp',\n",
       " 'logfil',\n",
       " 'bind00005000',\n",
       " 'accesslogfil',\n",
       " 'frontend',\n",
       " 'command',\n",
       " 'yarn',\n",
       " 'run',\n",
       " 'dev',\n",
       " 'start',\n",
       " 'flask',\n",
       " 'develop',\n",
       " 'server',\n",
       " 'watch',\n",
       " 'j',\n",
       " 'cs',\n",
       " 'chang',\n",
       " 'yarn',\n",
       " 'run',\n",
       " 'jsdev',\n",
       " 'watch',\n",
       " 'j',\n",
       " 'cs',\n",
       " 'chang',\n",
       " 'frontend',\n",
       " 'note',\n",
       " 'yarn',\n",
       " 'run',\n",
       " 'dev',\n",
       " 'file',\n",
       " 'build',\n",
       " 'staticjsbuild',\n",
       " 'ignor',\n",
       " 'version',\n",
       " 'control',\n",
       " 'way',\n",
       " 'make',\n",
       " 'mani',\n",
       " 'chang',\n",
       " 'want',\n",
       " 'develop',\n",
       " 'webpack',\n",
       " 'recompil',\n",
       " 'file',\n",
       " 'never',\n",
       " 'show',\n",
       " 'vc',\n",
       " 'deploy',\n",
       " 'file',\n",
       " 'build',\n",
       " 'staticjsprod',\n",
       " 'thi',\n",
       " 'ignor',\n",
       " 'vc',\n",
       " 'becaus',\n",
       " 'heroku',\n",
       " 'creat',\n",
       " 'directori',\n",
       " 'thu',\n",
       " 'need',\n",
       " 'exist',\n",
       " 'repo',\n",
       " 'whi',\n",
       " 'contain',\n",
       " 'gitkeep',\n",
       " 'file',\n",
       " 'deploy',\n",
       " 'product',\n",
       " 'j',\n",
       " 'ha',\n",
       " 'build',\n",
       " 'via',\n",
       " 'postinstal',\n",
       " 'script',\n",
       " 'thi',\n",
       " 'mean',\n",
       " 'run',\n",
       " 'yarn',\n",
       " 'yarn',\n",
       " 'add',\n",
       " 'packag',\n",
       " 'local',\n",
       " 'insid',\n",
       " 'docker',\n",
       " 'get',\n",
       " 'compil',\n",
       " 'file',\n",
       " 'staticjsprod',\n",
       " 'show',\n",
       " 'version',\n",
       " 'control',\n",
       " 'delet',\n",
       " 'import',\n",
       " 'note',\n",
       " 'build',\n",
       " 'j',\n",
       " 'deploy',\n",
       " 'heroku',\n",
       " 'need',\n",
       " 'run',\n",
       " 'postinstal',\n",
       " 'script',\n",
       " 'packagejson',\n",
       " 'thi',\n",
       " 'also',\n",
       " 'mean',\n",
       " 'everi',\n",
       " 'time',\n",
       " 'run',\n",
       " 'yarn',\n",
       " 'yarn',\n",
       " 'add',\n",
       " 'packag',\n",
       " 'go',\n",
       " 'trigger',\n",
       " 'build',\n",
       " 'gener',\n",
       " 'bunch',\n",
       " 'file',\n",
       " 'staticjsprod',\n",
       " 'commit',\n",
       " 'run',\n",
       " 'test',\n",
       " 'run',\n",
       " 'project',\n",
       " 'test',\n",
       " 'run',\n",
       " 'make',\n",
       " 'test',\n",
       " 'secur',\n",
       " 'find',\n",
       " 'vulner',\n",
       " 'thi',\n",
       " 'repo',\n",
       " 'plea',\n",
       " 'report',\n",
       " 'securitytexastribuneorg',\n",
       " 'thermomet',\n",
       " 'wall',\n",
       " 'thi',\n",
       " 'queri',\n",
       " 'salesforc',\n",
       " 'opportun',\n",
       " 'inform',\n",
       " 'massag',\n",
       " 'nice',\n",
       " 'json',\n",
       " 'format',\n",
       " 'store',\n",
       " 's3',\n",
       " 'ultim',\n",
       " 'access',\n",
       " 'browsersweb',\n",
       " 'app',\n",
       " 'test',\n",
       " 'make',\n",
       " 'test',\n",
       " 'run',\n",
       " 'configur',\n",
       " 'env',\n",
       " 'file',\n",
       " 'salesforc',\n",
       " 'anw',\n",
       " 'aw',\n",
       " 'variabl',\n",
       " 'make',\n",
       " 'licen',\n",
       " 'thi',\n",
       " 'project',\n",
       " 'licen',\n",
       " 'term',\n",
       " 'mit',\n",
       " 'licen',\n",
       " 'scuol',\n",
       " 'italian',\n",
       " 'school',\n",
       " 'public',\n",
       " 'school',\n",
       " '3',\n",
       " 'setup',\n",
       " 'thi',\n",
       " 'project',\n",
       " 'assum',\n",
       " 'use',\n",
       " 'provid',\n",
       " 'docker',\n",
       " 'postgresql',\n",
       " 'databa',\n",
       " 'build',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'docker',\n",
       " 'run',\n",
       " 'run',\n",
       " 'make',\n",
       " 'dockerdb',\n",
       " 'thi',\n",
       " 'creat',\n",
       " 'data',\n",
       " 'volum',\n",
       " 'instanc',\n",
       " 'postgresql',\n",
       " 'django',\n",
       " 'ensur',\n",
       " 'django',\n",
       " 'know',\n",
       " 'look',\n",
       " 'need',\n",
       " 'set',\n",
       " 'databaseurl',\n",
       " 'use',\n",
       " 'docker',\n",
       " 'provid',\n",
       " 'databa',\n",
       " 'use',\n",
       " 'databaseurl',\n",
       " 'tell',\n",
       " 'app',\n",
       " 'youv',\n",
       " 'instead',\n",
       " 'export',\n",
       " 'databaseurlpostgresdockerdockerdockerlocal5432dock',\n",
       " 'app',\n",
       " 'runnabl',\n",
       " 'normal',\n",
       " 'creat',\n",
       " 'virtual',\n",
       " 'environ',\n",
       " 'mkvirtualenv',\n",
       " 'scuoledev',\n",
       " 'instal',\n",
       " 'requir',\n",
       " 'pip',\n",
       " 'instal',\n",
       " 'r',\n",
       " 'requirementslocaltxt',\n",
       " 'check',\n",
       " 'ani',\n",
       " 'migrat',\n",
       " 'python',\n",
       " 'managepi',\n",
       " 'migrat',\n",
       " 'see',\n",
       " 'run',\n",
       " 'python',\n",
       " 'managepi',\n",
       " 'runserv',\n",
       " 'good',\n",
       " 'let',\n",
       " 'go',\n",
       " 'admin',\n",
       " 'thi',\n",
       " 'like',\n",
       " 'admin',\n",
       " 'interfac',\n",
       " 'welcom',\n",
       " 'use',\n",
       " 'check',\n",
       " 'thing',\n",
       " 'get',\n",
       " 'load',\n",
       " 'first',\n",
       " 'need',\n",
       " 'creat',\n",
       " 'super',\n",
       " 'user',\n",
       " 'ever',\n",
       " 'blow',\n",
       " 'away',\n",
       " 'databa',\n",
       " 'python',\n",
       " 'managepi',\n",
       " 'createsuperus',\n",
       " 'python',\n",
       " 'managepi',\n",
       " 'runserv',\n",
       " 'visit',\n",
       " 'httplocalhost8000admin',\n",
       " 'use',\n",
       " 'credenti',\n",
       " 'setup',\n",
       " 'get',\n",
       " 'access',\n",
       " 'everi',\n",
       " 'thing',\n",
       " 'set',\n",
       " 'readonli',\n",
       " 'risk',\n",
       " 'bork',\n",
       " 'anyth',\n",
       " 'readmerst',\n",
       " 'txsalari',\n",
       " 'thi',\n",
       " 'django',\n",
       " 'applic',\n",
       " 'wa',\n",
       " 'gener',\n",
       " 'use',\n",
       " 'texa',\n",
       " 'tribun',\n",
       " 'gener',\n",
       " 'django',\n",
       " 'app',\n",
       " 'templat',\n",
       " 'instal',\n",
       " 'instal',\n",
       " 'thi',\n",
       " 'use',\n",
       " 'pip',\n",
       " 'like',\n",
       " 'thi',\n",
       " 'pip',\n",
       " 'instal',\n",
       " 'txsalari',\n",
       " 'usag',\n",
       " 'txsalari',\n",
       " 'mean',\n",
       " 'use',\n",
       " 'conjunct',\n",
       " 'datum',\n",
       " 'receiv',\n",
       " 'variou',\n",
       " 'depart',\n",
       " 'around',\n",
       " 'state',\n",
       " 'must',\n",
       " 'request',\n",
       " 'thi',\n",
       " 'data',\n",
       " 'want',\n",
       " 'use',\n",
       " 'txsalari',\n",
       " 'import',\n",
       " 'data',\n",
       " 'datum',\n",
       " 'import',\n",
       " 'use',\n",
       " 'importsalarydata',\n",
       " 'manag',\n",
       " 'command',\n",
       " 'run',\n",
       " 'salariestexastribuneorg',\n",
       " 'repo',\n",
       " 'onc',\n",
       " 'txsalari',\n",
       " 'properli',\n",
       " 'instal',\n",
       " 'like',\n",
       " 'python',\n",
       " 'salariesmanagepi',\n",
       " 'importsalarydata',\n",
       " 'pathtosomesalaryspreadsheetxlsx',\n",
       " 'datum',\n",
       " 'import',\n",
       " 'use',\n",
       " 'csvkit',\n",
       " 'import',\n",
       " 'ani',\n",
       " 'spreadsheet',\n",
       " 'format',\n",
       " 'csvkit',\n",
       " 'in2csv',\n",
       " 'understand',\n",
       " 'setup',\n",
       " 'pull',\n",
       " 'master',\n",
       " 'salariestexastribuneorg',\n",
       " 'txsalari',\n",
       " 'get',\n",
       " 'rid',\n",
       " 'old',\n",
       " 'virtual',\n",
       " 'environ',\n",
       " 'salari',\n",
       " 'rmvirtualenv',\n",
       " 'name',\n",
       " 'virtual',\n",
       " 'env',\n",
       " 'make',\n",
       " 'new',\n",
       " 'virtual',\n",
       " 'environ',\n",
       " 'mkvirtualenv',\n",
       " 'name',\n",
       " 'virtual',\n",
       " 'env',\n",
       " 'instal',\n",
       " 'requir',\n",
       " 'pip',\n",
       " 'instal',\n",
       " 'r',\n",
       " 'requirementslocaltxt',\n",
       " 'instal',\n",
       " 'local',\n",
       " 'txsalari',\n",
       " 'thi',\n",
       " 'pip',\n",
       " 'instal',\n",
       " 'e',\n",
       " 'txsalari',\n",
       " 'youpipr',\n",
       " 'use',\n",
       " 'local',\n",
       " 'postgr',\n",
       " 'databa',\n",
       " 'recommend',\n",
       " 'need',\n",
       " 'set',\n",
       " 'first',\n",
       " 'set',\n",
       " 'databaseurl',\n",
       " 'export',\n",
       " 'databaseurlpostgreslocalhostsalari',\n",
       " 'pull',\n",
       " 'backup',\n",
       " 'make',\n",
       " 'localdbfetch',\n",
       " 'load',\n",
       " 'make',\n",
       " 'localdbrestor',\n",
       " 'good',\n",
       " 'work',\n",
       " 'txsalari',\n",
       " 'like',\n",
       " 'normal',\n",
       " 'ani',\n",
       " 'transform',\n",
       " 'alreadi',\n",
       " 'progress',\n",
       " 'need',\n",
       " 'merg',\n",
       " 'master',\n",
       " 'txsalari',\n",
       " 'forget',\n",
       " 'start',\n",
       " 'salariestexastribuneorg',\n",
       " 'server',\n",
       " 'termin',\n",
       " 'go',\n",
       " 'salariestexastribuneorg',\n",
       " 'repo',\n",
       " 'transform',\n",
       " 'live',\n",
       " 'txsalari',\n",
       " 'datum',\n",
       " 'manag',\n",
       " 'happen',\n",
       " 'salariestexastribun',\n",
       " 'repo',\n",
       " 'run',\n",
       " 'command',\n",
       " 'workon',\n",
       " 'name',\n",
       " 'virtual',\n",
       " 'env',\n",
       " 'export',\n",
       " 'databaseurlpostgreslocalhostsalari',\n",
       " 'python',\n",
       " 'salariesmanagepi',\n",
       " 'runserv',\n",
       " 'check',\n",
       " 'localhost8000',\n",
       " 'run',\n",
       " 'write',\n",
       " 'new',\n",
       " 'transform',\n",
       " 'thi',\n",
       " 'section',\n",
       " 'walk',\n",
       " 'creat',\n",
       " 'new',\n",
       " 'import',\n",
       " 'wer',\n",
       " 'go',\n",
       " 'use',\n",
       " 'fiction',\n",
       " 'rio',\n",
       " 'grand',\n",
       " 'counti',\n",
       " 'fiction',\n",
       " 'texa',\n",
       " 'least',\n",
       " 'transform',\n",
       " 'requir',\n",
       " 'two',\n",
       " 'thing',\n",
       " 'entri',\n",
       " 'transform',\n",
       " 'map',\n",
       " 'txsalariesutilstransformersinitpi',\n",
       " 'actual',\n",
       " 'transform',\n",
       " 'capabl',\n",
       " 'process',\n",
       " 'file',\n",
       " 'entri',\n",
       " 'transform',\n",
       " 'dictionari',\n",
       " 'make',\n",
       " 'uniqu',\n",
       " 'hash',\n",
       " 'serv',\n",
       " 'key',\n",
       " 'give',\n",
       " 'spreadsheet',\n",
       " 'callabl',\n",
       " 'function',\n",
       " 'transform',\n",
       " 'gener',\n",
       " 'key',\n",
       " 'run',\n",
       " 'follow',\n",
       " 'command',\n",
       " 'salariestexastribuneorg',\n",
       " 'virtualenv',\n",
       " 'python',\n",
       " 'salariesmanagepi',\n",
       " 'generatetransformerhash',\n",
       " 'pathtoriograndecountyxl',\n",
       " 'sheetdatasheet',\n",
       " 'rownumberofheaderrow',\n",
       " 'output',\n",
       " '40charact',\n",
       " 'string',\n",
       " 'copi',\n",
       " 'valu',\n",
       " 'open',\n",
       " 'txsalariesutilstransformersinitpi',\n",
       " 'file',\n",
       " 'contain',\n",
       " 'know',\n",
       " 'transform',\n",
       " 'find',\n",
       " 'spot',\n",
       " 'riograndecounti',\n",
       " 'would',\n",
       " 'fit',\n",
       " 'alphabet',\n",
       " 'dictionari',\n",
       " 'transform',\n",
       " 'add',\n",
       " 'thi',\n",
       " 'line',\n",
       " 'gener',\n",
       " 'hash',\n",
       " 'riograndecountytransform',\n",
       " 'gener',\n",
       " 'hash',\n",
       " 'alreadi',\n",
       " 'exist',\n",
       " 'anoth',\n",
       " 'transform',\n",
       " 'provid',\n",
       " 'tupl',\n",
       " 'text',\n",
       " 'label',\n",
       " 'transform',\n",
       " 'transform',\n",
       " 'modul',\n",
       " 'like',\n",
       " 'thi',\n",
       " 'gener',\n",
       " 'hash',\n",
       " 'rio',\n",
       " 'grand',\n",
       " 'county',\n",
       " 'riograndecountytransform',\n",
       " 'exist',\n",
       " 'county',\n",
       " 'othercountytransform',\n",
       " 'note',\n",
       " 'second',\n",
       " 'valu',\n",
       " 'string',\n",
       " 'instead',\n",
       " 'modul',\n",
       " 'need',\n",
       " 'import',\n",
       " 'modul',\n",
       " 'go',\n",
       " 'top',\n",
       " 'initpi',\n",
       " 'file',\n",
       " 'add',\n",
       " 'import',\n",
       " 'import',\n",
       " 'riograndecounti',\n",
       " 'save',\n",
       " 'file',\n",
       " 'next',\n",
       " 'need',\n",
       " 'creat',\n",
       " 'new',\n",
       " 'modul',\n",
       " 'referenc',\n",
       " 'insid',\n",
       " 'txsalariesutilstransform',\n",
       " 'directori',\n",
       " 'creat',\n",
       " 'new',\n",
       " 'file',\n",
       " 'call',\n",
       " 'riograndecountypi',\n",
       " 'first',\n",
       " 'pas',\n",
       " 'look',\n",
       " 'like',\n",
       " 'thi',\n",
       " 'import',\n",
       " 'base',\n",
       " 'import',\n",
       " 'mixin',\n",
       " 'import',\n",
       " 'string',\n",
       " 'datetim',\n",
       " 'import',\n",
       " 'date',\n",
       " 'add',\n",
       " 'necessari',\n",
       " 'sheetrequ',\n",
       " 'datum',\n",
       " 'row3',\n",
       " 'class',\n",
       " 'transformedrecord',\n",
       " 'mixinsgenericcompensationmixin',\n",
       " 'mixinsgenericidentifiermixin',\n",
       " 'mixinsgenericpersonmixin',\n",
       " 'mixinsmembershipmixin',\n",
       " 'mixinsorganizationmixin',\n",
       " 'mixinspostmixin',\n",
       " 'mixinsracemixin',\n",
       " 'mixinslinkmixin',\n",
       " 'basebasetransformedrecord',\n",
       " 'map',\n",
       " 'lastname',\n",
       " 'label',\n",
       " 'last',\n",
       " 'name',\n",
       " 'firstname',\n",
       " 'label',\n",
       " 'first',\n",
       " 'name',\n",
       " 'department',\n",
       " 'label',\n",
       " 'department',\n",
       " 'jobtitle',\n",
       " 'label',\n",
       " 'job',\n",
       " 'title',\n",
       " 'hiredate',\n",
       " 'label',\n",
       " 'hire',\n",
       " 'date',\n",
       " 'compensation',\n",
       " 'label',\n",
       " 'compensation',\n",
       " 'gender',\n",
       " 'label',\n",
       " 'gender',\n",
       " 'race',\n",
       " 'label',\n",
       " 'race',\n",
       " 'compensationtype',\n",
       " 'label',\n",
       " 'ftpt',\n",
       " 'status',\n",
       " 'order',\n",
       " 'name',\n",
       " 'field',\n",
       " 'build',\n",
       " 'full',\n",
       " 'name',\n",
       " 'fullnam',\n",
       " 'map',\n",
       " 'need',\n",
       " 'thi',\n",
       " 'namefield',\n",
       " 'firstname',\n",
       " 'lastname',\n",
       " 'name',\n",
       " 'organ',\n",
       " 'thi',\n",
       " 'show',\n",
       " 'site',\n",
       " 'doubl',\n",
       " 'check',\n",
       " 'organizationnam',\n",
       " 'rio',\n",
       " 'grand',\n",
       " 'county',\n",
       " 'type',\n",
       " 'organ',\n",
       " 'thi',\n",
       " 'thi',\n",
       " 'must',\n",
       " 'match',\n",
       " 'use',\n",
       " 'site',\n",
       " 'doubl',\n",
       " 'check',\n",
       " 'salariestexastribuneorg',\n",
       " 'organizationclassif',\n",
       " 'county',\n",
       " 'ymd',\n",
       " 'agenc',\n",
       " 'provid',\n",
       " 'data',\n",
       " 'dateprovid',\n",
       " 'date2013',\n",
       " '10',\n",
       " '31',\n",
       " 'track',\n",
       " 'gender',\n",
       " 'need',\n",
       " 'map',\n",
       " 'use',\n",
       " 'f',\n",
       " 'gendermap',\n",
       " 'female',\n",
       " 'f',\n",
       " 'male',\n",
       " 'url',\n",
       " 'find',\n",
       " 'raw',\n",
       " 'datum',\n",
       " 's3',\n",
       " 'bucket',\n",
       " 'url',\n",
       " 'httprawtexastribuneorgs3amazonawscom',\n",
       " 'pathto',\n",
       " 'riograndecountyxl',\n",
       " 'properti',\n",
       " 'def',\n",
       " 'isvalidself',\n",
       " 'adjust',\n",
       " 'return',\n",
       " 'fal',\n",
       " 'invalid',\n",
       " 'field',\n",
       " 'exampl',\n",
       " 'return',\n",
       " 'selflastnamestrip',\n",
       " 'properti',\n",
       " 'def',\n",
       " 'personself',\n",
       " 'name',\n",
       " 'selfgetnam',\n",
       " 'print',\n",
       " 'selfgendermapselfgenderstrip',\n",
       " 'r',\n",
       " 'familyname',\n",
       " 'namelast',\n",
       " 'givenname',\n",
       " 'namefirst',\n",
       " 'additionalname',\n",
       " 'namemiddl',\n",
       " 'name',\n",
       " 'unicodenam',\n",
       " 'gender',\n",
       " 'selfgendermapselfgenderstrip',\n",
       " 'return',\n",
       " 'r',\n",
       " 'properti',\n",
       " 'def',\n",
       " 'compensationtypeself',\n",
       " 'comptyp',\n",
       " 'selfgetmappedvaluecompensationtype',\n",
       " 'comptypeupp',\n",
       " 'full',\n",
       " 'time',\n",
       " 'return',\n",
       " 'ft',\n",
       " 'el',\n",
       " 'return',\n",
       " 'pt',\n",
       " 'properti',\n",
       " 'def',\n",
       " 'descriptionself',\n",
       " 'comptyp',\n",
       " 'selfgetmappedvaluecompensationtype',\n",
       " 'comptyp',\n",
       " 'ft',\n",
       " 'return',\n",
       " 'annual',\n",
       " 'gross',\n",
       " 'salary',\n",
       " 'elif',\n",
       " 'comptyp',\n",
       " 'pt',\n",
       " 'return',\n",
       " 'parttim',\n",
       " 'annual',\n",
       " 'gross',\n",
       " 'salary',\n",
       " 'transform',\n",
       " 'basetransformfactorytransformedrecord',\n",
       " 'label',\n",
       " 'xxx',\n",
       " 'field',\n",
       " 'adjust',\n",
       " 'match',\n",
       " 'appropri',\n",
       " 'column',\n",
       " 'give',\n",
       " 'spreadsheet',\n",
       " 'file',\n",
       " 'requir',\n",
       " 'special',\n",
       " 'sheet',\n",
       " 'row',\n",
       " 'handl',\n",
       " 'note',\n",
       " 'sheet',\n",
       " 'row',\n",
       " 'flag',\n",
       " 'comment',\n",
       " 'top',\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thi       242\n",
       "use       178\n",
       "instal    134\n",
       "django     98\n",
       "set        89\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_freq = pd.Series(python_words).value_counts()\n",
    "js_freq = pd.Series(js_words).value_counts()\n",
    "css_freq = pd.Series(css_words).value_counts()\n",
    "html_freq = pd.Series(html_words).value_counts()\n",
    "shell_freq = pd.Series(shell_words).value_counts()\n",
    "docker_freq = pd.Series(docker_words).value_counts()\n",
    "maker_freq = pd.Series(maker_words).value_counts()\n",
    "ruby_freq = pd.Series(ruby_words).value_counts()\n",
    "jupyter_freq = pd.Series(jupyter_words).value_counts()\n",
    "coffee_freq = pd.Series(coffee_words).value_counts()\n",
    "\n",
    "all_freq = pd.Series(all_words).value_counts()\n",
    "\n",
    "python_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
