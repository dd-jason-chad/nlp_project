{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAN\n",
    "\n",
    "- [x] Acquisition\n",
    "    - [x] Select what list of repos to scrape.\n",
    "    - [x] Get requests from the site.\n",
    "    - [x] Save responses to csv.\n",
    "- [x] Preparation\n",
    "    - [x] Prepare the data for analysis.\n",
    "- [ ] Exploration\n",
    "    - [ ] Answer the following prompts:\n",
    "        - [x] What are the most common words in READMEs?\n",
    "        - [ ] What does the distribution of IDFs look like for the most common words? - Jason\n",
    "        - [x] Does the length of the README vary by language? - Chad\n",
    "        - [x] Do different languages use a different number of unique words? DD\n",
    "- [ ] Modeling\n",
    "    - [x] Transform the data for machine learning; use language to predict.\n",
    "    - [x] Fit several models using different text representations.\n",
    "    - [ ] Build a function that will take in the text of a README file, and makes a prediction of language.\n",
    "- [ ] Delivery\n",
    "    - [ ] Github repo\n",
    "        - [x] This notebook.\n",
    "        - [ ] Documentation within the notebook.\n",
    "        - [x] README file in the repo.\n",
    "        - [ ] Python scripts if applicable.\n",
    "    - [ ] Google Slides\n",
    "        - [ ] 1-2 slides only summarizing analysis.\n",
    "        - [ ] Visualizations are labeled.\n",
    "        - [ ] Geared for the general audience.\n",
    "        - [ ] Share link @ readme file and/or classroom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ADDITIONAL_STOPWORDS = ['readme', '\\n\\n\\n', '-PRON-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACQUIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have decided to search Github for \"san antonio data\" and scrape the results.\n",
    "# https://github.com/open-austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_repo(url):\n",
    "    \"\"\"\n",
    "    This function takes a url and returns a dictionary that\n",
    "    contains the content and language of the readme file.\n",
    "    \"\"\"\n",
    "    response = get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    readme = soup.find('div', id='readme')\n",
    "    language = soup.find('span', class_='lang')\n",
    "    \n",
    "    d = dict()\n",
    "    if readme is None:\n",
    "        d['readme'] = 'No readme file.'\n",
    "    else:\n",
    "        d['readme'] = readme.text\n",
    "    if language is None:\n",
    "        d['language'] = 'No language specified.'\n",
    "    else:\n",
    "        d['language'] = language.text\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This line to test out the function.\n",
    "# get_github_repo('https://github.com/open-austin/atx-citysdk-js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_links(url):\n",
    "    \"\"\"\n",
    "    This function takes in a url and returns a list of links\n",
    "    that comes from each individual repo listing page.\n",
    "    \"\"\"\n",
    "    response = get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for link in soup.findAll('a', itemprop='name codeRepository', attrs={'href': re.compile(\"^/\")}):\n",
    "        links.append(link.get('href'))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This line to test out the function.\n",
    "# get_github_links('https://github.com/open-austin?page=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_github_links(path, num_pages):\n",
    "    \"\"\"\n",
    "    This function takes in a url path and number of pages\n",
    "    and returns a list of lists of all links.\n",
    "    \"\"\"\n",
    "    all_links = []\n",
    "    for i in range(num_pages):      # Number of pages plus one\n",
    "        page = i + 1\n",
    "        response = get(path + str(page))\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        all_links.append(get_github_links(path + '?page=' + str(page)))\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # This line to test out the function.\n",
    "# get_all_github_links('https://github.com/open-austin', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(o, tree_types=(list, tuple)):\n",
    "    if isinstance(o, tree_types):\n",
    "        for value in o:\n",
    "            for subvalue in traverse(value, tree_types):\n",
    "                yield subvalue\n",
    "    else:\n",
    "        yield o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_readme(url, num_pages, cache=True):\n",
    "    if cache and os.path.exists('github_readme.json'):\n",
    "        readme_text = json.load(open('github_readme.json'))\n",
    "    else:\n",
    "        data = get_all_github_links(url, num_pages)\n",
    "        readme_text = []\n",
    "        for value in traverse(data):\n",
    "            print('https://github.com'+value)\n",
    "            readme_text.append(get_github_repo('https://github.com' + value))\n",
    "        json.dump(readme_text, open('github_readme.json', 'w'))\n",
    "    return readme_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bringing it all together chaining...\n",
    "from pprint import pprint\n",
    "corpus = get_github_readme('https://github.com/texastribune', 8, cache=True)\n",
    "pprint(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(original):\n",
    "    word = original.lower()\n",
    "    word = unicodedata.normalize('NFKD', word)\\\n",
    "                                .encode('ascii', 'ignore')\\\n",
    "                                .decode('utf-8', 'ignore')\n",
    "    word = re.sub(r\"[^a-z'\\s]\", ' ', word)\n",
    "    word = word.replace('\\n',' ')\n",
    "    word = word.replace('\\t',' ')\n",
    "    return word\n",
    "\n",
    "def tokenize(original):\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    return tokenizer.tokenize(basic_clean(original))\n",
    "\n",
    "def stem(original):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in original.split()]\n",
    "    original_stemmed = ' '.join(stems)\n",
    "    return original_stemmed\n",
    "\n",
    "def lemmatize(original):\n",
    "    nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
    "    doc = nlp(original) # process the text with spacy\n",
    "    lemmas = [word.lemma_ for word in doc]\n",
    "    original_lemmatized = ' '.join(lemmas)\n",
    "    return original_lemmatized\n",
    "\n",
    "def remove_stopwords(original, extra_words=['readmemd'], exclude_words=[]):\n",
    "    tokenizer = ToktokTokenizer()\n",
    "\n",
    "    stopword_list = stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "\n",
    "    for word in extra_words:\n",
    "        stopword_list.append(word)\n",
    "    for word in exclude_words:\n",
    "        stopword_list.remove(word)\n",
    "\n",
    "    words = original.split()\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "\n",
    "    print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "    print('---')\n",
    "\n",
    "    original_nostop = ' '.join(filtered_words)\n",
    "\n",
    "    return original_nostop\n",
    "\n",
    "def prep_article(article):\n",
    "\n",
    "#    article_stemmed = stem(basic_clean(article['readme']))\n",
    "#    Note the stem line immediately above has been commented out,\n",
    "#    the first item below retains the same name as the stem line above, to make everything else work.\n",
    "    article_stemmed = basic_clean(article['readme'])\n",
    "    article_lemmatized = lemmatize(article_stemmed)\n",
    "    article_without_stopwords = remove_stopwords(article_lemmatized)\n",
    "    \n",
    "    article['stemmed'] = article_stemmed\n",
    "    article['lemmatized'] = article_lemmatized\n",
    "    article['clean'] = article_without_stopwords\n",
    "    \n",
    "    return article\n",
    "\n",
    "def prepare_article_data(corpus):\n",
    "    transformed  = []\n",
    "    for article in corpus:\n",
    "        transformed.append(prep_article(article))\n",
    "    return transformed\n",
    "\n",
    "# This is to fix the string as list of words per readme file glitch\n",
    "def clean(text):\n",
    "    'A simple function to cleanup text data'\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "             .encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', ' ', text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(prepare_article_data(corpus))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['clean', 'language']]\n",
    "# remove_stopwords(df.iloc[11].clean) - ZACH'S DIAGNOSTIC TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = pd.concat([df.language.value_counts(),\n",
    "                    df.language.value_counts(normalize=True)], axis=1)\n",
    "languages.columns = ['n', 'ratio']\n",
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all rows that has 'No language specified.'\n",
    "df = df[df.language != 'No language specified.']\n",
    "df = df.rename(index=str, columns={\"clean\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = pd.concat([df.language.value_counts(),\n",
    "                    df.language.value_counts(normalize=True)], axis=1)\n",
    "languages.columns = ['n', 'ratio']\n",
    "languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DECISION POINT**\n",
    "\n",
    "Based on results of the above language distribution, we have made the decision to focus our analysis efforts primarily on Python and JavaScript languages, which comprises 67% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['language'].isin(['Python', 'JavaScript'])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explore the data that you have scraped. Here are some ideas for exploration:*\n",
    "\n",
    "- What are the most common words in READMEs?\n",
    "- What does the distribution of IDFs look like for the most common words?\n",
    "- Does the length of the README vary by language?\n",
    "- Do different languages use a different number of unique words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['readme_len'] = df2['text'].apply(len)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_df = df2[df2['language'] == 'Python']\n",
    "js_df = df2[df2['language'] == 'JavaScript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_df.readme_len.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_df.readme_len.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_df.readme_len.mean() - python_df.readme_len.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ANSWER:**\n",
    "*Yes, the length of README file does vary by language.  On average, README files associated with JavaScript language are 533 characters longer than Python README files.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating series of words by language:\n",
    "python_words = clean(' '.join(df[df.language == 'Python'].text))\n",
    "js_words = clean(' '.join(df[df.language == 'JavaScript'].text))\n",
    "\n",
    "all_words = clean(' '.join(df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_freq = pd.Series(all_words).value_counts()\n",
    "python_freq = pd.Series(python_words).value_counts()\n",
    "\n",
    "js_freq = pd.Series(js_words).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_freq.shape)\n",
    "print(python_freq.shape)\n",
    "print(js_freq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = (pd.concat([python_freq, js_freq, all_freq], axis=1, sort=True)\n",
    "                .set_axis(['python', 'js', 'all'], axis=1, inplace=False)\n",
    "                .fillna(0)\n",
    "                .apply(lambda s: s.astype(int)))\n",
    "\n",
    "word_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most frequently occuring words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any words that uniquely identify a particular language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([word_counts[word_counts.js == 0].sort_values(by='python').tail(5),\n",
    "           word_counts[word_counts.python == 0].sort_values(by='js').tail(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out the percentage of language distribution\n",
    "(word_counts\n",
    " .assign(p_python=word_counts.python / word_counts['all'],\n",
    "         p_js=word_counts.js / word_counts['all']\n",
    "        )\n",
    " .sort_values(by='all')\n",
    " [['p_python',\n",
    "   'p_js'\n",
    "  ]]\n",
    " .tail(20)\n",
    " .sort_values('p_python')\n",
    " .plot.barh(stacked=True, figsize=(12,5), width=.9))\n",
    "\n",
    "plt.title('Proportions of Languages for the 20 most common words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts[(word_counts.python > 10) & (word_counts.js > 10)]\\\n",
    "    .assign(ratio=lambda df: df.python / df.js)\\\n",
    "    .sort_values(by='ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cloud = WordCloud(background_color='white', height=1000, width=400, random_state=123).generate(' '.join(all_words))\n",
    "python_cloud = WordCloud(background_color='white', height=600, width=800, random_state=123).generate(' '.join(python_words))\n",
    "js_cloud = WordCloud(background_color='white', height=600, width=800, random_state=123).generate(' '.join(js_words))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "axs = [plt.axes([0, 0, .5, 1]), plt.axes([.5, .5, .5, .5]), plt.axes([.5, 0, .5, .5])]\n",
    "\n",
    "axs[0].imshow(all_cloud)\n",
    "axs[1].imshow(python_cloud)\n",
    "axs[2].imshow(js_cloud)\n",
    "\n",
    "axs[0].set_title('All Words')\n",
    "axs[1].set_title('Python')\n",
    "axs[2].set_title('JS')\n",
    "\n",
    "for ax in axs: ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_python_bigrams = (pd.Series(nltk.ngrams(python_words, 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_python_bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_js_bigrams = (pd.Series(nltk.ngrams(js_words, 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_js_bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_python_bigrams.sort_values().plot.barh(color='blue', width=.9, figsize=(10, 6))\n",
    "\n",
    "plt.title('20 Most frequently occuring Python bigrams')\n",
    "plt.ylabel('Bigram')\n",
    "plt.xlabel('# Occurances')\n",
    "\n",
    "# make the labels pretty\n",
    "ticks, _ = plt.yticks()\n",
    "labels = top_20_python_bigrams.reset_index()['index'].apply(lambda t: t[0] + ' ' + t[1])\n",
    "_ = plt.yticks(ticks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_js_bigrams.sort_values().plot.barh(color='orange', width=.9, figsize=(10, 6))\n",
    "\n",
    "plt.title('20 Most frequently occuring JavaScript bigrams')\n",
    "plt.ylabel('Bigram')\n",
    "plt.xlabel('# Occurances')\n",
    "\n",
    "# make the labels pretty\n",
    "ticks, _ = plt.yticks()\n",
    "labels = top_20_js_bigrams.reset_index()['index'].apply(lambda t: t[0] + ' ' + t[1])\n",
    "_ = plt.yticks(ticks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_python_bigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400, random_state=123).generate_from_frequencies(data)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Top 20 Python Bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_js_bigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400, random_state=123).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Top 20 JavaScript Bigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = df.copy()\n",
    "monkey['readme_len'] = monkey['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey.head()\n",
    "#monkey['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  K-Nearest_Neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(monkey.text)\n",
    "y = monkey.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 123)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_train)\n",
    "y_pred_proba = knn.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.2%}'.format(knn.score(X_train, y_train)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(y_train, y_pred))\n",
    "print('---')\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Precision:** the higher this number is, the more you were able to pinpoint all positives correctly.  If this is a low score, you predicted a lot of positives where there were none.\n",
    "    - tp / (tp + fp)\n",
    "\n",
    "\n",
    "- **Recall:** if this score is high, you didn’t miss a lot of positives. But as it gets lower, you are not predicting the positives that are actually there.\n",
    "    - tp / (tp + fn)\n",
    "\n",
    "\n",
    "- **f1-score:** The balanced harmonic mean of Recall and Precision, giving both metrics equal weight. The higher the F-Measure is, the better.\n",
    "\n",
    "\n",
    "- **Support:** number of occurrences of each class in where y is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of KNN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "k_range = range(1, 20)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    scores.append(knn.score(X_test, y_test))\n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, scores)\n",
    "plt.xticks([0,5,10,15,20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Decision Tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(monkey.text)\n",
    "y = monkey.language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model\n",
    "- *Create the Decision Tree Object*\n",
    "- *Fit the model to the training data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state = 123)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=2, random_state=123)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Estimate language*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "y_pred[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Estimate the probability of a species*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = clf.predict_proba(X_train)\n",
    "y_pred_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model\n",
    "- *Compute the Accuracy*\n",
    "- *Accuracy:  number of correct predictions over the number of total instances that have been evaluated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a confusion matrix\n",
    "- **True Positive:** number of occurrences where y is true and y is predicted true.\n",
    "- **True Negative:** number of occurrences where y is false and y is predicted false.\n",
    "- **False Positive:** number of occurrences where y is false and y is predicted true.\n",
    "- **False Negative:** number of occurrences where y is true and y is predicted false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels = sorted(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_train, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Precision:** the higher this number is, the more you were able to pinpoint all positives correctly.  If this is a low score, you predicted a lot of positives where there were none.\n",
    "    - tp / (tp + fp)\n",
    "\n",
    "\n",
    "- **Recall:** if this score is high, you didn’t miss a lot of positives. But as it gets lower, you are not predicting the positives that are actually there.\n",
    "    - tp / (tp + fn)\n",
    "\n",
    "\n",
    "- **f1-score:** The balanced harmonic mean of Recall and Precision, giving both metrics equal weight. The higher the F-Measure is, the better.\n",
    "\n",
    "\n",
    "- **Support:** number of occurrences of each class in where y is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df4.text, df.language, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(X_train)\n",
    "train_tfidf_values = tfidf.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_tfidf_values, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(train_tfidf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame(dict(actual=y_train, predicted=predictions))\n",
    "pd.crosstab(df4.predicted,df4.actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df4.actual, df4.predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Precision:** the higher this number is, the more you were able to pinpoint all positives correctly.  If this is a low score, you predicted a lot of positives where there were none.\n",
    "    - tp / (tp + fp)\n",
    "\n",
    "\n",
    "- **Recall:** if this score is high, you didn’t miss a lot of positives. But as it gets lower, you are not predicting the positives that are actually there.\n",
    "    - tp / (tp + fn)\n",
    "\n",
    "\n",
    "- **f1-score:** The balanced harmonic mean of Recall and Precision, giving both metrics equal weight. The higher the F-Measure is, the better.\n",
    "\n",
    "\n",
    "- **Support:** number of occurrences of each class in where y is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(unknown_text):\n",
    "    return model.predict(tfidf.transform([unknown_text]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidfs = tfidf.fit_transform(monkey.text.values())\n",
    "tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidfs.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
